---
layout: notes
title: presentations
---

*Links to many of my presentations*

## 2024 / 2025

- [Rethinking interp lecture](https://docs.google.com/presentation/d/1UK5neDH6qDq1IDjRDtbLLIpVchzmSwRx8-FeIbJu4Yo/edit?usp=sharing) (1-hr, cornell tech fall '25)
- [LLMs for interp / data science lecture](https://docs.google.com/presentation/d/1PnsRaykeXovIU45lu029LOdPSeEWwbJwNcBujP7YWAM/edit?usp=sharing) (1-hr, berkeley spring '25)
  - In this lecture I’ll cover a selection of topics underlying modern transformers/LLMs and emerging directions of research. I’ll start with a technical overview of LLMs and general research directions and progress. Then, I’ll go into vignettes of recent research work on interpretability, test-time training, and frontier applications for LLMs. I’ll end with some open questions in LLM research.
- [Using LLMs to bridge data-driven models and scientific theories in language neuroscience](https://docs.google.com/presentation/d/1bFZZ8-OwwNxN3DjPdyPFKuP16yyYTP9DlaTzKOaYVWI/)
  - <details>
    <summary>UCSF Biostat talk (45-min)</summary>
    Title: Using LLMs to bridge data-driven models and scientific theories in language neuroscience<br/><br/>
    Abstract: Science faces an explainability crisis: data-driven deep learning methods are proving capable of *predicting* many natural phenomena but not at *explaining* them. One emblematic field is language neuroscience, where LLMs are highly effective at predicting human brain responses to natural language, but are virtually impossible to interpret or analyze by hand. To overcome this challenge, we introduce a framework that translates deep learning models of language selectivity in the brain into concise verbal explanations, and then designs follow-up experiments to verify that these explanations are causally related to brain activity. This approach is successful at explaining selectivity both in individual voxels and cortical regions of interest, demonstrating that LLMs can be used to bridge the widening gap between data-driven models and formal scientific theories. This talk covers 2 papers: Benara et al. ([NeurIPS 2024](https://arxiv.org/abs/2405.16714v1)) & Antonello et al. ([arXiv, 2024](https://arxiv.org/abs/2410.00812)).
    </details>
    - <details>
      <summary>MSR project green talk</summary>
      Title: From Data-Driven Models to Scientific Theories: A Case Study in Language Neuroscience<br/><br/>
      Abstract: Modern data-driven methods are proving capable of predicting many natural phenomena, but not at explaining them. This talk will cover a case study where LLMs can be carefully used to convert predictive models of the human brain into interpretable, testable scientific theories.
      </details>

## 2023

- [uniting LLMs and trees](https://docs.google.com/presentation/d/1I9OWLhrmBiBjoe6CJtxdI6qlIuvL26GZRjeO_ir3i_Y/edit)
  - Title: Uniting Large Language Models and Decision Trees
  - <details>
    <summary>Abstract 1</summary>
    Decision trees are the cornerstone for a wide range of applications, especially in tabular data, where they are often used as a transparent model However, decision trees can fail to model complex interactions and dependencies, an area where modern large language models (LLMs) excel. In this talk, I will discuss recent works that unite decision trees and LLMs to bring out the best in both for NLP applications. Specifically, I will discuss how decision trees can be used to steer LLMs by structuring sequential prompted calls, and how LLMs can be used to improve transparent decision trees by augmenting individual nodes with relevant features.
    </details>
  - <details>
    <summary>Abstract 2</summary>
    Decision trees are a pillar of modern machine learning, forming a foundation for transparent, accurate decision making. However, decision trees often fail to model complex interactions, an area where modern large language models (LLMs) excel. In this talk, I will discuss our recent works that unite decision trees and LLMs to bring out the best in both for NLP applications. Specifically, I will discuss (1) how how LLMs can be used to improve transparent decision trees by augmenting individual nodes with relevant features and (2) how decision trees can be used to steer LLMs by structuring sequential prompted calls.
    </details>

- [explanations from text data](https://docs.google.com/presentation/d/1qL_cATZWiwOg4EjgrQ93m2zEpNMYYdIUUqqMO1REbIk/edit#slide=id.g21b26013510_0_4)

## 2022

- [animated thesis](https://docs.google.com/presentation/d/1TGTwvh1qOUDcSmn4xlFDK5D-j54PUs3IMAxsRL19LYA/edit#slide=id.g11022bc8220_0_8)
- [thesis slides](https://docs.google.com/presentation/d/1IyxCrB5Ol8RsvFBTTy4Y5DQH3TxS9Wgl7Pxt1thrBBY/edit)
  - Title: Useful interpretations for machine learning in science & medicine
  - <details>
    <summary>Abstract</summary>
    Machine-learning models have achieved impressive predictive performance by learning complex functions of many variables. However, this increase in complexity has often come at the cost of interpretability, a crucial consideration in many domains, such as science and medicine. In this talk, I will discuss recent work that extracts useful interpretations from machine-learning models by scoring and distilling interactions. Then, I will showcase how we have worked with domain experts to improve models using these interpretations in different computer vision settings, including bioimaging and cosmology.
    </details>
- [research overview](https://docs.google.com/presentation/d/19fTICv0pyRiwGE39mqE_eGTq0Arn3OtVnDSrURFbPMA/present?slide=id.p)
- [overview slide](https://docs.google.com/presentation/d/1gvYfUkb-7yCXUUerBbgsOhGT82lj2ZPTYPop1qvSVzE/edit#slide=id.ga03c02da4e_0_126)
- [title slide](https://docs.google.com/presentation/d/1gvYfUkb-7yCXUUerBbgsOhGT82lj2ZPTYPop1qvSVzE/edit#slide=id.ga03c02da4e_0_126)
  - [title slide old](https://docs.google.com/presentation/d/13GyKciZZ1zZWmWqCSksvZtGpydAkDjOL0lYVr75kpAU/edit)

## msft talks

- [emb-gam](https://docs.google.com/presentation/d/1ctUCnboHFtEsgJm8J7k66PtPn6xDLH-Y3aq_pNTFPBo/edit#slide=id.p)
- [iprompt](https://docs.google.com/presentation/d/1_4X1IZMm6B621H5_81QZ5DuK9n4E-Vj8sVryVm1kjSE/edit#slide=id.p)
  - [iprompt 5-min](https://docs.google.com/presentation/d/1rbnnV5RNb9OkmoR9go0OWi0Jb5a7uiEFCBGOobH5USI/edit?usp=sharing)

## phd dnn-interp overview talks

<details>
<summary>Abstract</summary>
Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
</details>

- [qual slides](https://docs.google.com/presentation/d/1cdzZsyRYRs9GiR9s2-V7OO8oIcaabT5TVJFGR9qk_HY/edit#slide=id.p)
  - [acd paige slides](https://docs.google.com/presentation/d/1aZvpZVk6pMmcUkQD0MaBQrmoecU835x6bqi1cbQNXeU/edit#slide=id.g51955b76d5_0_7)
    - Title: Disentangled interpretations for deep learning
    - <details>
          <summary>Abstract</summary>
          Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
          </details>
  - [dl joint reading group slides](https://docs.google.com/presentation/d/1MNEtfdD1ng8o_s75FYTZPbUDSmAf7smIc8jJnZ2FbO0/edit?usp=sharing)
    - Title: Disentangled interpretations for deep learning with ACD
    - <details>
          <summary>Abstract</summary>
          Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss our recent works aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed method (named agglomerative contextual decomposition, or ACD) disentangles the importance of features in isolation and the interactions between groups of features. These attributions yield insights across domains, including in NLP/computer vision and can be used to directly improve generalization in interesting ways.
          </details>
  - <details>
      <summary>Summary</summary>
      We focus on a problem in cosmology, where it is crucial to interpret how a model trained on simulations predicts fundamental cosmological parameters. By extending ACD to interpret transformations of input features, we vet the model by analyzing attributions in the frequency domain. Finally, we discuss ongoing work using ACD to develop simple transformations (e.g. adaptive wavelets) which can be both predictive and interpretable for cosmological parameter prediction.
      </details>
        - Paper links: hierarchical interpretations [(ICLR 2019)](https://openreview.net/pdf?id=SkEqro0ctQ), interpreting transformations in cosmology [(ICLR workshop 2020)](https://arxiv.org/abs/2003.01926), penalizing explanations [(ICML 2020)](https://github.com/laura-rieger/deep-explanation-penalization)
  - [bair sem slides](https://docs.google.com/presentation/d/1vpunbuggj1sHxz3g_20pj2vtesHxJNUQoPbsHMMfz3A/edit#slide=id.p)
    - Title: Interpreting and Improving Neural Networks via Disentangled Attributions
    - <details>
      <summary>Abstract</summary>
      Machine learning models have achieved impressive predictive performance by learning complex functions of many variables. However, the inability to effectively interpret these functions has limited their use across many fields, such as science and medicine. This talk will cover a recent line of work aiming to interpret models by attributing importance to features / feature groups for a single prediction. Importantly, the proposed attributions disentangle the importance of features in isolation and the interactions between groups of features. These attributions are shown to yield insights across domains, including an NLP classification task and in understanding cosmological models. Moreover, these attributions can be used during training to improve generalization in interesting ways, such as forcing an image classification model to focus on shape instead of texture. The talk will place a large emphasis on rigorously evaluating and testing the proposed attributions to assure they are properly describing the model.
      </details>
  - [acd + interpretable ml talk](https://docs.google.com/presentation/d/1x4zzugqu1kMhUKdM94MWrfB9Y6-YfrnHsv5MDFvzfmE/edit#slide=id.p)
    - [nlp version](https://docs.google.com/presentation/d/1bFIdoarqhZdwyXmvNKgqfbhKXByfgPW9h27HYGDXKGM/edit)
    - [interpretable ml](https://docs.google.com/presentation/d/13jbgFyYSSDaMUd2w4RY9GHteTcWJj1drS6_2sOkvnv4/present?slide=id.p) (discussion slides for group meeting)
  - [15 min overall talk](https://docs.google.com/presentation/d/16u_23-P7uvouNo_HvpLCgG1Um3ZDu5c3d76YBhQsjjI/edit#slide=id.gc5100c8fb5_0_519)
    - [cd + acd + cdep 5 min talk](https://docs.google.com/presentation/d/1qWor5d7AXVQfcLtQmFdMPUDPHeBkN1wWZid_pfxZjVQ/edit#slide=id.g51955b76d5_0_7)
    - [biohub meeting](https://docs.google.com/presentation/d/1-VadvMFR9UutmjGFXASMTdPLAiuU_6nzqKvPsEjKkW0/edit?usp=sharing) (w/ wooseok)
    - [dudoit group meeting](https://docs.google.com/presentation/d/1xA4aaBXKV4cO5_iXZZcUnheFpcMsT1U5fJZdaE7FwME/edit#slide=id.p)
  - [simons 2020 talk](https://docs.google.com/presentation/d/1OW4LxoKd6qxPaTP6XXH_gTLfrDV9DVfO7Wa00oznJv4/edit#slide=id.p) ([announcement](https://simons.berkeley.edu/events/disentangled-interpretations-and-how-we-can-use-them))
    - Title: Disentangled interpretations and how we can use them
    - <details>
      <summary>Abstract</summary>
      Recent machine learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. This talk will cover recent work aiming to interpret models by attributing importance to features / feature groups for a single prediction. Importantly, the proposed attributions disentangle the importance of features in isolation and the interactions between groups of features. These attributions are shown to yield insights across domains, including an NLP classification task and in understanding cosmological models. Moreover, these attributions can be used during training to directly improve generalization in interesting ways. The talk will place a large emphasis on how to cogently define and evauate the proposed interpretations.
      </details>

### interp individual presentations

- [cogsci acd slides](https://docs.google.com/presentation/d/1I6djTqVn6YGKqxvQk59-4C39LbE68mNQbX1Go5pzTH4/present?slide=id.p)
  - [acd white-theme slides for bin](https://docs.google.com/presentation/d/1GjL0IJWft3IpdWxAwprXNiZkAG4Jl5mU6FNT2t0DGkQ/edit#slide=id.g5d6ee59cb4_0_0)
  - [acd bavrd](https://docs.google.com/presentation/d/1IVeb5ibe561VR5PAQEN5hdnwmpAHquK4JLb1vzT9fd8/present) (3 min pres. given at bavrd 2019)
- [cdep-focused slides 40 min](https://docs.google.com/presentation/d/1F9spZOwKbxtXqpCiKv2V332v4Vs-UV6mQ_7Xs_cnrLg/edit#slide=id.p)
- [trim_cosmology_group_meeting (03/17/21)](https://docs.google.com/presentation/d/1sDtuaZt7bnAzvEB6tEv04rhVifnGSvyA1o6qUA-Z4ys/edit#slide=id.p)
  - [transform interp](https://docs.google.com/presentation/d/1mH1uG38qJg-ar0G-LiVPZWNKPO_2GiD-uayWM5AI-bo/present?slide=id.p)
- [awd (neurips 15 min)](https://docs.google.com/presentation/d/1YrzAir94D0KWewau34dZAXXPdATRuqeVf8Jt-o_wqG8/edit#slide=id.p)
  - [awd (cnls 20 min)](https://docs.google.com/presentation/d/1TuYEzCZrUX_EbjlQId8dig9lmCC0iw68sPxLqXwd-PM/edit#slide=id.p)
- [pdr](https://docs.google.com/presentation/d/18Tdiym7hDeu0c4tj5XezrznIDfbltR4QQJJZjaHi7tk/present?slide=id.p)
- [auxilin prediction](https://docs.google.com/presentation/d/1sQXbFUTSEyrmDkovV8g759Wj8E9LBpATAVV1iweKeGo/edit#slide=id.gc0f646bcee_0_0)
- [interpretation to causation](https://docs.google.com/presentation/d/1vwgpNp36ssspO5LcUgnohtm9JT-PeSwsX47LpZjvono/edit#slide=id.g9d8da641c4_0_36)
- [covid-forecasting](https://docs.google.com/presentation/d/1nAYfBHj9qP-Qzpjho4dyMTx2Bc5Pccxu1VqIFiEaeYg/edit)

## rules (generally white background)

- [figs](https://docs.google.com/presentation/d/1Gk5mEcSp6uePS72q-oF-GhXgLSbHfq2jWqMTKfwDaJw/present?slide=id.g112865da5f6_0_0)
  - [figs old](https://docs.google.com/presentation/d/1hMF3kMoQDdrM6BHDq1CWVYsibgVyaOVtz0l2WEIbsy0/edit?usp=sharing)
- [HS ICML pres](https://docs.google.com/presentation/d/1z8CzmEWWsAq3-QvlLST4RA9JKpZd5ELtkf5vx6ysXCQ/edit?usp=sharing)
  - [hierarchical shrinkage](https://docs.google.com/presentation/d/1inyZnryrs6dNO6VCn7ng6Rjc-XCkuagxx9YVHKJWqrM/present?slide=id.g1154243f54b_2_372) (group meeting)

- [iai](https://docs.google.com/presentation/d/19liiotfBKNPm2lli6U1ZOPd8jCHEL8Y99HcEZFHOSsQ/edit#slide=id.gc6d28d5565_0_0) (group meeting)
  - [iai-rulevetting](https://docs.google.com/presentation/d/1Ui-NWeJobJ4MhcK-1RG20q8PSyQq4harjOsZ-ml8K4s/edit#slide=id.gc6d28d5565_0_0) (215A presentation)
- [stablerules preliminary](https://docs.google.com/presentation/d/1KSUncdizsQ3cmpsZE0uGGWudQuewMKnukI8V3uManHc/edit#slide=id.gd4a2e647f0_0_0)

## [yu-group software](https://docs.google.com/presentation/d/1phLaC8n8iKG68RQtm99BE3gDkOj-NQhiO2AtJGjBFkM/edit#slide=id.gfc50a11ff5_0_0)

- [yu-group python software](https://docs.google.com/presentation/d/1fKBsTGe8hjbxxk2HbV38dhMiboPhPsVeZlnTE47P5nk/edit)
- [imodels gasp 7-min talk](https://docs.google.com/presentation/d/1fpHo-MtIJntlIrYlQj0yK3spJ7Unf10CNTH-PG9LQ_A/edit?usp=sharing)
- [vflow group meeting](https://docs.google.com/presentation/d/1ReJ3Lqh4VZqpu6X6sP47f6Re6PvLIX9ZRAdjb8ZhFXE/edit#slide=id.g1047cb8ef63_0_0)
- [vflow (white, outdated)](https://docs.google.com/presentation/d/1bcrdF5TozKNLdvxvRK63wsDHS8qRDvg2MgsAZYUy2jw/edit)

## dnn misc individual presentations

- [faces final pres](https://docs.google.com/presentation/d/19Z4TnHCDkNENutyKmE_kZBSJX4jMUam6DoH3HckkMrI/present?slide=id.p)
  - [faces midterm pres](https://docs.google.com/presentation/d/1YxSZtsSOdQ_OZYgctFE1Cgfymv0cmBTzNDkt68d-RE0/present?slide=id.p)
  - [faces lit rvw](https://docs.google.com/presentation/d/1C6l4qq0O_-SosHswPwqo3ixJweMI8gkNku9crzZMw80/present?slide=id.p)
- [sensible local interpretations](https://docs.google.com/presentation/d/1tKVgg2bo7jSgE8TMyt15VAOkGhDPncFpMnZ-mmtoMNI/edit#slide=id.p) (5 min update for pacmed internship)
- [semantic segmentation (facebook pres)](https://docs.google.com/presentation/d/1n_EImcRN8_R-smL6h-Gfxa11vsUczljj/present?slide=id.p1)
- [dnn experiments](http://csinva.github.io/pres/dnn_experiments/#/)
  - [deep learning trends 2020](https://docs.google.com/presentation/d/15NRwGfKyGkcDIDKYwoq82PXdJrkJeRmSQfUbk5oQvuM/edit#slide=id.g8da3737a00_0_24)

## teaching

- [machine learning (cs 189)](https://csinva.github.io/pres/189/#/)
- [intro to ai (cs 188)](https://csinva.github.io/pres/188/#/)
- [interpretability workshop](https://docs.google.com/presentation/d/1RIdbV279r20marRrN0b1bu2z9STkrivsMDa_Dauk8kE/present?slide=id.p)

## coursework

- [alphafold 2](https://docs.google.com/presentation/d/1-MjMAgO2F7RH842BcuKjBlmuf5UA9IPsYADhQ56mkuE/edit#slide=id.p)
- [vissci ovw](https://docs.google.com/presentation/d/1d2prZlhmG72whCTzfGJ_pk-Pgv9Sx-bGG0e503whYvw/present?slide=id.p)
- [harms of ai](https://docs.google.com/presentation/d/1yZkEkDU-ELvh_Od3xvhd8Lsu70U7Cjs5QJnpNIVLgII/present) (1 hr talk given in possible minds course 2019)
- [hummingbird tracking](https://docs.google.com/presentation/d/15iygjXGLu7Ha096GwMV5t6sjP7IPcTHqimPkTDwrpPI/present?slide=id.p)

## undergrad

- [wsimule urds](https://docs.google.com/presentation/d/1GO6lN5o2idozOUdnObXGnXKFbZiJiKKKkmx73uE4BAI/present?slide=id.p) (5 min talk given at URDS 2017)
  - [wsimule tomtom](https://docs.google.com/presentation/d/1KghayB2g8u5xwVuILzT4XtalKi3rerVROaoRb6RUVBk/edit) (10 min talk given at Tom Tom 2017)
  - [wsimule slide](/assets/write_ups/wsimule_17_nips_slide.pdf) (1 min pres. given at AMLCID Neurips 2017 workshop)
- [linearization](https://docs.google.com/presentation/d/1JriXXofysuXyfU4CeyNHJUTYSfa18R9Q3EhkCwFwh4g/present?slide=id.p) (5 min talk given at URDS 2017)
- [sparse coding class pres](https://docs.google.com/presentation/d/199lCpVaOA6ez4QXkt9W8-fMv8rF_rGv_9rsXIwe1LKI/edit#slide=id.p)

## posters

- [acd poster](/assets/write_ups/acd_18_bairday_poster.pdf)
  - [acd + interp poster](/assets/write_ups/utokyo_19_interp_poster.pdf)
- [random forest image segmentation](/assets/write_ups/singh_15_rf_segmentation.pdf)
- [wsimule poster](/assets/write_ups/wsimule_17_nips_poster.pdf)

## recordings

- talks
  - [thesis 2022](https://drive.google.com/file/d/1h9M29xcMB0LMHQx8DFi-aJjtIPqJG8Cv/view)
  - [grad slam 2022](https://www.youtube.com/watch?v=1JahZhczriY)
  - [bair sem feb 2020 best vid](https://www.youtube.com/watch?v=avrnelFZSS4&feature=youtu.be)
    - [bair sem feb 2020 livestream](https://www.youtube.com/watch?v=x6AHX-VrcdM&feature=youtu.be)
  - [grad slam 2019](https://www.youtube.com/watch?v=6VVppY-uUgE&feature=youtu.be&t=5600)
  - [textxd 2019](https://berkeley-haas.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=276daa74-7298-40b1-9503-ab17014b5863)
- other talks list
  - trustml symposium talk ([2022](https://www.trustworthyml.org/symposium))
  - frontiers in deep learning workshop (msr, 2022)
  - [cnls physics-informed ml conference](https://web.cvent.com/event/ead9c1d8-c632-4896-ba9e-d5f3fec81e86/websitePage:a9dfad1e-e21c-46b6-bf79-eee883bd1448) (2022)
  - disentangled interpretations and how we can use them ([simons workshop, 2020](https://simons.berkeley.edu/events/disentangled-interpretations-and-how-we-can-use-them))
  - tom tom founder's ml conference (2017)
- misc releases
  - [grad slam 2022](https://eecs.berkeley.edu/news/2022/03/chandan-singh-2022-berkeley-grad-slam-competition-semi-finalist)
  - [covid coe article](https://engineering.berkeley.edu/news/2020/04/getting-the-right-equipment-to-the-right-people/)
    - [berkeley science rvw blurb](https://berkeleysciencereview.com/article/ppe-to-the-ppeople-2/)
    - [berkeley data blurb](https://data.berkeley.edu/news/statistics-computer-sciences-team-reflects-tackling-covid-outbreaks)
  - [bids iml piece](https://bids.berkeley.edu/publications/definitions-methods-and-applications-interpretable-machine-learning)
  - [uvacsnews](https://uvacsnews.wordpress.com/2017/05/04/chandansingh/)
    - [uvacsnews2](https://uvacsnews.wordpress.com/2017/05/12/congratulations-to-our-cs-award-winners/)
    - [uvacsnewstweet](https://twitter.com/CS_UVA/status/860155364347715584)
  - imodels
    - [UC Berkeley Researchers Introduce 'imodels: A Python Package For Fitting Interpretable Machine Learning Models - MarkTechPost](https://www.marktechpost.com/2022/02/10/uc-berkeley-researchers-introduce-imodels-a-python-package-for-fitting-interpretable-machine-learning-models/)
    - [UC Berkeley researchers develop a Python library for predictive modelling](https://analyticsindiamag.com/uc-berkeley-researchers-develop-a-python-library-for-predictive-modelling/)

## bios

<details>
<summary>One-paragraph bio (may 2025)</summary>
Chandan works on interpretable machine learning with the broad goal of improving science and medicine using data. Recently, he has focused on language models and how they can be used to directly explain data from language neuroscience. Separately, he has also worked on developing highly accurate transparent models, such as improving linear models and decision trees. He received his PhD from UC Berkeley in 2022.
</details>

<details>
<summary>One-paragraph bio (nov 2023)</summary>
Chandan is a senior researcher at Microsoft Research, where he works on interpretable machine learning with the broad goal of improving science and medicine using data. Recently, he has focused on language models and how they can be used to directly explain data or to improve transparent models. He completed his Computer Science PhD from UC Berkeley in 2022.
</details>

<details>
<summary>One-paragraph bio (jan 2022)</summary>
Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
</details>

<details>
<summary>2-sentence bio (feb 2022)</summary>
Chandan is a fifth and final-year PhD student in computer science. He hopes to build on recent advances in machine-learning to improve the world of healthcare. His research focuses on how to interpret machine-learning models with the goal of ensuring that they can be reliably used when someone’s health is at stake.
</details>

## research overviews

<details>
<summary>4-paragraph research overview (feb 2022)</summary>
<p>🔎 My research focuses on how we can build trustworthy machine-learning systems by making them interpretable. In my work, interpretability is grounded seriously via close collaboration with domain experts, e.g. medical doctors or cell biologists. These collaborations have given rise to useful methodology, roughly split into two areas: (1) building more effective <em>transparent models</em> and (2) improving the trustworthiness of <em>black-box models</em>. Going forward, I hope to help bridge the gap between transparent models and black-box models to improve real-world healthcare.</p>
<p>🌳 Whever possible, building transparent models is the most effective route towards ensuring interpretability. Transparent models are interpretable by design, including models such as (concise) decision trees, rule lists, and linear models. My work in this area was largely motivated by the problem of <a href="">clinical decision-rule development</a>. Clinical decision rules (especially those used in emergency medicine), need to be extremely transparent so they can be readily audited and used by physicians making split-second decisions. To this end, we have developed methodology for enhancing decision trees. For example, replacing the standard CART algorithm with a novel <a href="https://arxiv.org/abs/2201.11931">greedy algorithm</a> for tree-sums can substantially improve predictive performance without sacrificing predictive performance. Additionally, <a href="https://arxiv.org/abs/2202.00858">hierarchical regularization</a> can improve the predictions of an already fitted model without altering its interpretability. Despite their effectiveness, transparent models such as these often get overlooked in favor of black-box models; to address thie issue, we&#39;ve spent a lot of time curating <a href="https://github.com/csinva/imodels">imodels</a>, an open-source package for fitting state-of-the-art transparent models.</p>
<p>🌀 My second line of my work focuses on interpreting and improving black-box models, such as neural networks, for the cases when a transparent model simply can&#39;t predict well enough. Here, I work closely on real-world problems such as analyzing imaging data from <a href="">cell biology</a> and <a href="https://arxiv.org/abs/2003.01926">cosmology</a>. Interpretability in these contexts demands more nuanced information than standard notions of &quot;feature importance; common in the literature. As a result, we have developed methods to characterize and summarize the <a href="https://arxiv.org/abs/1806.05337">interactions</a> in a neural network, particularly in <a href="https://arxiv.org/abs/2003.01926">transformed domains</a> (such as the Fourier domain), where domain interpretations can be more natural. I&#39;m particularly interested in how we can ensure that these interpretations are <em>useful</em>, either by using them to <a href="http://proceedings.mlr.press/v119/rieger20a.html">embed prior knowledge</a> into a model or identify when it can be trusted.</p>
<p>🤝 There is a lot more work to do on bridging the gap between transparent models and black-box models in the real world. One promising avenue is distillation, whereby we can use a black-box model to build a better transparent model. For example, in <a href="https://proceedings.neurips.cc/paper/2021/hash/acaa23f71f963e96c8847585e71352d6-Abstract.html">one work</a> we were able to distill state-of-the-art neural networks in cell-biology and cosmology into transparent wavelet models with &lt;40 parameters. Despite this huge size reduction, these models actually <em>improve</em> prediction performance. By incorporating close domain knowledge into models and the way we approach problems, I believe interpretability can help unlock many benefits of machine-learning for improving healthcare and science.</p>
</details>

<details class="research_details">
      <summary> Read research overview (interpretable modeling)</summary>
      <div class="research_details_text">
          <p>🔎 My research focuses on how we can build trustworthy machine-learning systems by making them
              interpretable. In
              my work, interpretability is grounded seriously via close collaboration with domain experts, e.g.
              medical
              doctors or cell biologists. These collaborations have given rise to useful methodology, roughly split
              into two
              areas: (1) building more effective <em>transparent models</em> and (2) improving the trustworthiness of
              <em>black-box
                  models</em>. Going forward, I hope to help bridge the gap between transparent models and black-box
              models to
              improve real-world healthcare.
          </p>
          <p>🌳 Whenever possible, <b>building transparent models</b> is the most effective route towards ensuring
              interpretability.
              Transparent models are interpretable by design, including models such as (concise) decision trees, rule
              lists,
              and linear models. My work in this area was largely motivated by the problem of
              <a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000076">clinical
                  decision-rule development</a>. Clinical decision rules (especially those used in emergency
              medicine), need
              to be extremely transparent so they can be readily audited and used by physicians making split-second
              decisions.
              To this end, we have developed methodology for enhancing decision trees. For example, replacing the
              standard
              CART algorithm with a novel <a href="https://arxiv.org/abs/2201.11931">greedy algorithm</a> for
              tree-sums can
              substantially improve predictive performance without sacrificing predictive performance. Additionally,
              <a href="https://arxiv.org/abs/2202.00858">hierarchical regularization</a> can improve the predictions
              of
              an already fitted model without altering its interpretability. Despite their effectiveness, transparent
              models
              such as these often get overlooked in favor of black-box models; to address this issue, we&#39;ve spent
              a lot of
              time curating <a href="https://github.com/csinva/imodels">imodels</a>, an open-source package for
              fitting
              state-of-the-art transparent models.
          </p>
          <p>🌀 My second line of my work focuses on <b>interpreting and improving black-box models</b>, such as
              neural
              networks, for
              the cases when a transparent model simply can&#39;t predict well enough. Here, I work closely on
              real-world
              problems such as analyzing imaging data from <a href="">cell biology</a> and <a
                  href="https://arxiv.org/abs/2003.01926">cosmology</a>. Interpretability in these contexts demands
              more
              nuanced information than standard notions of &quot;feature importance&quot; common in the literature. As
              a result, we
              have developed methods to characterize and summarize the <a
                  href="https://arxiv.org/abs/1806.05337">interactions</a> in a neural network, particularly in <a
                  href="https://arxiv.org/abs/2003.01926">transformed domains</a> (such as the Fourier domain), where
              domain interpretations can be more natural. I&#39;m particularly interested in how we can ensure that
              these
              interpretations are <em>useful</em>, either by using them to <a
                  href="http://proceedings.mlr.press/v119/rieger20a.html">embed prior knowledge</a> into a model or
              identify when it can be trusted.</p>
          <p>🤝 There is a lot more work to do on bridging the gap between transparent models and black-box models in
              the real
              world. One promising avenue is distillation, whereby we can use a black-box model to build a better
              transparent
              model. For example, in <a
                  href="https://proceedings.neurips.cc/paper/2021/hash/acaa23f71f963e96c8847585e71352d6-Abstract.html">one
                  work</a> we were able to distill state-of-the-art neural networks in cell-biology and cosmology into
              transparent wavelet models with &lt;40 parameters. Despite this huge size reduction, these models
              actually <em>improve</em>
              prediction performance. By incorporating close domain knowledge into models and the way we approach
              problems, I
              believe interpretability can help unlock many benefits of machine-learning for improving healthcare and
              science.
          </p>
      </div>
  </details>

<details>
<summary>1000-character summary of MSR work (may 2024)</summary>
<p><strong>LLM Interpretability/Steering</strong> I led the <a href="https://www.nature.com/articles/s41467-023-43713-1">Aug-imodels paper</a>, which leverages LLMs to build transparent models, achieving dramatic (&gt;1000x) efficiency improvements for a large set of text-classification tasks. I have additionally led efforts to steer LLMs, both for black-box models (<a href="https://aclanthology.org/2023.emnlp-main.384/">Tree-Prompt</a>) and for open-source models (<a href="https://openreview.net/pdf?id=xZDWO0oejD">Attention steering</a>, intern's paper).</p>

<p><strong>Next-generation transformers [still exploratory]</strong> I have been exploring how to extract useful insights from brain representations of language (through <a href="https://arxiv.org/abs/2305.09863">SASC</a> and 2 major upcoming submissions). My current focus is on how to leverage these insights in "Hyperdimensional LMs", an architecture that heavily uses retrieval to build CPU-only LMs. Early results are showing some potential, and I'm hoping to get GPT-2 level performance in the next few months, which would be a huge cost-saving for many simple tasks that Microsoft could use in deployment.</p>

<p>I have additionally been involved in broader efforts, including an ongoing collaboration with health futures on using LLMs on clinical text (which has produced a <a href="https://arxiv.org/abs/2306.00024">couple</a> <a href="https://arxiv.org/abs/2403.01002">papers</a> already) and some one-off ideas (e.g. <a href="https://arxiv.org/abs/2402.03774">learning a decision tree</a> algorithm with transformers).</p>

</details>

<!-- <div class="iframe-box" style="margin-top: -30px"> -->
<div class="iframe-box">
    <iframe class="iframe"
        src="https://docs.google.com/presentation/d/e/2PACX-1vSj1GlDHEk8AhlYSL9eRb0sFHDF-QqvgS9SckgeekmzTtYdNQWGalhOR5MlmfKsgyW3TtOYq-SpyPkA/embed?rm=minimal"
        frameborder="0" width="100%" height="auto" allowfullscreen="true" mozallowfullscreen="true"
        webkitallowfullscreen="true">
    </iframe>
</div>

<!--<table>-->
<!--    <tr>-->
<!--        <th>-->
<!--            <strong style="font-size:21px;"> interpretable ml </strong> <br/>-->
<!--            <a href="/blog/research/interp"> what is interpretability? </a> <br/>-->
<!--            <a href="/blog/research/interp_eval"> evaluating interpretability </a>-->
<!--        </th>-->
<!--&lt;!&ndash;        <th><strong style="font-size:21px;"> interpretability applications </strong></th>&ndash;&gt;-->
<!--        <th>-->
<!--            <strong style="font-size:21px;"> science </strong> <br/>-->
<!--            <a href="/blog/research/connectomics"> Connectomics</a> <br/>-->
<!--            <a href="/blog/research/neural_coding" > neural coding </a>-->
<!--        </th>-->
<!--&lt;!&ndash;        <th>&ndash;&gt;-->
<!--&lt;!&ndash;            <strong style="font-size:21px;"> ml theory </strong>&ndash;&gt;-->
<!--&lt;!&ndash;        </th>&ndash;&gt;-->
<!--    </tr>-->
<!--    <tr>-->
<!--        <th><br/></th>-->
<!--    </tr>-->
<!--    <tr>-->
<!--        <th><img src="{{ site.baseurl }}/assets/img/alexnet.png" class="research_thumb"></th>-->
<!--&lt;!&ndash;        <th><img src="{{ site.baseurl }}/assets/img/cosmo.png" class="research_thumb"></th>&ndash;&gt;-->
<!--        <th><img src="{{ site.baseurl }}/assets/img/neuron.gif" class="research_thumb"></th>-->
<!--&lt;!&ndash;        <th><img src="{{ site.baseurl }}/assets/img/complexity.png" class="research_thumb"></th>&ndash;&gt;-->
<!--    </tr>-->
<!--</table>-->

<!--<br/>-->