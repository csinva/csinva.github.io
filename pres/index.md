---
layout: notes
title: presentations
---

*Links to many of my presentations*

## in progress

- [animated thesis](https://docs.google.com/presentation/d/1TGTwvh1qOUDcSmn4xlFDK5D-j54PUs3IMAxsRL19LYA/edit#slide=id.g11022bc8220_0_8)
- [thesis slides](https://docs.google.com/presentation/d/1IyxCrB5Ol8RsvFBTTy4Y5DQH3TxS9Wgl7Pxt1thrBBY/edit)
	- Title: Useful interpretations for machine learning in science & medicine
	- <details>
    <summary>Abstract</summary>
    Machine-learning models have achieved impressive predictive performance by learning complex functions of many variables. However, this increase in complexity has often come at the cost of interpretability, a crucial consideration in many domains, such as science and medicine. In this talk, I will discuss recent work that extracts useful interpretations from machine-learning models by scoring and distilling interactions. Then, I will showcase how we have worked with domain experts to improve models using these interpretations in different computer vision settings, including bioimaging and cosmology.
    </details>
- [research overview](https://docs.google.com/presentation/d/19fTICv0pyRiwGE39mqE_eGTq0Arn3OtVnDSrURFbPMA/present?slide=id.p) 
- [overview slide](https://docs.google.com/presentation/d/1gvYfUkb-7yCXUUerBbgsOhGT82lj2ZPTYPop1qvSVzE/edit#slide=id.ga03c02da4e_0_126)
- [title slide](https://docs.google.com/presentation/d/1gvYfUkb-7yCXUUerBbgsOhGT82lj2ZPTYPop1qvSVzE/edit#slide=id.ga03c02da4e_0_126)
  - [title slide old](https://docs.google.com/presentation/d/13GyKciZZ1zZWmWqCSksvZtGpydAkDjOL0lYVr75kpAU/edit)

## phd dnn-interp overview talks

<details>
<summary>Abstract</summary>
Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
</details>

- [qual slides](https://docs.google.com/presentation/d/1cdzZsyRYRs9GiR9s2-V7OO8oIcaabT5TVJFGR9qk_HY/edit#slide=id.p)
    - [acd paige slides](https://docs.google.com/presentation/d/1aZvpZVk6pMmcUkQD0MaBQrmoecU835x6bqi1cbQNXeU/edit#slide=id.g51955b76d5_0_7)
        - Title: Disentangled interpretations for deep learning
        - <details>
          <summary>Abstract</summary>
          Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
          </details>
    - [dl joint reading group slides](https://docs.google.com/presentation/d/1MNEtfdD1ng8o_s75FYTZPbUDSmAf7smIc8jJnZ2FbO0/edit?usp=sharing)
        - Title: Disentangled interpretations for deep learning with ACD
        - <details>
          <summary>Abstract</summary>
          Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss our recent works aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed method (named agglomerative contextual decomposition, or ACD) disentangles the importance of features in isolation and the interactions between groups of features. These attributions yield insights across domains, including in NLP/computer vision and can be used to directly improve generalization in interesting ways.
          </details>
    - <details>
      <summary>Summary</summary>
      We focus on a problem in cosmology, where it is crucial to interpret how a model trained on simulations predicts fundamental cosmological parameters. By extending ACD to interpret transformations of input features, we vet the model by analyzing attributions in the frequency domain. Finally, we discuss ongoing work using ACD to develop simple transformations (e.g. adaptive wavelets) which can be both predictive and interpretable for cosmological parameter prediction.
      </details>
        - Paper links: hierarchical interpretations [(ICLR 2019)](https://openreview.net/pdf?id=SkEqro0ctQ), interpreting transformations in cosmology [(ICLR workshop 2020)](https://arxiv.org/abs/2003.01926), penalizing explanations [(ICML 2020)](https://github.com/laura-rieger/deep-explanation-penalization)
    - [bair sem slides](https://docs.google.com/presentation/d/1vpunbuggj1sHxz3g_20pj2vtesHxJNUQoPbsHMMfz3A/edit#slide=id.p)
        - Title: Interpreting and Improving Neural Networks via Disentangled Attributions
        - <details>
      <summary>Abstract</summary>
      Machine learning models have achieved impressive predictive performance by learning complex functions of many variables. However, the inability to effectively interpret these functions has limited their use across many fields, such as science and medicine. This talk will cover a recent line of work aiming to interpret models by attributing importance to features / feature groups for a single prediction. Importantly, the proposed attributions disentangle the importance of features in isolation and the interactions between groups of features. These attributions are shown to yield insights across domains, including an NLP classification task and in understanding cosmological models. Moreover, these attributions can be used during training to improve generalization in interesting ways, such as forcing an image classification model to focus on shape instead of texture. The talk will place a large emphasis on rigorously evaluating and testing the proposed attributions to assure they are properly describing the model.
      </details>
    - [acd + interpretable ml talk](https://docs.google.com/presentation/d/1x4zzugqu1kMhUKdM94MWrfB9Y6-YfrnHsv5MDFvzfmE/edit#slide=id.p)
        - [nlp version](https://docs.google.com/presentation/d/1bFIdoarqhZdwyXmvNKgqfbhKXByfgPW9h27HYGDXKGM/edit)
        - [interpretable ml](https://docs.google.com/presentation/d/13jbgFyYSSDaMUd2w4RY9GHteTcWJj1drS6_2sOkvnv4/present?slide=id.p) (discussion slides for group meeting)
    - [15 min overall talk](https://docs.google.com/presentation/d/16u_23-P7uvouNo_HvpLCgG1Um3ZDu5c3d76YBhQsjjI/edit#slide=id.gc5100c8fb5_0_519)
        - [cd + acd + cdep 5 min talk](https://docs.google.com/presentation/d/1qWor5d7AXVQfcLtQmFdMPUDPHeBkN1wWZid_pfxZjVQ/edit#slide=id.g51955b76d5_0_7)
        - [biohub meeting](https://docs.google.com/presentation/d/1-VadvMFR9UutmjGFXASMTdPLAiuU_6nzqKvPsEjKkW0/edit?usp=sharing) (w/ wooseok)
        - [dudoit group meeting](https://docs.google.com/presentation/d/1xA4aaBXKV4cO5_iXZZcUnheFpcMsT1U5fJZdaE7FwME/edit#slide=id.p)
    - [simons 2020 talk](https://docs.google.com/presentation/d/1OW4LxoKd6qxPaTP6XXH_gTLfrDV9DVfO7Wa00oznJv4/edit#slide=id.p) ([announcement](https://simons.berkeley.edu/events/disentangled-interpretations-and-how-we-can-use-them))
      - Title: Disentangled interpretations and how we can use them
      - <details>
      <summary>Abstract</summary>
      Recent machine learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. This talk will cover recent work aiming to interpret models by attributing importance to features / feature groups for a single prediction. Importantly, the proposed attributions disentangle the importance of features in isolation and the interactions between groups of features. These attributions are shown to yield insights across domains, including an NLP classification task and in understanding cosmological models. Moreover, these attributions can be used during training to directly improve generalization in interesting ways. The talk will place a large emphasis on how to cogently define and evauate the proposed interpretations.
      </details>

### interp individual presentations

- [cogsci acd slides](https://docs.google.com/presentation/d/1I6djTqVn6YGKqxvQk59-4C39LbE68mNQbX1Go5pzTH4/present?slide=id.p)
    - [acd white-theme slides for bin](https://docs.google.com/presentation/d/1GjL0IJWft3IpdWxAwprXNiZkAG4Jl5mU6FNT2t0DGkQ/edit#slide=id.g5d6ee59cb4_0_0)
    - [acd bavrd](https://docs.google.com/presentation/d/1IVeb5ibe561VR5PAQEN5hdnwmpAHquK4JLb1vzT9fd8/present) (3 min pres. given at bavrd 2019)
- [cdep-focused slides 40 min](https://docs.google.com/presentation/d/1F9spZOwKbxtXqpCiKv2V332v4Vs-UV6mQ_7Xs_cnrLg/edit#slide=id.p)    
- [trim_cosmology_group_meeting (03/17/21)](https://docs.google.com/presentation/d/1sDtuaZt7bnAzvEB6tEv04rhVifnGSvyA1o6qUA-Z4ys/edit#slide=id.p)
    - [transform interp](https://docs.google.com/presentation/d/1mH1uG38qJg-ar0G-LiVPZWNKPO_2GiD-uayWM5AI-bo/present?slide=id.p)
- [awd (neurips 15 min)](https://docs.google.com/presentation/d/1YrzAir94D0KWewau34dZAXXPdATRuqeVf8Jt-o_wqG8/edit#slide=id.p)
  - [awd (cnls 20 min)](https://docs.google.com/presentation/d/1TuYEzCZrUX_EbjlQId8dig9lmCC0iw68sPxLqXwd-PM/edit#slide=id.p)
- [pdr](https://docs.google.com/presentation/d/18Tdiym7hDeu0c4tj5XezrznIDfbltR4QQJJZjaHi7tk/present?slide=id.p)
- [auxilin prediction](https://docs.google.com/presentation/d/1sQXbFUTSEyrmDkovV8g759Wj8E9LBpATAVV1iweKeGo/edit#slide=id.gc0f646bcee_0_0)
- [interpretation to causation](https://docs.google.com/presentation/d/1vwgpNp36ssspO5LcUgnohtm9JT-PeSwsX47LpZjvono/edit#slide=id.g9d8da641c4_0_36)
- [covid-forecasting](https://docs.google.com/presentation/d/1nAYfBHj9qP-Qzpjho4dyMTx2Bc5Pccxu1VqIFiEaeYg/edit)

## rules (generally white background)

- [figs](https://docs.google.com/presentation/d/1Gk5mEcSp6uePS72q-oF-GhXgLSbHfq2jWqMTKfwDaJw/present?slide=id.g112865da5f6_0_0)
  - [figs old](https://docs.google.com/presentation/d/1hMF3kMoQDdrM6BHDq1CWVYsibgVyaOVtz0l2WEIbsy0/edit?usp=sharing)
- [HS ICML pres](https://docs.google.com/presentation/d/1z8CzmEWWsAq3-QvlLST4RA9JKpZd5ELtkf5vx6ysXCQ/edit?usp=sharing)
  - [hierarchical shrinkage](https://docs.google.com/presentation/d/1inyZnryrs6dNO6VCn7ng6Rjc-XCkuagxx9YVHKJWqrM/present?slide=id.g1154243f54b_2_372) (group meeting)

- [iai](https://docs.google.com/presentation/d/19liiotfBKNPm2lli6U1ZOPd8jCHEL8Y99HcEZFHOSsQ/edit#slide=id.gc6d28d5565_0_0) (group meeting)
  - [iai-rulevetting](https://docs.google.com/presentation/d/1Ui-NWeJobJ4MhcK-1RG20q8PSyQq4harjOsZ-ml8K4s/edit#slide=id.gc6d28d5565_0_0) (215A presentation)
- [stablerules preliminary](https://docs.google.com/presentation/d/1KSUncdizsQ3cmpsZE0uGGWudQuewMKnukI8V3uManHc/edit#slide=id.gd4a2e647f0_0_0)

## [yu-group software](https://docs.google.com/presentation/d/1phLaC8n8iKG68RQtm99BE3gDkOj-NQhiO2AtJGjBFkM/edit#slide=id.gfc50a11ff5_0_0)

- [yu-group python software](https://docs.google.com/presentation/d/1fKBsTGe8hjbxxk2HbV38dhMiboPhPsVeZlnTE47P5nk/edit)
- [imodels gasp 7-min talk](https://docs.google.com/presentation/d/1fpHo-MtIJntlIrYlQj0yK3spJ7Unf10CNTH-PG9LQ_A/edit?usp=sharing)
- [vflow group meeting](https://docs.google.com/presentation/d/1ReJ3Lqh4VZqpu6X6sP47f6Re6PvLIX9ZRAdjb8ZhFXE/edit#slide=id.g1047cb8ef63_0_0)
- [vflow (white, outdated)](https://docs.google.com/presentation/d/1bcrdF5TozKNLdvxvRK63wsDHS8qRDvg2MgsAZYUy2jw/edit)




### dnn misc individual presentations
- [faces final pres](https://docs.google.com/presentation/d/19Z4TnHCDkNENutyKmE_kZBSJX4jMUam6DoH3HckkMrI/present?slide=id.p)
    - [faces midterm pres](https://docs.google.com/presentation/d/1YxSZtsSOdQ_OZYgctFE1Cgfymv0cmBTzNDkt68d-RE0/present?slide=id.p)
    - [faces lit rvw](https://docs.google.com/presentation/d/1C6l4qq0O_-SosHswPwqo3ixJweMI8gkNku9crzZMw80/present?slide=id.p)
- [sensible local interpretations](https://docs.google.com/presentation/d/1tKVgg2bo7jSgE8TMyt15VAOkGhDPncFpMnZ-mmtoMNI/edit#slide=id.p) (5 min update for pacmed internship)
- [semantic segmentation (facebook pres)](https://docs.google.com/presentation/d/1n_EImcRN8_R-smL6h-Gfxa11vsUczljj/present?slide=id.p1)
- [dnn experiments](http://csinva.github.io/pres/dnn_experiments/#/)
    - [deep learning trends 2020](https://docs.google.com/presentation/d/15NRwGfKyGkcDIDKYwoq82PXdJrkJeRmSQfUbk5oQvuM/edit#slide=id.g8da3737a00_0_24)


### teaching
- [machine learning (cs 189)](https://csinva.github.io/pres/189/#/)
- [intro to ai (cs 188)](https://csinva.github.io/pres/188/#/)
- [interpretability workshop](https://docs.google.com/presentation/d/1RIdbV279r20marRrN0b1bu2z9STkrivsMDa_Dauk8kE/present?slide=id.p)

### coursework
- [alphafold 2](https://docs.google.com/presentation/d/1-MjMAgO2F7RH842BcuKjBlmuf5UA9IPsYADhQ56mkuE/edit#slide=id.p)
- [vissci ovw](https://docs.google.com/presentation/d/1d2prZlhmG72whCTzfGJ_pk-Pgv9Sx-bGG0e503whYvw/present?slide=id.p)
- [harms of ai](https://docs.google.com/presentation/d/1yZkEkDU-ELvh_Od3xvhd8Lsu70U7Cjs5QJnpNIVLgII/present) (1 hr talk given in possible minds course 2019)
- [hummingbird tracking](https://docs.google.com/presentation/d/15iygjXGLu7Ha096GwMV5t6sjP7IPcTHqimPkTDwrpPI/present?slide=id.p)

### undergrad

- [wsimule urds](https://docs.google.com/presentation/d/1GO6lN5o2idozOUdnObXGnXKFbZiJiKKKkmx73uE4BAI/present?slide=id.p) (5 min talk given at URDS 2017)
    - [wsimule tomtom](https://docs.google.com/presentation/d/1KghayB2g8u5xwVuILzT4XtalKi3rerVROaoRb6RUVBk/edit) (10 min talk given at Tom Tom 2017)
    - [wsimule slide](/assets/write_ups/wsimule_17_nips_slide.pdf) (1 min pres. given at AMLCID Neurips 2017 workshop)
- [linearization](https://docs.google.com/presentation/d/1JriXXofysuXyfU4CeyNHJUTYSfa18R9Q3EhkCwFwh4g/present?slide=id.p) (5 min talk given at URDS 2017)
- [sparse coding class pres](https://docs.google.com/presentation/d/199lCpVaOA6ez4QXkt9W8-fMv8rF_rGv_9rsXIwe1LKI/edit#slide=id.p)

### posters
- [acd poster](/assets/write_ups/acd_18_bairday_poster.pdf)
	- [acd + interp poster](/assets/write_ups/utokyo_19_interp_poster.pdf)
- [random forest image segmentation](/assets/write_ups/singh_15_rf_segmentation.pdf)
- [wsimule poster](/assets/write_ups/wsimule_17_nips_poster.pdf)

### recordings
- talks
    - [thesis 2022](https://drive.google.com/file/d/1h9M29xcMB0LMHQx8DFi-aJjtIPqJG8Cv/view)
    - [grad slam 2022](https://www.youtube.com/watch?v=1JahZhczriY)
    - [bair sem feb 2020 best vid](https://www.youtube.com/watch?v=avrnelFZSS4&feature=youtu.be)
        - [bair sem feb 2020 livestream](https://www.youtube.com/watch?v=x6AHX-VrcdM&feature=youtu.be)
    - [grad slam 2019](https://www.youtube.com/watch?v=6VVppY-uUgE&feature=youtu.be&t=5600)
    - [textxd 2019](https://berkeley-haas.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=276daa74-7298-40b1-9503-ab17014b5863)
- other talks list
    - [cnls physics-informed ml conference](https://web.cvent.com/event/ead9c1d8-c632-4896-ba9e-d5f3fec81e86/websitePage:a9dfad1e-e21c-46b6-bf79-eee883bd1448) (2022)
    - disentangled interpretations and how we can use them ([simons workshop](https://simons.berkeley.edu/events/disentangled-interpretations-and-how-we-can-use-them))
    - tom tom founder's ml conference (2017)
- misc releases
  - [grad slam 2022](https://eecs.berkeley.edu/news/2022/03/chandan-singh-2022-berkeley-grad-slam-competition-semi-finalist)
  - [covid coe article](https://engineering.berkeley.edu/news/2020/04/getting-the-right-equipment-to-the-right-people/)
    - [berkeley science rvw blurb](https://berkeleysciencereview.com/article/ppe-to-the-ppeople-2/)
    - [berkeley data blurb](https://data.berkeley.edu/news/statistics-computer-sciences-team-reflects-tackling-covid-outbreaks)
  - [bids iml piece](https://bids.berkeley.edu/publications/definitions-methods-and-applications-interpretable-machine-learning)
  - [uvacsnews](https://uvacsnews.wordpress.com/2017/05/04/chandansingh/)
    - [uvacsnews2](https://uvacsnews.wordpress.com/2017/05/12/congratulations-to-our-cs-award-winners/)
    - [uvacsnewstweet](https://twitter.com/CS_UVA/status/860155364347715584)
  - imodels
    - [UC Berkeley Researchers Introduce 'imodels: A Python Package For Fitting Interpretable Machine Learning Models - MarkTechPost](https://www.marktechpost.com/2022/02/10/uc-berkeley-researchers-introduce-imodels-a-python-package-for-fitting-interpretable-machine-learning-models/)
    - [UC Berkeley researchers develop a Python library for predictive modelling](https://analyticsindiamag.com/uc-berkeley-researchers-develop-a-python-library-for-predictive-modelling/)


### bios

<details>
<summary>One-paragraph bio (jan 2022)</summary>
Deep learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. I will discuss a recent line of work aiming to interpret neural networks by attributing importance to features and feature interactions for individual predictions. Importantly, the proposed methods disentangle the importance of features in isolation and the interactions between groups of features. These attributions significantly enhance interpretability and can be used to directly improve generalization in interesting ways. I will showcase how we have worked with domain experts to make these attributions useful in different computer vision settings, including in bioimaging and cosmology.
</details>

<details>
<summary>2-sentence bio (feb 2022)</summary>
Chandan is a fifth and final-year PhD student in computer science. He hopes to build on recent advances in machine-learning to improve the world of healthcare.His research focuses on how to interpret machine-learning models with the goal of ensuring that they can be reliably used when someone‚Äôs health is at stake.
</details>

<details>
<summary>4-paragraph research overview (feb 2022)</summary>
<p>üîé My research focuses on how we can build trustworthy machine-learning systems by making them interpretable. In my work, interpretability is grounded seriously via close collaboration with domain experts, e.g. medical doctors or cell biologists. These collaborations have given rise to useful methodology, roughly split into two areas: (1) building more effective <em>transparent models</em> and (2) improving the trustworthiness of <em>black-box models</em>. Going forward, I hope to help bridge the gap between transparent models and black-box models to improve real-world healthcare.</p>
<p>üå≥ Whever possible, building transparent models is the most effective route towards ensuring interpretability. Transparent models are interpretable by design, including models such as (concise) decision trees, rule lists, and linear models. My work in this area was largely motivated by the problem of <a href="">clinical decision-rule development</a>. Clinical decision rules (especially those used in emergency medicine), need to be extremely transparent so they can be readily audited and used by physicians making split-second decisions. To this end, we have developed methodology for enhancing decision trees. For example, replacing the standard CART algorithm with a novel <a href="https://arxiv.org/abs/2201.11931">greedy algorithm</a> for tree-sums can substantially improve predictive performance without sacrificing predictive performance. Additionally, <a href="https://arxiv.org/abs/2202.00858">hierarchical regularization</a> can improve the predictions of an already fitted model without altering its interpretability. Despite their effectiveness, transparent models such as these often get overlooked in favor of black-box models; to address thie issue, we&#39;ve spent a lot of time curating <a href="https://github.com/csinva/imodels">imodels</a>, an open-source package for fitting state-of-the-art transparent models.</p>
<p>üåÄ My second line of my work focuses on interpreting and improving black-box models, such as neural networks, for the cases when a transparent model simply can&#39;t predict well enough. Here, I work closely on real-world problems such as analyzing imaging data from <a href="">cell biology</a> and <a href="https://arxiv.org/abs/2003.01926">cosmology</a>. Interpretability in these contexts demands more nuanced information than standard notions of &quot;feature importance; common in the literature. As a result, we have developed methods to characterize and summarize the <a href="https://arxiv.org/abs/1806.05337">interactions</a> in a neural network, particularly in <a href="https://arxiv.org/abs/2003.01926">transformed domains</a> (such as the Fourier domain), where domain interpretations can be more natural. I&#39;m particularly interested in how we can ensure that these interpretations are <em>useful</em>, either by using them to <a href="http://proceedings.mlr.press/v119/rieger20a.html">embed prior knowledge</a> into a model or identify when it can be trusted.</p>
<p>ü§ù There is a lot more work to do on bridging the gap between transparent models and black-box models in the real world. One promising avenue is distillation, whereby we can use a black-box model to build a better transparent model. For example, in <a href="https://proceedings.neurips.cc/paper/2021/hash/acaa23f71f963e96c8847585e71352d6-Abstract.html">one work</a> we were able to distill state-of-the-art neural networks in cell-biology and cosmology into transparent wavelet models with &lt;40 parameters. Despite this huge size reduction, these models actually <em>improve</em> prediction performance. By incorporating close domain knowledge into models and the way we approach problems, I believe interpretability can help unlock many benefits of machine-learning for improving healthcare and science.</p>
</details>