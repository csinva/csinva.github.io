
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.1. unsupervised</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/ml/unsupervised';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3.2. structure learning" href="structure_ml.html" />
    <link rel="prev" title="3. ml" href="ml.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview üëã
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ai/ai.html">1. ai</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ai/knowledge_rep.html">1.1. representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/psychology.html">1.2. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/fairness_sts.html">1.3. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/philosophy.html">1.4. philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/ai_futures.html">1.5. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/cogsci.html">1.6. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/llms.html">1.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/logic.html">1.8. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/search.html">1.9. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/decisions_rl.html">1.10. decisions, rl</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/math.html">2. math</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/linear_algebra.html">2.1. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/optimization.html">2.2. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/differential_equations.html">2.3. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/chaos.html">2.4. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/math_basics.html">2.5. math basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/signals.html">2.6. signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/calculus.html">2.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/proofs.html">2.8. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/analysis.html">2.9. real analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ml.html">3. ml</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">3.1. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="structure_ml.html">3.2. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="learning_theory.html">3.3. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="deep_learning.html">3.4. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="comp_vision.html">3.5. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernels.html">3.6. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="nlp.html">3.7. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="feature_selection.html">3.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluation.html">3.9. evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification.html">3.10. classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stat/stat.html">4. stat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stat/time_series.html">4.1. time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/graphical_models.html">4.2. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/causal_inference.html">4.3. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/game_theory.html">4.4. game theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/data_analysis.html">4.7. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/testing.html">4.8. testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">5. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">5.1. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">5.2. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">5.3. development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">5.4. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">5.5. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">5.6. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">5.7. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">5.8. brain basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">6. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">6.1. cs theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">6.2. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">6.3. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">6.4. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">6.5. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">6.6. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">6.7. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">6.8. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">6.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">6.10. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">6.11. reproducibility</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../research_ovws/research_ovws.html">7. research_ovws</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_disentanglement.html">7.1. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_uncertainty.html">7.2. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_generalization.html">7.3. generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_causal_inference.html">7.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_omics.html">7.5. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">7.6. ml in medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_llms.html">7.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">7.8. transfer learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interp.html">7.9. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_complexity.html">7.10. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_dl_theory.html">7.11. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_scat.html">7.12. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interesting_science.html">7.13. interesting science</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/ml/unsupervised.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>unsupervised</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">3.1.1. clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical">3.1.1.1. hierarchical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partitional">3.1.1.2. partitional</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-clustering-j-10">3.1.1.3. statistical clustering (j 10)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-mixture-models-regression-classification-j-10">3.1.1.4. conditional mixture models - regression/classification (j 10)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dim-reduction">3.1.2. dim reduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering">3.1.2.1. spectral clustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">3.1.2.2. pca</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">3.1.2.3. topic modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-coding-sparse-dictionary-learning">3.1.2.4. sparse coding = sparse dictionary learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ica">3.1.2.5. ica</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topological">3.1.2.6. topological</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">3.1.2.7. misc</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models">3.1.3. generative models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-models">3.1.3.1. autoregressive models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-models">3.1.3.1.1. flow models</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vaes">3.1.3.2. vaes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gans">3.1.3.3. gans</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-energy-based-models">3.1.3.4. diffusion / energy-based models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-semi-supervised">3.1.4. self/semi-supervised</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-supervised">3.1.4.1. self-supervised</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#semi-supervised">3.1.4.2. semi-supervised</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression">3.1.4.3. compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-learning">3.1.4.4. contrastive learning</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="unsupervised">
<h1><span class="section-number">3.1. </span>unsupervised<a class="headerlink" href="#unsupervised" title="Link to this heading">#</a></h1>
<section id="clustering">
<h2><span class="section-number">3.1.1. </span>clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>labels are not given</p></li>
<li><p>intra-cluster distances are minimized, inter-cluster distances are maximized</p></li>
<li><p>distance measures</p>
<ul>
<li><p>symmetric D(A,B)=D(B,A)</p></li>
<li><p>self-similarity D(A,A)=0</p></li>
<li><p>positivity separation D(A,B)=0 iff A=B</p></li>
<li><p>triangular inequality D(A,B) &lt;<span class="math notranslate nohighlight">\(\leq\)</span> D(A,C)+D(B,C)</p></li>
<li><p>ex. Minkowski Metrics <span class="math notranslate nohighlight">\(d(x,y)=\sqrt[r]{\sum \vert x_i-y_i\vert ^r}\)</span></p>
<ul>
<li><p>r=1 Manhattan distance</p></li>
<li><p>r=1 when y is binary -&gt; Hamming distance</p></li>
<li><p>r=2 Euclidean</p></li>
<li><p>r=<span class="math notranslate nohighlight">\(\infty\)</span> ‚Äúsup‚Äù distance</p></li>
</ul>
</li>
</ul>
</li>
<li><p>correlation coefficient - unit independent</p></li>
<li><p>edit distance</p></li>
</ul>
<section id="hierarchical">
<h3><span class="section-number">3.1.1.1. </span>hierarchical<a class="headerlink" href="#hierarchical" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>two approaches:</p>
<ol class="arabic simple">
<li><p>bottom-up agglomerative clustering - starts with each object in separate cluster then joins</p></li>
<li><p>top-down divisive - starts with 1 cluster then separates</p></li>
</ol>
</li>
<li><p>ex. starting with each item in its own cluster, find best pair to merge into a new cluster</p>
<ul>
<li><p>repeatedly do this to make a tree (dendrogram)</p></li>
</ul>
</li>
<li><p>distances between clusters defined by <em>linkage function</em></p>
<ul>
<li><p>single-link - closest members (long, skinny clusters)</p></li>
<li><p>complete-link - furthest members  (tight clusters)</p></li>
<li><p>average - most widely used</p></li>
</ul>
</li>
<li><p>ex. MST - keep linking shortest link</p></li>
<li><p><em>ultrametric distance</em> - tighter than triangle inequality</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d(x, y) \leq \max[d(x,z), d(y,z)]\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="partitional">
<h3><span class="section-number">3.1.1.2. </span>partitional<a class="headerlink" href="#partitional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>partition n objects into a set of K clusters (must be specified)</p></li>
<li><p>globally optimal: exhaustively enumerate all partitions</p></li>
<li><p>minimize sum of squared distances from cluster centroid</p></li>
<li><p>evaluation w/ labels - purity - ratio between dominant class in cluster and size of cluster</p></li>
<li><p>k-means++ - better at not getting stuck in local minima</p>
<ul>
<li><p>randomly move centers apart</p></li>
</ul>
</li>
<li><p>Complexity: <span class="math notranslate nohighlight">\(O(n^2p)\)</span> for first iteration and then can only get worse</p></li>
</ul>
</section>
<section id="statistical-clustering-j-10">
<h3><span class="section-number">3.1.1.3. </span>statistical clustering (j 10)<a class="headerlink" href="#statistical-clustering-j-10" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>latent vars</em> - values not specified in the observed data</p></li>
<li><p><img alt="" src="../../_images/j10_1.png" /></p></li>
<li><p><em>K-Means</em></p>
<ul>
<li><p>start with random centers</p></li>
<li><p>E: assign everything to nearest center: <span class="math notranslate nohighlight">\(O(\|\text{clusters}\|*np) \)</span></p></li>
<li><p>M: recompute centers <span class="math notranslate nohighlight">\(O(np)\)</span> and repeat until nothing changes</p></li>
<li><p>partition amounts to Voronoi diagram</p></li>
<li><p>can be viewed as minimizing <em>distortion measure</em> <span class="math notranslate nohighlight">\(J=\sum_n \sum_i z_n^i ||x_n - \mu_i||^2\)</span></p></li>
</ul>
</li>
<li><p><em>GMMs</em>: <span class="math notranslate nohighlight">\(p(x|\theta) = \underset{i}{\Sigma} \pi_i \mathcal{N}(x|\mu_i, \Sigma_i)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(l(\theta|x) = \sum_n \log \: p(x_n|\theta) \\ = \sum_n \log \sum_i \pi_i \mathcal{N}(x_n|\mu_i, \Sigma_i)\)</span></p></li>
<li><p>hard to maximize bcause log acts on a sum</p></li>
<li><p>‚Äúsoft‚Äù version of K-means - update means as weighted sums of data instead of just normal mean</p></li>
<li><p>sometimes initialize K-means w/ GMMs</p></li>
</ul>
</li>
</ul>
</section>
<section id="conditional-mixture-models-regression-classification-j-10">
<h3><span class="section-number">3.1.1.4. </span>conditional mixture models - regression/classification (j 10)<a class="headerlink" href="#conditional-mixture-models-regression-classification-j-10" title="Link to this heading">#</a></h3>
<div class="highlight-mermaid notranslate"><div class="highlight"><pre><span></span>graph LR;
  X--&gt;Y;
  X --&gt; Z
  Z --&gt; Y
</pre></div>
</div>
<ul class="simple">
<li><p>ex. <img alt="" src="../../_images/j5_16.png" /></p></li>
<li><p>latent variable Z has multinomial distr.</p>
<ul>
<li><p><em>mixing proportions</em>: <span class="math notranslate nohighlight">\(P(Z^i=1|x, \xi)\)</span></p>
<ul>
<li><p>ex. <span class="math notranslate nohighlight">\( \frac{e^{\xi_i^Tx}}{\sum_je^{\xi_j^Tx}}\)</span></p></li>
</ul>
</li>
<li><p><em>mixture components</em>: <span class="math notranslate nohighlight">\(p(y|Z^i=1, x, \theta_i)\)</span> ~ different choices</p></li>
<li><p>ex. mixture of linear regressions</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p(y| x, \theta) = \sum_i \underbrace{\pi_i (x, \xi)}_{\text{mixing prop.}} \cdot \underbrace{\mathcal{N}(y|\beta_i^Tx, \sigma_i^2)}_{\text{mixture comp.}}\)</span></p></li>
</ul>
</li>
<li><p>ex. mixtures of logistic regressions</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p(y|x, \theta_i) = \underbrace{\pi_i (x, \xi)}_{\text{mixing prop.}} \cdot \underbrace{\mu(\theta_i^Tx)^y\cdot[1-\mu(\theta_i^Tx)]^{1-y}}_{\text{mixture comp.}}\)</span> where <span class="math notranslate nohighlight">\(\mu\)</span> is the logistic function</p></li>
</ul>
</li>
</ul>
</li>
<li><p>also, nonlinear optimization for this (including EM)</p></li>
</ul>
</section>
</section>
<section id="dim-reduction">
<h2><span class="section-number">3.1.2. </span>dim reduction<a class="headerlink" href="#dim-reduction" title="Link to this heading">#</a></h2>
<p>In general there is some tension between preserving global properties (e.g. PCA) and local peroperties (e.g. nearest neighborhoods)</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Analysis objective</p></th>
<th class="head"><p>Temporal smoothing</p></th>
<th class="head"><p>Explicit noise model</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PCA</p></td>
<td><p>Covariance</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p>orthogonality</p></td>
</tr>
<tr class="row-odd"><td><p>FA</p></td>
<td><p>Covariance</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>like PCA, but with errors (not biased by variance)</p></td>
</tr>
<tr class="row-even"><td><p>LDS/GPFA</p></td>
<td><p>Dynamics</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>NLDS</p></td>
<td><p>Dynamics</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>LDA</p></td>
<td><p>Classification</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Demixed</p></td>
<td><p>Regression</p></td>
<td><p>No</p></td>
<td><p>Yes/No</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Isomap/LLE</p></td>
<td><p>Manifold discovery</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>T-SNE</p></td>
<td><p>‚Ä¶.</p></td>
<td><p>‚Ä¶.</p></td>
<td><p>‚Ä¶</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>UMAP</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p>NMF - <span class="math notranslate nohighlight">\(\min_{D \geq 0, A \geq 0} \|\|X-DA\|\|_F^2\)</span></p>
<ul>
<li><p>SEQNMF</p></li>
</ul>
</li>
<li><p>LDA/QDA - finds basis that separates classes</p>
<ul>
<li><p>reduced to axes which separate classes (perpendicular to the boundaries)</p></li>
</ul>
</li>
<li><p>K-means - can be viewed as a linear decomposition</p></li>
</ul>
<section id="spectral-clustering">
<h3><span class="section-number">3.1.2.1. </span>spectral clustering<a class="headerlink" href="#spectral-clustering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>spectral</em> clustering - does dim reduction on eigenvalues (spectrum) of similarity matrix before clustering in few dims</p>
<ul>
<li><p>uses adjacency matrix</p></li>
<li><p>basically like PCA then k-means</p></li>
<li><p>performs better with regularization - add small constant to the adjacency matrix</p></li>
</ul>
</li>
</ul>
</section>
<section id="pca">
<h3><span class="section-number">3.1.2.2. </span>pca<a class="headerlink" href="#pca" title="Link to this heading">#</a></h3>
<ul>
<li><p>want new set of axes (linearly combine original axes) in the direction of greatest variability</p>
<ul class="simple">
<li><p>this is best for visualization, reduction, classification, noise reduction</p></li>
<li><p>assume <span class="math notranslate nohighlight">\(X\)</span> (nxp) has zero mean</p></li>
</ul>
</li>
<li><p>derivation:</p>
<ul class="simple">
<li><p>minimize variance of X projection onto a unit vector v</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{n} \sum (x_i^Tv)^2 = \frac{1}{n}v^TX^TXv\)</span> subject to <span class="math notranslate nohighlight">\(v^T v=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\implies v^T(X^TXv-\lambda v)=0\)</span>: solution is achieved when <span class="math notranslate nohighlight">\(v\)</span> is eigenvector corresponding to largest eigenvalue</p></li>
</ul>
</li>
<li><p>like minimizing perpendicular distance between data points and subspace onto which we project</p></li>
</ul>
</li>
<li><p>SVD: let <span class="math notranslate nohighlight">\(U D V^T = SVD(Cov(X))\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(X) = \frac{1}{n}X^TX\)</span>, where X has been demeaned</p></li>
</ul>
</li>
<li><p>equivalently, eigenvalue decomposition of covariance matrix <span class="math notranslate nohighlight">\(\Sigma = X^TX\)</span></p>
<ul class="simple">
<li><p>each eigenvalue represents prop. of explained variance: <span class="math notranslate nohighlight">\(\sum \lambda_i = tr(\Sigma) = \sum Var(X_i)\)</span></p></li>
<li><p><em>screeplot</em>  - eigenvalues in decreasing order, look for num dims with kink</p>
<ul>
<li><p>don‚Äôt automatically center/normalize, especially for positive data</p></li>
</ul>
</li>
</ul>
</li>
<li><p>SVD is easier to solve than eigenvalue decomposition, can also solve other ways</p>
<ol class="arabic simple">
<li><p>multidimensional scaling (MDS)</p></li>
</ol>
<ul class="simple">
<li><p>based on eigenvalue decomposition</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>adaptive PCA</p></li>
</ol>
<ul class="simple">
<li><p>extract components sequentially, starting with highest variance so you don‚Äôt have to extract them all</p></li>
</ul>
</li>
<li><p>good PCA code: <a class="reference external" href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="c1">## zero-center data (nxd)</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">## get cov. matrix (dxd)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span> <span class="c1">## compute svd, (all dxd)</span>
<span class="n">Xrot_reduced</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="c1">## project onto first 2 dimensions (n x 2)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>nonlinear pca</p>
<ul>
<li><p>usually uses an auto-associative neural network</p></li>
</ul>
</li>
</ul>
</section>
<section id="topic-modeling">
<h3><span class="section-number">3.1.2.3. </span>topic modeling<a class="headerlink" href="#topic-modeling" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>similar, try to discover topics in a model (which maybe can be linearly combined to produce the original document)</p></li>
<li><p>ex. LDA - generative model: posits that each document is a mixture of a <strong>small number of topics</strong> and that <strong>each word‚Äôs presence is attributable to one of the document‚Äôs topics</strong></p></li>
</ul>
</section>
<section id="sparse-coding-sparse-dictionary-learning">
<h3><span class="section-number">3.1.2.4. </span>sparse coding = sparse dictionary learning<a class="headerlink" href="#sparse-coding-sparse-dictionary-learning" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\underset {\mathbf{D}} \min \underset t \sum \underset {\mathbf{a^{(t)}}} \min ||\mathbf{x^{(t)}} - \mathbf{Da^{(t)}}||_2^2 + \lambda ||\mathbf{a^{(t)}}||_1\]</div>
<ul class="simple">
<li><p>D is like autoencoder output weight matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(a\)</span> is more complicated - requires solving inner minimization problem</p></li>
<li><p>outer loop is not quite lasso - weights are not what is penalized</p></li>
<li><p>impose norm <span class="math notranslate nohighlight">\(D\)</span> not too big</p></li>
<li><p>algorithms</p>
<ul>
<li><p>thresholding (simplest) - do <span class="math notranslate nohighlight">\(D^Ty\)</span> and then threshold this</p></li>
<li><p>basis pursuit - change <span class="math notranslate nohighlight">\(l_0\)</span> to <span class="math notranslate nohighlight">\(l_1\)</span></p>
<ul>
<li><p>this will work under certain conditions (with theoretical guarantees)</p></li>
</ul>
</li>
<li><p>matching purusuit - greedy, find support one at a time, then look for the next one</p></li>
</ul>
</li>
</ul>
</section>
<section id="ica">
<h3><span class="section-number">3.1.2.5. </span>ica<a class="headerlink" href="#ica" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>overview</p>
<ul>
<li><p>remove correlations and higher order dependence</p></li>
<li><p>all components are equally important</p></li>
<li><p>like PCA, but instead of the dot product between components being 0, the mutual info between components is 0</p></li>
<li><p>goals</p>
<ul>
<li><p>minimize statistical dependence between components</p></li>
<li><p>maximize information transferred in a network of non-linear units</p></li>
<li><p>uses information theoretic unsupervised learning rules for neural networks</p></li>
</ul>
</li>
<li><p>problem - doesn‚Äôt rank features for us</p></li>
</ul>
</li>
<li><p>goal: want to decompose <span class="math notranslate nohighlight">\(X\)</span> into <span class="math notranslate nohighlight">\(z\)</span>, where we assume <span class="math notranslate nohighlight">\(X = Az\)</span></p>
<ul>
<li><p>assumptions</p>
<ul>
<li><p>independence: <span class="math notranslate nohighlight">\(P(z) = \prod_i P(z_i)\)</span></p></li>
<li><p>non-gaussianity of <span class="math notranslate nohighlight">\(z\)</span></p></li>
</ul>
</li>
<li><p>2 ways to get <span class="math notranslate nohighlight">\(z\)</span> that matches these assumptions</p>
<ol class="arabic simple">
<li><p>maximize non-gaussianity of <span class="math notranslate nohighlight">\(z\)</span> - use kurtosis, negentropy</p></li>
<li><p>minimize mutual info between components of <span class="math notranslate nohighlight">\(z\)</span> - use KL, max entropy</p>
<ol class="arabic simple">
<li><p>often equivalent</p></li>
</ol>
</li>
</ol>
</li>
<li><p>identifiability: <span class="math notranslate nohighlight">\(z\)</span> is identifiable up to a permutation and scaling of sources when</p>
<ul>
<li><p>at most one of the sources <span class="math notranslate nohighlight">\(z_k\)</span> is gaussian</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span> is full-rank</p></li>
</ul>
</li>
</ul>
</li>
<li><p>ICA learns components which are completely independent, whereas PCA learns orthogonal components</p></li>
<li><p><strong>non-linear ica</strong>: <span class="math notranslate nohighlight">\(X \approx f(z)\)</span>, where assumptions on <span class="math notranslate nohighlight">\(z\)</span> are the same, but <span class="math notranslate nohighlight">\(f\)</span> can be nonlinear</p>
<ul>
<li><p>to obtain identifiability, we need to restrict <span class="math notranslate nohighlight">\(f\)</span> and/or constrain the distr of the sources <span class="math notranslate nohighlight">\(z\)</span></p></li>
</ul>
</li>
<li><p>bell &amp; sejnowski 1995 original formulation (slightly different)</p>
<ul>
<li><p>entropy maximization - try to find a nonlinear function <span class="math notranslate nohighlight">\(g(x)\)</span> which lets you map that distr <span class="math notranslate nohighlight">\(f(x)\)</span> to uniform</p>
<ul>
<li><p>then, that function <span class="math notranslate nohighlight">\(g(x)\)</span> is the cdf of <span class="math notranslate nohighlight">\(f(x)\)</span></p></li>
</ul>
</li>
<li><p>in ICA, we do this for higher dims - want to map distr of <span class="math notranslate nohighlight">\(x_1, ..., x_p\)</span> to <span class="math notranslate nohighlight">\(y_1, ..., y_p\)</span> where distr over <span class="math notranslate nohighlight">\(y_i\)</span>‚Äôs is uniform (implying that they are independent)</p>
<ul>
<li><p>additionally we want the map to be information preserving</p></li>
</ul>
</li>
<li><p>mathematically: <span class="math notranslate nohighlight">\(\underset{W} \max I(x; y) = \underset{W} \max H(y)\)</span> since <span class="math notranslate nohighlight">\(H(y|x)\)</span> is zero (there is no randomness)</p>
<ul>
<li><p>assume <span class="math notranslate nohighlight">\(y = \sigma (W x)\)</span> where <span class="math notranslate nohighlight">\(\sigma\)</span> is elementwise</p></li>
<li><p>(then S = WX, <span class="math notranslate nohighlight">\(W=A^{-1}\)</span>)</p></li>
<li><p>requires certain assumptions so that <span class="math notranslate nohighlight">\(p(y)\)</span> is still a distr: <span class="math notranslate nohighlight">\(p(y) = p(x) / |J|\)</span> where J is Jacobian</p></li>
</ul>
</li>
<li><p>learn W via gradient ascent <span class="math notranslate nohighlight">\(\Delta W \propto \partial / \partial W (\log |J|)\)</span></p>
<ul>
<li><p>there is now something faster called fast ICA</p></li>
</ul>
</li>
</ul>
</li>
<li><p>topographic ICA (make nearby coefficient like each other)</p></li>
<li><p>interestingly, some types of self-supervised learning perform ICA assuming certain data structure (e.g. time-contrastive learning (hyvarinen et al. 2016))</p></li>
</ul>
</section>
<section id="topological">
<h3><span class="section-number">3.1.2.6. </span>topological<a class="headerlink" href="#topological" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>multidimensional scaling (MDS)</strong></p>
<ul>
<li><p>given a a distance matrix, MDS tries to recover low-dim coordinates s.t. distances are preserved</p></li>
<li><p>minimizes goodness-of-fit measure called <em>stress</em> = <span class="math notranslate nohighlight">\(\sqrt{\sum (d_{ij} - \hat{d}_{ij})^2 / \sum d_{ij}^2}\)</span></p></li>
<li><p>visualize in low dims the similarity between individial points in high-dim dataset</p></li>
<li><p>classical MDS assumes Euclidean distances and uses eigenvalues</p>
<ul>
<li><p>constructing configuration of n points using distances between n objects</p></li>
<li><p>uses distance matrix</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d_{rr} = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_{rs} \geq 0\)</span></p></li>
</ul>
</li>
<li><p>solns are invariant to translation, rotation, relfection</p></li>
<li><p>solutions types</p>
<ol class="arabic simple">
<li><p>non-metric methods - use rank orders of distances</p>
<ul>
<li><p>invariant to uniform expansion / contraction</p></li>
</ul>
</li>
<li><p>metric methods - use values</p></li>
</ol>
</li>
<li><p>D is <em>Euclidean</em> if there exists points s.t. D gives interpoint Euclidean distances</p>
<ul>
<li><p>define B = HAH</p>
<ul>
<li><p>D Euclidean iff B is psd</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>t-sne</strong> preserves pairwise neighbors</p>
<ul>
<li><p><a class="reference external" href="https://distill.pub/2016/misread-tsne/">t-sne tutorial</a></p></li>
<li><p>t-sne tries to match pairwise distances between the original data and the latent space data: <img alt="Screen Shot 2020-09-11 at 12.35.35 AM" src="../../_images/tsne.png" /></p></li>
<li><p>original data</p>
<ul>
<li><p>distances are converted to probabilities by assuming points are means of Gaussians, then normalizing over all pairs</p>
<ul>
<li><p>variance of each Gaussian is scaled depending on the desired perplexity</p></li>
</ul>
</li>
</ul>
</li>
<li><p>latent data</p>
<ul>
<li><p>distances are calculated using some kernel function</p>
<ul>
<li><p>t-SNE uses heavy-tailed Student‚Äôs t-distr kernel (van der Maaten &amp; Hinton, 2008)</p></li>
<li><p>SNE use Gausian kernel (Hinton &amp; Roweis, 2003)</p></li>
</ul>
</li>
<li><p>kernels have some parameters that can be picked or learned</p></li>
<li><p><strong>perplexity</strong> - how to balance between local/global aspects of data</p></li>
</ul>
</li>
<li><p>optimization - for optimization purposes, this can be decomposed into attractive/repulsive forces</p></li>
</ul>
</li>
<li><p><strong>umap</strong>: Uniform Manifold Approximation and Projection for Dimension Reduction</p>
<ul>
<li><p><a class="reference external" href="https://umap-learn.readthedocs.io/en/latest/how_umap_works.html">umap tutorial</a></p></li>
</ul>
</li>
<li><p><strong>pacmap</strong></p>
<ul>
<li><p><img alt="pacmap" src="../../_images/pacmap.png" /></p></li>
</ul>
</li>
</ul>
</section>
<section id="misc">
<h3><span class="section-number">3.1.2.7. </span>misc<a class="headerlink" href="#misc" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Sparse Component Analysis (<a class="reference external" href="https://www.biorxiv.org/content/10.1101/2024.02.05.578988v1.full.pdf">zimnik‚Ä¶cunningham, paninski, churchland, &amp; glaser, 2024</a>)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\arg \min _{U, V}\left(\|W(X-X U V)\|_F^2+\lambda_{\text {sparse }}\|X U\|_1+\lambda_{\text {orth }}\left\|V V^{\top}-I\right\|_F^2\right)\)</span></p>
<ul>
<li><p>where <span class="math notranslate nohighlight">\(U\)</span> is encoding matrix and <span class="math notranslate nohighlight">\(V\)</span> is decoding, the final loss term is imposing orthogonality of the columns of V</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.08212">NNK-Means: Dictionary Learning using Non-Negative Kernel regression</a> (shekkizhar &amp; ortega, 2021)</p>
<ul>
<li><p>data summarization - represent large datasets by a small set of elements (e.g. k-means)</p></li>
<li><p>here, use dictionary learning instead of k-means to summarize data</p>
<ul>
<li><p>each dictionary element is a sparse combination of inputs</p></li>
<li><p>use non-negative kernel regesion (NNK) to measure distances when designing the dictionary (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9054425/">shekkizar &amp; ortega, 2020</a>)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="generative-models">
<h2><span class="section-number">3.1.3. </span>generative models<a class="headerlink" href="#generative-models" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>overview: <a class="reference external" href="https://blog.openai.com/generative-models/">https://blog.openai.com/generative-models/</a></p></li>
<li><p>notes for <a class="reference external" href="https://sites.google.com/view/berkeley-cs294-158-sp20/home">deep unsupervised learning</a></p></li>
<li><p>MLE equivalent to minimizing KL for density estimation:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\min_\theta KL(p|| p_\theta) =\\ \min_\theta-H(p) + \mathbb E_{x\sim p}[-\log p_\theta(x)] \\ \max_\theta E_p[\log p_\theta(x)]\)</span></p></li>
</ul>
</li>
</ul>
<section id="autoregressive-models">
<h3><span class="section-number">3.1.3.1. </span>autoregressive models<a class="headerlink" href="#autoregressive-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>model input based on input</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p(x_1)\)</span> is a histogram (learned prior)</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x_2|x_1)\)</span> is a distr. ouptut by a neural net (output is logits, followed by softmax)</p></li>
<li><p>all conditional distrs. can be given by neural net</p></li>
</ul>
</li>
<li><p>can model using an RNN: e.g. char-rnn (karpathy, 2015): <span class="math notranslate nohighlight">\(\log p(x) - \sum_i \log p(x_i | x_{1:i-1})\)</span>, where each <span class="math notranslate nohighlight">\(x_i\)</span> is a character</p></li>
<li><p>can also use masks</p>
<ul>
<li><p>masked autoencoder for distr. estimation - mask some weights so that autoencoder output is a factorized distr.</p>
<ul>
<li><p>pick an odering for the pixels to be conditioned on</p></li>
</ul>
</li>
<li><p>ex. 1d masked convolution on wavenet (use past points to predict future points)</p></li>
<li><p>ex. pixelcnn - use masking for pixels to the topleft</p></li>
<li><p>ex. gated pixelcnn - fixes issue with blindspot</p></li>
<li><p>ex. pixelcnn++ - nearby pixel values are likely to cooccur</p></li>
<li><p>ex. pixelSNAIL - uses attention and can get wider receptive field</p></li>
<li><p><strong>attention:</strong><span class="math notranslate nohighlight">\(A(q, K, V) = \sum_i \frac{\exp(q \cdot k_i)}{\sum_j \exp (q \cdot k_j)} v_i\)</span></p>
<ul>
<li><p>masked attention can be more flexible than masked convolution</p></li>
</ul>
</li>
<li><p>can do super resolution, hierarchical autoregressive model</p></li>
</ul>
</li>
<li><p>problems</p>
<ul>
<li><p>slow - have to sample each pixel (can speed up by caching activations)</p>
<ul>
<li><p>can also speed up by assuming some pixels conditionally independent</p></li>
</ul>
</li>
</ul>
</li>
<li><p>hard to get a latent reprsentation</p>
<ul>
<li><p>can use <strong>Fisher score</strong> <span class="math notranslate nohighlight">\(\nabla_\theta \log p_\theta (x)\)</span></p></li>
</ul>
</li>
</ul>
<section id="flow-models">
<h4><span class="section-number">3.1.3.1.1. </span>flow models<a class="headerlink" href="#flow-models" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>good intro to implementing invertible neural networks: <a class="reference external" href="https://hci.iwr.uni-heidelberg.de/vislearn/inverse-problems-invertible-neural-networks/">https://hci.iwr.uni-heidelberg.de/vislearn/inverse-problems-invertible-neural-networks/</a></p>
<ul>
<li><p>input / output dimension need to have same dimension</p></li>
<li><p>we can get around this by padding one of the dimensions with noise variables (and we might want to penalize these slightly during training)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1908.09257.pdf">normalizing flows</a></p></li>
<li><p>ultimate goal: a likelihood-based model with</p>
<ul>
<li><p>fast sampling</p></li>
<li><p>fast inference (evaluating the likelihood)</p></li>
<li><p>fast training</p></li>
<li><p>good samples</p></li>
<li><p>good compression</p></li>
</ul>
</li>
<li><p>transform some <span class="math notranslate nohighlight">\(p(x)\)</span> to some <span class="math notranslate nohighlight">\(p(z)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \to z = f_\theta (x)\)</span>, where <span class="math notranslate nohighlight">\(z \sim p_Z(z)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_\theta (x) dx = p(z)dz\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p_\theta(x) = p(f_\theta(x))|\frac {\partial f_\theta (x)}{\partial x}|\)</span></p></li>
</ul>
</li>
<li><p>autoregressive flows</p>
<ul>
<li><p>map <span class="math notranslate nohighlight">\(x\to z\)</span> invertible</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \to z\)</span> is same as log-likelihood computation</p></li>
<li><p><span class="math notranslate nohighlight">\(z\to x\)</span> is like sampling</p></li>
</ul>
</li>
<li><p>end up being as deep as the number of variables</p></li>
</ul>
</li>
<li><p>realnvp (dinh et al. 2017) - can couple layers to preserve invertibility but still be tractable</p>
<ul>
<li><p>downsample things and have different latents at different spatial scales</p></li>
</ul>
</li>
<li><p>other flows</p>
<ul>
<li><p>flow++</p></li>
<li><p>glow</p></li>
<li><p>FFJORD - continuous time flows</p></li>
</ul>
</li>
<li><p>discrete data can be harder to model</p>
<ul>
<li><p><strong>dequantization</strong> - add noise (uniform) to discrete data</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="vaes">
<h3><span class="section-number">3.1.3.2. </span>vaes<a class="headerlink" href="#vaes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">intuitively understanding vae</a></p></li>
<li><p><a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">VAE tutorial</a></p>
<ul>
<li><p>minimize <span class="math notranslate nohighlight">\(\mathbb E_{q_\phi(z|x)}[\log p_\theta(x|z)- D_{KL}(q_\phi(z|x)\:||\:p(z))]\)</span></p>
<ul>
<li><p>want latent <span class="math notranslate nohighlight">\(z\)</span> to be standard normal - keeps the space smooth</p></li>
</ul>
</li>
<li><p>hard to directly calculate <span class="math notranslate nohighlight">\(p(z|x)\)</span>, since it includes <span class="math notranslate nohighlight">\(p(x)\)</span>, so we approximate it with the variational posterior <span class="math notranslate nohighlight">\(q_\phi (z|x)\)</span>, which we assume to be Gaussian</p></li>
<li><p>goal: <span class="math notranslate nohighlight">\(\text{argmin}_\phi KL(q_\phi(z|x) \:|| \:p(z|x))\)</span></p>
<ul>
<li><p>still don‚Äôt have acess to <span class="math notranslate nohighlight">\(p(x)\)</span>, so rewrite <span class="math notranslate nohighlight">\(\log p(x) = ELBO(\phi) + KL(q_\phi(z|x) \: || \: p(z|x))\)</span></p></li>
<li><p>instead of minimizing <span class="math notranslate nohighlight">\(KL\)</span>, we can just maximize the <span class="math notranslate nohighlight">\(ELBO=\mathbb E_q [\log p(x, z)] - \mathbb E_q[\log q_\phi (z|x)]\)</span></p></li>
</ul>
</li>
<li><p><strong>mean-field variational inference</strong> - each point has its own params (e.g. different encoder DNN) vs <strong>amortized inference</strong> - same encoder for all points</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://pyro.ai/examples/vae.html">pyro explanation</a></p>
<ul>
<li><p>want large evidence <span class="math notranslate nohighlight">\(\log p_\theta (\mathbf x)\)</span> (means model is a good fit to the data)</p></li>
<li><p>want good fit to the posterior <span class="math notranslate nohighlight">\(q_\phi(z|x)\)</span></p></li>
</ul>
</li>
<li><p>just an autoencoder where the middle hidden layer is supposed to be unit gaussian</p>
<ul>
<li><p>add a kl loss to measure how well it matches a unit gaussian</p>
<ul>
<li><p>for calculation purposes, encoder actually produces means / vars of gaussians in hidden layer rather than the continuous values‚Ä¶.</p></li>
</ul>
</li>
<li><p>this kl loss is not too complicated‚Ä¶<a class="reference external" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf">https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf</a></p></li>
</ul>
</li>
<li><p>generally less sharp than GANs</p>
<ul>
<li><p>uses mse loss instead of gan loss‚Ä¶</p></li>
<li><p>intuition: vaes put mass between modes while GANs push mass towards modes</p></li>
</ul>
</li>
<li><p>constraint forces the encoder to be very efficient, creating information-rich latent variables. This improves generalization, so latent variables that we either randomly generated, or we got from encoding non-training images, will produce a nicer result when decoded.</p></li>
</ul>
</section>
<section id="gans">
<h3><span class="section-number">3.1.3.3. </span>gans<a class="headerlink" href="#gans" title="Link to this heading">#</a></h3>
<ul>
<li><p>evaluating gans</p>
<ul class="simple">
<li><p>don‚Äôt have explicit objective like likelihood anymore</p></li>
<li><p>kernel density = parzen-window density based on samples yields likelihood</p></li>
<li><p>inception score <span class="math notranslate nohighlight">\(IS(\mathbf x) = \exp(\underbrace{H(\mathbf y)}_{\text{want to generate diversity of classes}} - \underbrace{H(\mathbf y|\mathbf x)}_{\text{each image should be distinctly recognizable}})\)</span></p></li>
<li><p><strong>FID</strong> - Frechet inception score works directly on embedded features from inception v3 model</p>
<ul>
<li><p>embed population of images and calculate mean + variance in embedding space</p></li>
<li><p>measure distance between these means / variances for real/synthetic images using Frechet distance = Wasseterstein-2 distance</p></li>
</ul>
</li>
</ul>
</li>
<li><p>infogan</p>
<ul class="simple">
<li><p><img alt="infogan" src="../../_images/infogan.png" /></p></li>
</ul>
</li>
<li><p>problems</p>
<ul class="simple">
<li><p>mode collapse - pick just one mode in the distr.</p></li>
</ul>
</li>
<li><p>train network to be loss function</p></li>
<li><p>original gan paper (2014)</p></li>
<li><p><em>generative adversarial network</em></p></li>
<li><p>goal: want G to generate distribution that follows data</p>
<ul class="simple">
<li><p>ex. generate good images</p></li>
</ul>
</li>
<li><p>two models</p>
<ul class="simple">
<li><p><em>G</em> - generative</p></li>
<li><p><em>D</em> - discriminative</p></li>
</ul>
</li>
<li><p>G generates adversarial sample x for D</p>
<ul class="simple">
<li><p>G has prior z</p></li>
<li><p>D gives probability p that x comes from data, not G</p>
<ul>
<li><p>like a binary classifier: 1 if from data, 0 from G</p></li>
</ul>
</li>
<li><p><em>adversarial sample</em> - from G, but tricks D to predicting 1</p></li>
</ul>
</li>
<li><p>training goals</p>
<ul class="simple">
<li><p>G wants D(G(z)) = 1</p></li>
<li><p>D wants D(G(z)) = 0</p>
<ul>
<li><p>D(x) = 1</p></li>
</ul>
</li>
<li><p>converge when D(G(z)) = 1/2</p></li>
<li><p>G loss function: <span class="math notranslate nohighlight">\(G = \text{argmin}_G \log(1-D(G(Z))\)</span></p></li>
<li><p>overall <span class="math notranslate nohighlight">\(\min_g \max_D\)</span> log(1-D(G(Z))</p></li>
</ul>
</li>
<li><p>training algorithm</p>
<ul class="simple">
<li><p>in the beginning, since G is bad, only train  my minimizing G loss function</p></li>
</ul>
</li>
<li><p><strong>projecting into gan latent space (=gan inversion)</strong></p>
<ul class="simple">
<li><p>2 general approaches</p>
<ol class="arabic simple">
<li><p>learn an encoder to go image -&gt; latent space</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2004.00049.pdf">In-Domain GAN Inversion for Real Image Editing</a> (zhu et al. 2020)</p></li>
<li><p>learn encoder to project image into latent space, with regularizer to make sure it follows the right distr.</p></li>
</ul>
</li>
</ol>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p>optimize latent code wrt image directly</p>
<ol class="arabic simple">
<li><p>can also learn an encoder to initialize this optimization</p></li>
</ol>
</li>
<li><p>some work designing GANs that are intrinsically invertible</p></li>
<li><p>stylegan-specific - some works which exploit layer-wise noises</p>
<ol class="arabic simple">
<li><p>stylegan2 paper: optimize w along with noise maps - need to make sure noise maps don‚Äôt include signal</p></li>
</ol>
</li>
</ol>
</li>
</ul>
</section>
<section id="diffusion-energy-based-models">
<h3><span class="section-number">3.1.3.4. </span>diffusion / energy-based models<a class="headerlink" href="#diffusion-energy-based-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>first describe a procedure for gradually turning data into noise</p></li>
<li><p>then train a DNN to invert this procedure step-by-step</p>
<ul>
<li><p>single model handles many different noise levels with shared parameters</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://benanne.github.io/2022/01/31/diffusion.html">blog post</a></p></li>
<li><p>seminal paper: <a class="reference external" href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a> (song &amp; ermon, 2019)</p>
<ul>
<li><p>really started earlier: <a class="reference external" href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a> (sohl-dickstein, ‚Ä¶, ganguli, 2015)</p></li>
</ul>
</li>
<li><p>[Improved Denoising Diffusion Probabilistic Models](<a class="reference external" href="https://arxiv.org/abs/2102.0">https://arxiv.org/abs/2102.0</a> 9672) (2021)</p>
<ul>
<li><p>can make this class-conditionnal by incorporating classifier into the model which inverts the noise</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2105.05233">Diffusion Models Beat GANs on Image Synthesis</a> (2021)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="self-semi-supervised">
<h2><span class="section-number">3.1.4. </span>self/semi-supervised<a class="headerlink" href="#self-semi-supervised" title="Link to this heading">#</a></h2>
<section id="self-supervised">
<h3><span class="section-number">3.1.4.1. </span>self-supervised<a class="headerlink" href="#self-supervised" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>blog: <a class="reference external" href="https://dawn.cs.stanford.edu/2017/07/16/weak-supervision/">https://dawn.cs.stanford.edu/2017/07/16/weak-supervision/</a></p>
<ul>
<li><p>training data is hard to get</p></li>
</ul>
</li>
<li><p>related paper: <a class="reference external" href="https://www.biorxiv.org/content/early/2018/06/16/339630">https://www.biorxiv.org/content/early/2018/06/16/339630</a></p></li>
</ul>
<p><img alt="semi" src="../../_images/semi.png" /></p>
<ul class="simple">
<li><p>basics: predict some part of the input (e.g. present from past, bottom from top, etc.)</p>
<ul>
<li><p>ex. denoising autoencoder</p></li>
<li><p>ex. in-painting (can use adversarial loss)</p></li>
<li><p>ex. colorization, split-brain autoencoder</p>
<ul>
<li><p>colorization in video given first frame (helps learn tracking)</p></li>
</ul>
</li>
<li><p>ex. relative patch prediction</p></li>
<li><p>ex. orientation prediction</p></li>
<li><p>ex. nlp</p>
<ul>
<li><p>word2vec</p></li>
<li><p>bert - predict blank word</p></li>
</ul>
</li>
</ul>
</li>
<li><p>contrastive predictive coding - translates generative modeling into classification</p>
<ul>
<li><p><em>contrastive loss</em> = <em>InfoNCE loss</em> uses cross-entropy loss to measure how well the model can classify the ‚Äúfuture‚Äù representation among a set of unrelated ‚Äúnegative‚Äù samples</p></li>
<li><p>negative samples may be from other batches or other parts of the input</p></li>
</ul>
</li>
<li><p>momentum contrast - queue of previous embeddings are ‚Äúkeys‚Äù</p>
<ul>
<li><p>match new embedding (query) against keys and use contrastive loss</p></li>
<li><p>similar idea as memory bank</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.html">Unsupervised Visual Representation Learning by Context Prediction</a> (efros 15)</p>
<ul>
<li><p>predict relative location of different patches</p></li>
</ul>
</li>
<li><p>SimCLR (<a class="reference external" href="https://arxiv.org/abs/2002.05709">Chen et al, 2020</a>)</p>
<ul>
<li><p>maximize agreement for different points after some augmentation (contrastive loss)</p></li>
</ul>
</li>
</ul>
</section>
<section id="semi-supervised">
<h3><span class="section-number">3.1.4.2. </span>semi-supervised<a class="headerlink" href="#semi-supervised" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>make the classifier more confident</p>
<ul>
<li><p>entropy minimization - try to minimize the entropy of output predictions (like making confident predictions labels)</p></li>
<li><p>pseudo labeling - just take argmax pred as if it were the label</p></li>
</ul>
</li>
<li><p>label consistency with data augmentation</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1905.00546.pdf">Billion-scale semi-supervised learning for image classification</a> (FAIR, 19)</p>
<ul>
<li><p>unsupervised learning + model distillation succeeds on imagenet</p></li>
</ul>
</li>
<li><p>ensembling</p>
<ul>
<li><p>temporal ensembling - ensemble multiple models at different training epochs</p></li>
<li><p>mean teachers - learn from exponential moving average of students</p></li>
</ul>
</li>
<li><p>unsupervised data augmentation - augment and ensure prediction is the same</p></li>
<li><p>distribution alignment - ex. cyclegan - enforce  cycle consistency = dual learning = back translation</p>
<ul>
<li><p>simpler is marginal matching</p></li>
</ul>
</li>
</ul>
</section>
<section id="compression">
<h3><span class="section-number">3.1.4.3. </span>compression<a class="headerlink" href="#compression" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>simplest - fixed-length code</p></li>
<li><p>variable-length code</p>
<ul>
<li><p>could append stop char to each codeword</p></li>
<li><p>general prefix-free code = binary tries</p>
<ul>
<li><p>codeword is path from froot to leaf</p></li>
<li><p><img alt="Screen Shot 2020-04-20 at 8.50.07 PM" src="../../_images/trie.png" /></p></li>
</ul>
</li>
<li><p>huffman code - higher prob = shorter</p></li>
</ul>
</li>
<li><p><strong>arithmetic coding</strong></p>
<ul>
<li><p>motivation: coding one symbol at a time incurs penalty of +1 per symbol - more efficient to encode groups of things</p></li>
<li><p>can be improved with good autoregressive model</p></li>
</ul>
</li>
</ul>
</section>
<section id="contrastive-learning">
<h3><span class="section-number">3.1.4.4. </span>contrastive learning<a class="headerlink" href="#contrastive-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.10243">What makes for good views for contrastive learning</a> (tian et al. 2020)</p>
<ul>
<li><p>how to select views (e.g. transformations we want to be invariant to)?</p></li>
<li><p>reduce the mutual information (MI) between views while keeping task-relevant information intact</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning</a> (khosla et al. 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1905.09272.pdf">Data-Efficient Image Recognition with Contrastive Predictive Coding</a></p>
<ul>
<li><p>pre-training with CPC on ImageNet improves accuracy</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2002.05714.pdf">Automatically Discovering and Learning New Visual Categories with Ranking Statistics</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ml.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>ml</p>
      </div>
    </a>
    <a class="right-next"
       href="structure_ml.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.2. </span>structure learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">3.1.1. clustering</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hierarchical">3.1.1.1. hierarchical</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partitional">3.1.1.2. partitional</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-clustering-j-10">3.1.1.3. statistical clustering (j 10)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-mixture-models-regression-classification-j-10">3.1.1.4. conditional mixture models - regression/classification (j 10)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dim-reduction">3.1.2. dim reduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-clustering">3.1.2.1. spectral clustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">3.1.2.2. pca</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">3.1.2.3. topic modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-coding-sparse-dictionary-learning">3.1.2.4. sparse coding = sparse dictionary learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ica">3.1.2.5. ica</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topological">3.1.2.6. topological</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">3.1.2.7. misc</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-models">3.1.3. generative models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-models">3.1.3.1. autoregressive models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-models">3.1.3.1.1. flow models</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vaes">3.1.3.2. vaes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gans">3.1.3.3. gans</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-energy-based-models">3.1.3.4. diffusion / energy-based models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-semi-supervised">3.1.4. self/semi-supervised</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-supervised">3.1.4.1. self-supervised</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#semi-supervised">3.1.4.2. semi-supervised</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression">3.1.4.3. compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contrastive-learning">3.1.4.4. contrastive learning</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>