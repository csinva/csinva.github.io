
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.9. interpretability</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/research_ovws/ovw_interp';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.10. complexity" href="ovw_complexity.html" />
    <link rel="prev" title="7.8. transfer learning" href="ovw_transfer_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ai/ai.html">1. ai</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ai/knowledge_rep.html">1.1. representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/psychology.html">1.2. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/fairness_sts.html">1.3. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/philosophy.html">1.4. philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/ai_futures.html">1.5. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/cogsci.html">1.6. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/llms.html">1.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/logic.html">1.8. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/search.html">1.9. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/decisions_rl.html">1.10. decisions, rl</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/math.html">2. math</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/linear_algebra.html">2.1. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/optimization.html">2.2. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/differential_equations.html">2.3. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/chaos.html">2.4. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/math_basics.html">2.5. math basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/signals.html">2.6. signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/calculus.html">2.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/proofs.html">2.8. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/analysis.html">2.9. real analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml/ml.html">3. ml</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml/unsupervised.html">3.1. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/structure_ml.html">3.2. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/learning_theory.html">3.3. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/deep_learning.html">3.4. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/comp_vision.html">3.5. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/kernels.html">3.6. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/nlp.html">3.7. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/feature_selection.html">3.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/evaluation.html">3.9. evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/classification.html">3.10. classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stat/stat.html">4. stat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stat/time_series.html">4.1. time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/graphical_models.html">4.2. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/causal_inference.html">4.3. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/game_theory.html">4.4. game theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/data_analysis.html">4.7. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/testing.html">4.8. testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">5. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">5.1. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">5.2. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">5.3. development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">5.4. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">5.5. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">5.6. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">5.7. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">5.8. brain basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">6. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">6.1. cs theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">6.2. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">6.3. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">6.4. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">6.5. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">6.6. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">6.7. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">6.8. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">6.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">6.10. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">6.11. reproducibility</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="research_ovws.html">7. research_ovws</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ovw_disentanglement.html">7.1. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_uncertainty.html">7.2. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_generalization.html">7.3. generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_causal_inference.html">7.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_omics.html">7.5. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_ml_medicine.html">7.6. ml in medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_llms.html">7.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_transfer_learning.html">7.8. transfer learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.9. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_complexity.html">7.10. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_dl_theory.html">7.11. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_scat.html">7.12. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_interesting_science.html">7.13. interesting science</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/research_ovws/ovw_interp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>interpretability</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-interpretability">7.9.1. evaluating interpretability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-centric">7.9.1.1. human-centric</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-weaknesses-sanity-checks">7.9.1.2. interpretation weaknesses &amp; sanity checks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intrinsic-interpretability-i-e-how-can-we-fit-a-simpler-model-transparent-models">7.9.2. intrinsic interpretability (i.e. how can we fit a simpler model) = transparent models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-overview">7.9.2.1. decision rules overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-sets">7.9.2.1.1. rule sets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-lists">7.9.2.1.2. rule lists</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trees">7.9.2.1.3. trees</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-diagrams">7.9.2.1.4. decision diagrams</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebraic-models">7.9.2.2. linear (+algebraic) models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gams-generalized-additive-models">7.9.2.2.1. gams (generalized additive models)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supersparse-models">7.9.2.2.2. supersparse models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-regression">7.9.2.2.3. symbolic regression</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-neural-nets">7.9.2.3. interpretable neural nets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#concepts">7.9.2.3.1. concepts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#localization">7.9.2.3.2. localization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-based-case-based-e-g-prototypes-nearest-neighbor">7.9.2.3.3. example-based = case-based (e.g. prototypes, nearest neighbor)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-dnns-and-rules">7.9.2.3.4. connecting dnns and rules</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-models-e-g-monotonicity">7.9.2.4. constrained models (e.g. monotonicity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misc-models">7.9.2.5. misc models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models">7.9.2.5.1. bayesian models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#programs">7.9.2.5.2. programs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posthoc-interpretability-i-e-how-can-we-interpret-a-fitted-model">7.9.3. posthoc interpretability (i.e. how can we interpret a fitted model)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-agnostic">7.9.3.1. model-agnostic</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-importances-vis">7.9.3.1.1. variable importances (VIs)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-curves">7.9.3.1.2. importance curves</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-ensembles">7.9.3.2. tree ensembles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-dnns">7.9.3.3. neural networks (dnns)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-visualization">7.9.3.3.1. dnn visualization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-concept-based-explanations">7.9.3.3.2. dnn concept-based explanations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-causal-motivated-attribution">7.9.3.3.3. dnn causal-motivated attribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-feature-importance">7.9.3.3.4. dnn feature importance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-language-models-transformers">7.9.3.3.5. dnn language models / transformers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions">7.9.3.4. interactions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-agnostic-interactions">7.9.3.4.1. model-agnostic interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-based-interactions">7.9.3.4.2. tree-based interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-interactions">7.9.3.4.3. dnn interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-interactions">7.9.3.4.4. linear interactions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-influential-examples">7.9.3.5. finding influential examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-summarization-distillation">7.9.3.6. model summarization / distillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-problems-perspectives">7.9.4. different problems / perspectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-models">7.9.4.1. improving models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-in-the-loop-hitl">7.9.4.2. human-in-the-loop (HITL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-automl">7.9.4.3. (interpretable) Automl</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recourse">7.9.4.4. recourse</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interp-for-rl">7.9.4.5. interp for rl</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-over-sets-perturbations">7.9.4.6. interpretation over sets / perturbations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-safety">7.9.4.7. ai safety</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><strong>Some interesting papers on interpretable machine learning, largely organized based on this <a class="reference external" href="https://arxiv.org/abs/1901.04592">interpretable ml review</a> (murdoch et al. 2019) and notes from this <a class="reference external" href="https://christophm.github.io/interpretable-ml-book/">interpretable ml book</a> (molnar 2019). For interpretability specific to transformers, see <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transformers.html">here</a></strong>.</p>
<section class="tex2jax_ignore mathjax_ignore" id="interpretability">
<h1><span class="section-number">7.9. </span>interpretability<a class="headerlink" href="#interpretability" title="Link to this heading">#</a></h1>
<p>The definition of interpretability I like most is that given in <a class="reference external" href="https://arxiv.org/abs/1901.04592">murdoch et al. 2019</a>, which states that  interpretability needs to be grounded in order to be useful. Specifically, interpretability is only defined with respect to a specific audience + problem and an interpretation should be evaluated in terms of how well it benefits that context.</p>
<p><img alt="" src="https://csinva.io/notes/cheat_sheets/interp.svg" /></p>
<details>
  <summary>Useful overview figures from the papers below</summary>
	<p>
	<img class="medium_image" src="../assets/black_box_explainers.png"/>
	<img class="medium_image" src="../assets/black_box_explainers_legend.png"/>
	<img class="medium_image" src="../assets/explainers_table.png"/>
	<img class="medium_image" src="../assets/vims.png"/>
</p>
</details>
<ul class="simple">
<li><p>Feature (variable) importance measurement review (VIM) (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0951832015001672">wei et al. 2015</a> )</p>
<ul>
<li><p>often-termed sensitivity, contribution, or impact</p></li>
<li><p>some of these can be applied to data directly w/out model (e.g. correlation coefficient, rank correlation coefficient, moment-independent VIMs)</p></li>
</ul>
</li>
<li><p>Towards a Generic Framework for Black-box Explanation Methods (<a class="reference external" href="https://hal.inria.fr/hal-02131174v2/document">henin &amp; metayer 2019</a>)</p>
<ul>
<li><p>sampling - selection of inputs to submit to the system to be explained</p></li>
<li><p>generation - analysis of links between selected inputs and corresponding outputs to generate explanations</p>
<ol class="arabic simple">
<li><p><em>proxy</em> - approximates model (ex. rule list, linear model)</p></li>
<li><p><em>explanation generation</em> - explains the proxy (ex. just give most important 2 features in rule list proxy, ex. LIME gives coefficients of linear model, Shap: sums of elements)</p></li>
</ol>
</li>
<li><p>interaction (with the user)</p></li>
<li><p>this is a super useful way to think about explanations (especially local), but doesn’t work for SHAP / CD which are more about how much a variable contributes rather than a local approximation</p></li>
</ul>
</li>
<li><p>Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges (<a class="reference external" href="https://arxiv.org/abs/2103.11251">rudin et al. ‘21</a>) - emphasizes building glass-box models</p></li>
</ul>
<section id="evaluating-interpretability">
<h2><span class="section-number">7.9.1. </span>evaluating interpretability<a class="headerlink" href="#evaluating-interpretability" title="Link to this heading">#</a></h2>
<p>Evaluating interpretability is often fraught (largely because it rarely makes sense to talk about interpretability outside of a specific context, so ML conference papers instead evaluate whatever proxy makes their method look good). The best possible evaluation of interpretability requires benchmarking it with respect to the relevant audience in a context. For example, if an interpretation claims to help understand radiology models, it should be tested based on how well it helps radiologists when actually making diagnoses. The papers here try to find more generic alternative ways to evaluate interp methods (or just define desiderata to do so).</p>
<ul>
<li><p>Towards A Rigorous Science of Interpretable Machine Learning (<a class="reference external" href="https://arxiv.org/pdf/1702.08608.pdf">doshi-velez &amp; kim 2017</a>)</p>
<ul>
<li><div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-left"><p>Humans</p></th>
<th class="head text-left"><p>Tasks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Application-grounded Evaluation</p></td>
<td class="text-left"><p>Real  Humans</p></td>
<td class="text-left"><p>Real Tasks</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Human-grounded Evaluation</p></td>
<td class="text-left"><p>Real Humans</p></td>
<td class="text-left"><p>Simple Tasks</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Functionally-grounded Evaluation</p></td>
<td class="text-left"><p>No Real Humans</p></td>
<td class="text-left"><p>Proxy Tasks</p></td>
</tr>
</tbody>
</table>
</div>
</li>
</ul>
</li>
<li><p>Feature importance</p>
<ul>
<li><p>Benchmarking Attribution Methods with Relative Feature Importance (<a class="reference external" href="https://arxiv.org/abs/1907.09701">yang &amp; kim 2019</a>) - train a classifier, add random stuff (like dogs) to the image, classifier should assign them little importance</p>
<p>Visualizing the Impact of Feature Attribution Baselines (<a class="reference external" href="https://distill.pub/2020/attribution-baselines/">sturmfels, lundberg, &amp; lee, 2020</a>)</p>
<ul class="simple">
<li><p>top-k-ablation: should identify top pixels, ablate them, and want it to actually decrease</p></li>
<li><p>center-of-mass ablation: also could identify center of mass of saliency map and blur a box around it (to avoid destroying feature correlations in the model)</p></li>
<li><p>should we be true-to-the-model or true-to-the-data?</p></li>
</ul>
</li>
<li><p>Evaluating Feature Importance Estimates (<a class="reference external" href="https://arxiv.org/abs/1806.10758">hooker et al. 2019</a>)</p>
<ul class="simple">
<li><p>remove-and-retrain test accuracy decrease</p></li>
</ul>
</li>
<li><p>Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the Performance of Explainability Algorithms (<a class="reference external" href="https://arxiv.org/abs/1910.07387">lin et al. 2019</a>)</p></li>
</ul>
</li>
<li><p>Misc</p>
<ul class="simple">
<li><p>Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition (<a class="reference external" href="https://arxiv.org/pdf/1904.03867.pdf">molnar 2019</a>)</p></li>
<li><p>Evaluating Explanation Without Ground Truth in Interpretable Machine Learning (<a class="reference external" href="https://arxiv.org/pdf/1907.06831.pdf">yang et al. 2019</a>)</p>
<ul>
<li><p>predictability (does the knowledge in the explanation generalize well)</p></li>
<li><p>fidelity (does explanation reflect the target system well)</p></li>
<li><p>persuasibility (does human satisfy or comprehend explanation well)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="human-centric">
<h3><span class="section-number">7.9.1.1. </span>human-centric<a class="headerlink" href="#human-centric" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning (<a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3313831.3376219">kaur, …, caruana, wallach, vaughan, 2020</a>)</p>
<ul>
<li><p>used contextual inquiry + survey of data scientists (SHAP &amp; InterpretML GAMs)</p></li>
<li><p>results indicate that data scientists over-trust and misuse interpretability tools</p></li>
<li><p>few of our participants were able to accurately describe the visualizations output by these tools</p></li>
</ul>
</li>
<li><p>An Evaluation of the Human-Interpretability of Explanation (<a class="reference external" href="https://arxiv.org/pdf/1902.00006.pdf">lage…been kim, gershman, doshi-velez 2019</a>) - controlled human experiments as a few key aspects (explanation length, number of concepts, repeated terms) are varied</p></li>
<li><p>Manipulating and Measuring Model Interpretability (<a class="reference external" href="https://arxiv.org/abs/1802.07810">sangdeh … vaughan, wallach 2019</a>)</p>
<ul>
<li><p>participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions</p></li>
<li><p>no improvements in the degree to which participants followed the model’s predictions when it was beneficial to do so.</p></li>
<li><p>increased transparency hampered people’s ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload</p></li>
</ul>
</li>
<li><p>On Evaluating Explanation Utility for Human-AI Decision-Making in NLP (<a class="reference external" href="https://openreview.net/pdf?id=8BR8EaWNTZ">chaleshtori, ghoshal, marasovic, 2023</a>) - different human evaluations of NLP explanations have succeeded in different ways (meta-analysis)</p></li>
<li><p>Evaluating the Utility of Model Explanations for Model Development (<a class="reference external" href="https://arxiv.org/abs/2312.06032">im, andreas, &amp; zhao, 2023</a>)</p></li>
</ul>
</section>
<section id="interpretation-weaknesses-sanity-checks">
<h3><span class="section-number">7.9.1.2. </span>interpretation weaknesses &amp; sanity checks<a class="headerlink" href="#interpretation-weaknesses-sanity-checks" title="Link to this heading">#</a></h3>
<ul>
<li><p>Sanity Checks for Saliency Maps (<a class="reference external" href="https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf">adebayo et al. 2018</a>)- test what happens when randomizing model parameters - attributions should be different for trained vs random model, but they aren’t for many attribution methods</p></li>
<li><p>Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability (<a class="reference external" href="https://openreview.net/forum?id=dYeAHXnpWJ4">srinivas &amp; fleuret, 2021</a>)</p>
<ul class="simple">
<li><p>logits can be arbitrarily shifted without affecting preds / gradient-based explanations</p></li>
<li><p>gradient-based explanations then, don’t necessarily capture info about <span class="math notranslate nohighlight">\(p_\theta(y|x)\)</span></p></li>
</ul>
</li>
<li><p>Interpretation of Neural Networks is Fragile (<a class="reference external" href="https://arxiv.org/abs/1710.10547">ghorbani et al. 2018</a>)</p>
<ul>
<li><p>minor perturbations to inputs can drastically change DNN interpretations</p></li>
<li><details>
<summary>Intuition figure</summary>
<p>	<img class="medium_image" src="../assets/Screenshot%202025-03-14%20at%202.22.58%E2%80%AFPM.png"/>
</p>
</details>
</li>
</ul>
</li>
<li><p>How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods (<a class="reference external" href="https://arxiv.org/abs/1911.02508">slack, …, singh, lakkaraju, 2020</a>)</p>
<ul class="simple">
<li><p>we can build classifiers which use important features (such as race) but explanations will not reflect that</p></li>
<li><p>basically classifier is different on X which is OOD (and used by LIME and SHAP)</p></li>
</ul>
</li>
<li><p>Fooling Neural Network Interpretations via Adversarial Model Manipulation (<a class="reference external" href="https://arxiv.org/abs/1902.02041">heo, joo, &amp; moon 2019</a>) - can change model weights so that it keeps predictive accuracy but changes its interpretation</p>
<ul class="simple">
<li><p>motivation: could falsely look like a model is “fair” because it places little saliency on sensitive attributes</p>
<ul>
<li><p>output of model can still be checked regardless</p></li>
</ul>
</li>
<li><p>fooled interpretation generalizes to entire validation set</p></li>
<li><p>can force the new saliency to be whatever we like</p>
<ul>
<li><p>passive fooling - highlighting uninformative pixels of the image</p></li>
<li><p>active fooling - highlighting a completely different object, the firetruck</p></li>
</ul>
</li>
<li><p><strong>model does not actually change that much</strong> - predictions when manipulating pixels in order of saliency remains similar, very different from random (fig 4)</p></li>
</ul>
</li>
<li><p>Counterfactual Explanations Can Be Manipulated (<a class="reference external" href="https://arxiv.org/abs/2106.02666">slack,…,singh, 2021</a>) - minor changes in the training objective can drastically change counterfactual explanations</p></li>
<li><p>Do Input Gradients Highlight Discriminative Features? (<a class="reference external" href="https://arxiv.org/abs/2102.12781">shah et a. 2021</a>) - input gradients often don’t highlight relevant features (they work better for adv. robust models)</p>
<ul class="simple">
<li><p>prove/demonstrate this in synthetic dataset where <span class="math notranslate nohighlight">\(x=ye_i\)</span> for standard basis vector <span class="math notranslate nohighlight">\(e_i\)</span>, <span class="math notranslate nohighlight">\(y=\{\pm 1\}\)</span></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="intrinsic-interpretability-i-e-how-can-we-fit-a-simpler-model-transparent-models">
<h2><span class="section-number">7.9.2. </span>intrinsic interpretability (i.e. how can we fit a simpler model) = transparent models<a class="headerlink" href="#intrinsic-interpretability-i-e-how-can-we-fit-a-simpler-model-transparent-models" title="Link to this heading">#</a></h2>
<p>For an implementation of many of these models, see the python <a class="reference external" href="https://github.com/csinva/imodels">imodels package</a>.</p>
<section id="decision-rules-overview">
<h3><span class="section-number">7.9.2.1. </span>decision rules overview<a class="headerlink" href="#decision-rules-overview" title="Link to this heading">#</a></h3>
<p><em>📌 see also notes on <a class="reference external" href="https://csinva.io/notes/ai/logic.html">logic</a></em></p>
<p><img alt="rule_models" src="../../_images/rule_models-6174107.png" /></p>
<ul class="simple">
<li><p>2 basic concepts for a rule</p>
<ul>
<li><p>coverage = support</p></li>
<li><p>accuracy = confidence = consistency</p>
<ul>
<li><p>measures for rules: precision, info gain, correlation, m-estimate, Laplace estimate</p></li>
</ul>
</li>
</ul>
</li>
<li><p>these algorithms often don’t natively supportregression, but you can get regression by cutting the outcome into intervals</p></li>
<li><p>why might these be useful?</p>
<ul>
<li><p>The Magical Mystery Four: How is Working Memory Capacity Limited, and Why? (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2864034/">cowan, 2010</a> ) - a central memory store is limited to 3 to 5 meaningful items in young adults</p></li>
<li><p><a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/11034211/">Feldman (2000)</a>: humans can understand logical rules with boolean complexity of up to 5–9, depending on their ability, where the boolean complexity is the length of the shortest Boolean formula logically equivalent to the concept, usually expressed in terms of the number of literals</p></li>
</ul>
</li>
<li><p>connections</p>
<ul>
<li><p>every decision list is a (one-sided) decision tree</p></li>
<li><p>every decision tree can be expressed as an equivalent decision list (by listing each path to a leaf as a decision rule)</p></li>
<li><p>leaves of a decision tree (or a decision list) form a decision set</p></li>
</ul>
</li>
<li><p>recent work directly optimizes the performance metric (e.g., accuracy) with soft or hard sparsity constraints on the tree size, where sparsity is measured by the number of leaves in the tree using:</p>
<ol class="arabic simple">
<li><p>mathematical programming, including mixed integer programming (MIP) / SAT solvers</p></li>
<li><p>stochastic search through the space of trees</p></li>
<li><p>customized dynamic programming algorithms that incorporate branch-and-bound techniques for reducing the size of the search space</p></li>
</ol>
</li>
</ul>
<section id="rule-sets">
<h4><span class="section-number">7.9.2.1.1. </span>rule sets<a class="headerlink" href="#rule-sets" title="Link to this heading">#</a></h4>
<p><em>Rule sets commonly look like a series of independent if-then rules. Unlike trees / lists, these rules can be overlapping and might not cover the whole space. Final predictions can be made via majority vote, using most accurate rule, or averaging predictions. Sometimes also called rule ensembles.</em></p>
<ul class="simple">
<li><p>popular ways to learn rule sets</p>
<ul>
<li><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1184&amp;amp;rep=rep1&amp;amp;type=pdf">SLIPPER</a> (cohen, &amp; singer, 1999) - repeatedly boosting a simple, greedy rule-builder</p></li>
<li><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.4619">Lightweight Rule Induction</a> (weiss &amp; indurkhya, 2000) - specify number + size of rules and classify via majority vote</p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/1390156.1390185?casa_token=Lj3Ypp6bLzoAAAAA:t4p9YRPHEXJEL723ygEW5BJ9qft8EeU5934vPJFf1GrF1GWm1kctIePQGeaRiKHJa6ybpqtTqGg1Ig">Maximum Likelihood Rule Ensembles</a> (Dembczyński et al. 2008) - MLRules - rule is base estimator in ensemble - build by greedily maximizing log-likelihood</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://projecteuclid.org/euclid.aoas/1223908046">rulefit</a> (friedman &amp; popescu, 2008) - extract rules from many decision trees, then fit sparse linear model on them</p>
<ul>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/1143844.1143943?casa_token=74Cp4L015WQAAAAA:V8gYM4NMkiqRTmuGxtsnnTVZFaXl-eSzmLWFt78aVfoukuuZ-Y4-H-p3e-bF7EhA23uxKJ_oqLNq">A statistical approach to rule learning</a> (ruckert &amp; kramer, 2006) - unsupervised objective to mine rules with large maring and low variance before fitting linear model</p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v97/wei19a.html">Generalized Linear Rule Models</a> (wei et al. 2019) - use column generation (CG) to intelligently search space of rules</p>
<ul>
<li><p>re-fit GLM as rules are generated, reweighting + discarding</p>
<ul>
<li><p>with large number of columns, can be intractable even to enumerate rules - CG avoids this by fitting a subset and using it to construct most promising next column</p></li>
</ul>
</li>
<li><p>also propose a non-CG algorithm using only 1st-degree rules</p></li>
<li><p>note: from every pair of complementary singleton rules (e.g., <span class="math notranslate nohighlight">\(X_j \leq1\)</span>, <span class="math notranslate nohighlight">\(X_j &gt; 1\)</span>), they remove one member as otherwise the pair together is collinear</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.jstor.org/stable/2241837?casa_token=15XQTGF9ItgAAAAA%3A3dmi0WboPsb0TMDj6d0yefz1YUWUrQmKzh21NvMgpyQ_gbuveUibjIQY2Gq6J8C9-9HJ70QZns9x2MfB70dEGlll5RIgZ_VS3qPhRbbBMsrLwuD8H-0wmQ&amp;amp;seq=1#metadata_info_tab_contents">Multivariate Adaptive Regression Splines (MARS)</a> (friedman, 1991) - sequentially learn weighted linear sum of ReLus (or products of ReLus)</p>
<ul>
<li><p>do backward deletion procedure at the end</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2111.11694.pdf">MARS via LASSO</a> (ki, fang, &amp; guntuboyina, 2021)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1903.01395">Hardy-Krause denoising</a> (fang, guntuboyina, &amp; sen, 2020) - builds additive models of piecewise constant functions along with interactions and then does LASSO on them</p></li>
</ul>
</li>
<li><p>more recent global versions of learning rule sets</p>
<ul>
<li><p><a class="reference external" href="https://dl.acm.org/citation.cfm?id=2939874">interpretable decision set</a> (lakkaraju et al. 2016) - set of if then rules</p>
<ul>
<li><p>short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://www.jmlr.org/papers/volume18/16-003/16-003.pdf">A Bayesian Framework for Learning Rule Sets for Interpretable Classification</a> (wang et al. 2017) - rules are a bunch of clauses OR’d together (e.g. if (X1&gt;0 AND X2&lt;1) OR (X2&lt;1 AND X3&gt;1) OR … then Y=1)</p>
<ul>
<li><p>they call this method “Bayesian Rule Sets”</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1504.07614">Or’s of And’s for Interpretable Classification, with Application to Context-Aware Recommender Systems</a> (wang et al. 2015) - BOA - Bayesian Or’s of And’s</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0167865521003081">Vanishing boosted weights: A consistent algorithm to learn interpretable rules</a> (sokolovska et al. 2021) - simple efficient fine-tuning procedure for decision stumps</p></li>
</ul>
</li>
<li><p>when learning sequentially, often useful to prune at each step (Furnkranz, 1997)</p></li>
<li><p>Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations (<a class="reference external" href="https://proceedings.neurips.cc/paper/2018/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf">wang, 2018</a>)</p>
<ul>
<li><p>measure how long it takes for people to calculate predictions from different rule-based models</p></li>
</ul>
</li>
</ul>
</section>
<section id="rule-lists">
<h4><span class="section-number">7.9.2.1.2. </span>rule lists<a class="headerlink" href="#rule-lists" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>oneR algorithm - select feature that carries most information about the outcome and then split multiple times on that feature</p></li>
<li><p>sequential covering - keep trying to cover more points sequentially</p></li>
<li><p>pre-mining frequent patterns (want them to apply to a large amount of data and not have too many conditions)</p>
<ul>
<li><p>FP-Growth algorithm (borgelt 2005) is fast</p></li>
<li><p>Aprior + Eclat do the same thing, but with different speeds</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1303.6223.pdf">random intersection trees</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://projecteuclid.org/download/pdfview_1/euclid.aoas/1446488742">interpretable classifiers using rules and bayesian analysis</a> (letham et al. 2015)</p>
<ul>
<li><p>start by pre-mining frequent patterns rules</p>
<ul>
<li><p>current approach does not allow for negation (e.g. not diabetes) and must split continuous variables into categorical somehow (e.g. quartiles)</p></li>
<li><p>mines things that frequently occur together, but doesn’t look at outcomes in this step - okay (since this is all about finding rules with high support)</p></li>
</ul>
</li>
<li><p>learn rules w/ prior for short rule conditions and short lists</p>
<ul>
<li><p>start w/ random list</p></li>
<li><p>sample new lists by adding/removing/moving a rule</p></li>
<li><p>at the end, return the list that had the highest probability</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://dl.acm.org/citation.cfm?id=3306086">scalable bayesian rule lists</a> (yang et al. 2017) - faster algorithm for computing</p>
<ul>
<li><p>doesn’t return entire posterior</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://dl.acm.org/citation.cfm?id=3098047">learning certifiably optimal rules lists</a> (angelino et al. 2017) - even faster optimization for categorical feature space</p>
<ul>
<li><p>can get upper / lower bounds for loss = risk + <span class="math notranslate nohighlight">\(\lambda\)</span> * listLength</p></li>
<li><p>doesn’t return entire posterior</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1903.09731">Expert-augmented machine learning</a> (gennatas et al. 2019)</p>
<ul>
<li><p>make rule lists, then compare the outcomes for each rule with what clinicians think should be outcome for each rule</p></li>
<li><p>look at rules with biggest disagreement and engineer/improve rules or penalize unreliable rules</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://psycnet.apa.org/record/1999-04366-001">Fast and frugal heuristics: The adaptive toolbox.</a> (gigerenzer et al. 1999) - makes rule lists that can split on either node of the tree each time</p></li>
</ul>
</section>
<section id="trees">
<h4><span class="section-number">7.9.2.1.3. </span>trees<a class="headerlink" href="#trees" title="Link to this heading">#</a></h4>
<p><em>Trees suffer from the fact that they have to cover the entire decision space and often we end up with replicated subtrees.</em></p>
<ul class="simple">
<li><p>history</p>
<ul>
<li><p>automatic interaction detection (AID) regression trees (Morgan &amp; Sonquist, 1963)</p></li>
<li><p>THeta Automatic Interaction Detection (THAID) classification trees (Messenger &amp; Mandell, 1972)</p></li>
<li><p>Chi-squared Automatic Interaction Detector (CHAID) (Kass, 1980)</p></li>
<li><p>CART: Classification And Regression Trees  (<a class="reference external" href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone">Breiman et al. 1984</a>) - splits on GINI</p></li>
<li><p>ID3 (Quinlan, 1986)</p></li>
<li><p>C4.5 (Quinlan, 1993) - splits on binary entropy instead of GINI</p></li>
</ul>
</li>
<li><p>optimal trees</p>
<ul>
<li><p>motivation</p>
<ul>
<li><p>cost-complexity pruning (<a class="reference external" href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-richard-olshen-charles-stone">breiman et al. 1984</a> ch 3) - greedily prune while minimizing loss function of loss + <span class="math notranslate nohighlight">\(\lambda \cdot (\text{numLeaves})\)</span></p></li>
<li><p>replicated subtree problem (<a class="reference external" href="https://link.springer.com/content/pdf/10.1007/BF00115895.pdf">Bagallo &amp; Haussler, 1990</a>) - they propose iterative algorithms to try to overcome it</p></li>
</ul>
</li>
<li><p>GOSDT: Generalized and Scalable Optimal Sparse Decision Trees (<a class="reference external" href="https://arxiv.org/abs/2006.08690">lin…rudin, seltzer, 2020</a>)</p>
<ul>
<li><p>optimize for <span class="math notranslate nohighlight">\(\min L(X, y) + \lambda \cdot (\text{numLeaves})\)</span></p></li>
<li><p>full decision tree optimization is NP-hard (<a class="reference external" href="https://people.csail.mit.edu/rivest/HyafilRivest-ConstructingOptimalBinaryDecisionTreesIsNPComplete.pdf">Laurent &amp; Rivest, 1976</a>)</p></li>
<li><p>can optimize many different losses (e.g. accuracy, AUC)</p></li>
<li><p>speedups: use dynamic programming, prune the search-space with bounds</p>
<ul>
<li><p>How Smart Guessing Strategies Can Yield Massive Scalability Improvements for Sparse Decision Tree Optimization (<a class="reference external" href="https://arxiv.org/abs/2112.00798">mctavish…rudin, seltzer, 2021</a>)</p></li>
<li><p>hash trees with bit-vectors that represent similar trees using shared subtrees</p>
<ul>
<li><p>tree is a <em>set</em> of <em>leaves</em></p></li>
</ul>
</li>
<li><p>derive many bounds</p>
<ul>
<li><p>e.g. if know best loss so far, know shouldn’t add too many leaves since each adds <span class="math notranslate nohighlight">\(\lambda\)</span> to the total loss</p></li>
<li><p>e.g. similar-support bound - if two features are similar, then bounds for splitting on the first can be used to obtain bounds for the second</p></li>
</ul>
</li>
</ul>
</li>
<li><p>OSDT: optimal sparse decision trees (<a class="reference external" href="https://arxiv.org/abs/1904.12847">hu et al. 2019</a>) - previous paper, slower</p>
<ul>
<li><p>bounds: Upper Bound on Number of Leaves, Leaf Permutation Bound</p></li>
</ul>
</li>
<li><p>Fast Optimization of Weighted Sparse Decision Trees for use in Optimal Treatment Regimes and Optimal Policy Design (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10039433/">behrouz…rudin, &amp; seltzer, 2022</a>) - extend method to work with weights on samples</p></li>
</ul>
</li>
<li><p>OCT: optimal classification trees methodology paper (<a class="reference external" href="https://link.springer.com/content/pdf/10.1007%2Fs10994-017-5633-9.pdf">bertsimas &amp; dunn, 2017</a>) - solve optimal tree with expensive, mixed-integer optimization - realistically, usually too slow</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{cl}
\min &amp; \overbrace{R_{x y}(T)}^{\text{misclassification err}}+\alpha|T| \\
\text { s.t. } &amp; N_{x}(l) \geq N_{\min } \quad \forall l \in \text { leaves }(T)
\end{array}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(|T|\)</span> is the number of branch nodes in tree <span class="math notranslate nohighlight">\(T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(N_x(l)\)</span> is the number of training points contained in leaf node <span class="math notranslate nohighlight">\(l\)</span></p></li>
<li><p>optimal classification trees on PECARN TBI (<a class="reference external" href="https://jamanetwork.com/journals/jamapediatrics/article-abstract/2733157">bertsimas et al. 2019</a>)</p></li>
</ul>
</li>
<li><p>Learning Optimal Fair Classification Trees (<a class="reference external" href="https://arxiv.org/pdf/2201.09932.pdf">jo et al. 2022</a>)</p></li>
<li><p>Better Short than Greedy: Interpretable Models through Optimal Rule Boosting (<a class="reference external" href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.40">boley, …, webb, 2021</a>) - find optimal tree <strong>ensemble</strong> (only works for very small data)</p></li>
</ul>
</li>
<li><p>connections with boosting</p>
<ul>
<li><p>Fast Interpretable Greedy-Tree Sums (FIGS) (<a class="reference external" href="https://arxiv.org/abs/2201.11931">tan et al. 2022</a>) - extend cart to learn concise tree ensembles 🌳 ➡️ 🌱+🌱</p>
<ul>
<li><p>very nice results for generalization + disentanglement</p></li>
</ul>
</li>
<li><p>AdaTree - learn Adaboost stumps then rewrite as a tree (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/1384899/">grossmann, 2004</a>) 🌱+🌱 ➡️ 🌳</p>
<ul>
<li><p>note: easy to rewrite boosted stumps as tree (just repeat each stump for each node at a given depth)</p></li>
</ul>
</li>
<li><p>MediBoost - again, learn boosted stumps then rewrite as a tree (<a class="reference external" href="https://www.nature.com/articles/srep37854.pdf">valdes…solberg 2016</a>) 🌱+🌱 ➡️ 🌳 but with 2 tweaks:</p>
<ul>
<li><p>shrinkage: use membership function that accelerates convergence to a decision (basically shrinkage during boosting)</p></li>
<li><p>prune the tree in a manner that does not affect the tree’s predictions</p>
<ul>
<li><p>prunes branches that are impossible to reach by tracking the valid domain for every attribute (during training)</p></li>
<li><p>post-prune the tree bottom-up by recursively eliminating the parent nodes of leaves with identical predictions</p></li>
</ul>
</li>
</ul>
</li>
<li><p>AddTree = additive tree - learn single tree, but rather than only current node’s data to decide the next split, also allow the remaining data to also influence this split, although with a potentially differing weight (<a class="reference external" href="https://www.pnas.org/content/116/40/19887">luna, …, friedman, solberg, valdes, 2019</a>)</p>
<ul>
<li><p>the weight is chosen as a hyperparameter</p></li>
</ul>
</li>
<li><p>Additive groves (<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-540-74958-5_31">sorokina, carauna, &amp; riedewald 2007</a>) - additive model of a few deep trees (gradually increase number and size of trees)</p></li>
<li><p>Random Planted Forest: a directly interpretable tree ensemble (<a class="reference external" href="https://arxiv.org/abs/2012.14563">hiabu, mammen, &amp; meyer, 2023</a>) - propose a planted tree very similar to FIGS, but use it in a randomized ensemble that restricts the number of interactions in a tree (similar to GAMs)</p></li>
</ul>
</li>
<li><p>bayesian trees</p>
<ul>
<li><p>Bayesian Treed Models (<a class="reference external" href="http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/treed-models.pdf">chipman et al. 2001</a>) - impose priors on tree parameters</p>
<ul>
<li><p>treed models - fit a model (e.g. linear regression) in leaf nodes</p></li>
<li><p>tree structure e.g. depth, splitting criteria</p></li>
<li><p>values in terminal nodes coditioned on tree structure</p></li>
<li><p>residual noise’s standard deviation</p></li>
<li><p>Stochastic gradient boosting (<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0167947301000652">friedman 2002</a>) - boosting where at iteration a subsample of the training data is used</p></li>
</ul>
</li>
<li><p>BART: Bayesian additive regression trees (<a class="reference external" href="https://arxiv.org/abs/0806.3286">chipman et al. 2008</a>) - learns an ensemble of tree models using MCMC on a distr. imbued with a prior (not interpretable)</p>
<ul>
<li><p>pre-specify number of trees in ensemble</p></li>
<li><p>MCMC step: add split, remove split, switch split</p></li>
<li><p>cycles through the trees one at a time</p></li>
</ul>
</li>
</ul>
</li>
<li><p>tree regularization</p>
<ul>
<li><p>Hierarchical Shrinkage: improving accuracy and interpretability of tree-based methods (<a class="reference external" href="https://arxiv.org/abs/2202.00858">agarwal et al. 2021</a>) - post-hoc shrinkage improves trees</p></li>
<li><p>Hierarchical priors for Bayesian CART shrinkage (<a class="reference external" href="https://link.springer.com/article/10.1023/A:1008980332240">chipman &amp; mcculloch, 2000</a>)</p></li>
<li><p>Connecting Interpretability and Robustness in Decision Trees through Separation (<a class="reference external" href="https://arxiv.org/abs/2102.07048">moshkovitz et al. 2021</a>)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2012.10438">Efficient Training of Robust Decision Trees Against Adversarial Examples</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1902.10660">Robust Decision Trees Against Adversarial Examples</a> (chen, …, hsieh, 2019)</p>
<ul>
<li><p>optimize tree performance under worst-case input-feature perturbation</p></li>
</ul>
</li>
</ul>
</li>
<li><p>trees for density estimation</p>
<ul>
<li><p>in density estimation, we seek a tree <span class="math notranslate nohighlight">\(\hat{f}(\mathbf{x})\)</span> approximating the underlying density function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span></p></li>
<li><p>ideal loss function to minimize is <span class="math notranslate nohighlight">\(\mathcal{R}=\int(\hat{f}(\mathbf{x})-f(\mathbf{x}))^2 \mathrm{~d} \mathbf{x}\)</span></p></li>
<li><p>instead, we minimize approximation for a node t: <span class="math notranslate nohighlight">\(\mathcal{R}(t) = \frac{1}{n} \frac{n_t}{Vol_t}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(n\)</span> is total points in dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(n_t\)</span> is points in node <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Vol_t\)</span> is volume of the subset of the data that meets the criteria leading to that node (e.g. given 2 splits on different features, it would be area of a square bounded on one side by the split and the other side by the max</p></li>
</ul>
</li>
<li><p>Density Estimation Trees (<a class="reference external" href="https://mlpack.org/papers/det.pdf">ram &amp; gray, 2011</a>)</p></li>
<li><p>Sparse Density Trees and Lists: An Interpretable Alternative to High-Dimensional Histograms (<a class="reference external" href="https://pubsonline.informs.org/doi/pdf/10.1287/ijds.2021.0001">goh, semenova, &amp; rudin, 2024</a>)</p></li>
<li><p>On the price of explainability for some clustering problems (<a class="reference external" href="https://arxiv.org/abs/2101.01576">laber et al. 2021</a>) - trees for clustering</p></li>
<li><p>Interpretable clustering: an optimization approach (<a class="reference external" href="https://link.springer.com/article/10.1007/s10994-020-05896-2">bertsimas…wilberg, 2020</a>)</p></li>
</ul>
</li>
<li><p>open problems: ensemble methods, improvements in splitting criteria, missing variables, longitudinal data, survival curves</p></li>
<li><p>misc</p>
<ul>
<li><p>On the Power of Decision Trees in Auto-Regressive Language Modeling (<a class="reference external" href="https://arxiv.org/pdf/2409.19150">gan, galanti, poggio, malach, 2024</a>)</p>
<ul>
<li><p>get token word embeddings</p></li>
<li><p>compute exp. weighted avg of embeddings (upweights most recent tokens)</p></li>
<li><p>predicts next embedding with XGBoost (regression loss) then finds closest token</p></li>
</ul>
</li>
<li><p>counterfactuals</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2103.01096">Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms</a> (2021)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2106.06631">Optimal Counterfactual Explanations in Tree Ensembles</a></p></li>
<li><p><a class="reference external" href="https://ojs.aaai.org//index.php/AAAI/article/view/5154">Desiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0275814">Human knowledge models: Learning applied knowledge from the data</a> (dudyrev…pianykh, 2022) - very concise logic model</p></li>
<li><p>extremely randomized trees (<a class="reference external" href="https://link.springer.com/article/10.1007/s10994-006-6226-1">geurts ernst, &amp; wehenkel, 2006</a>) - randomness goes further than Random Forest - randomly select not only the feature but also the split thresholds (and select the best out of some random set)</p></li>
<li><p>Constraint Enforcement on Decision Trees: A Survey (<a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3506734">nanfack…brenay, 2022</a>)</p>
<ul>
<li><p>3 classes of constraints</p>
<ol class="arabic simple">
<li><p>feature-level: monotonicity, attribute costs, hierarchy/interaction, fairness, privacy</p></li>
<li><p>structure-level - e.g. minimize #nodes</p></li>
<li><p>instance-level - must (cannot) link, robust predictions</p></li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Analysis of Boolean functions (<a class="reference external" href="https://en.wikipedia.org/wiki/Analysis_of_Boolean_functions">wiki</a>)</p>
<ul>
<li><p>Every real-valued function <span class="math notranslate nohighlight">\(f:\{-1,1\}^n \rightarrow \mathbb{R}\)</span> has a unique expansion as a multilinear polynomial:
$<span class="math notranslate nohighlight">\(
f(x)=\sum_{S \subseteq[n]} \hat{f}(S) \chi_S(x), \quad \overbrace{\chi_S(x)=\prod_{i \in S} x_i}^{\text{Interactions}}
\)</span>$</p></li>
</ul>
</li>
</ul>
</section>
<section id="decision-diagrams">
<h4><span class="section-number">7.9.2.1.4. </span>decision diagrams<a class="headerlink" href="#decision-diagrams" title="Link to this heading">#</a></h4>
<p><em>Also called decision graphs / decision streams</em></p>
<ul class="simple">
<li><p>Algorithms for learning diagrams usually postprocess a given tree (and taking an MDL perspective)</p></li>
<li><p>Decision diagrams for model compression</p>
<ul>
<li><p><a class="reference external" href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/simplifying-decision-trees-a-survey/CEE7A6994E66E821DB4A12DD83DC3810">Simplifying decision trees: A survey</a> (breslow &amp; aha, 1997)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1912.10934">Large Random Forests: Optimisation for Rapid Evaluation</a> (gossen &amp; steffen, 2019)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2205.14500.pdf">Optimal Decision Diagrams for Classification</a> (florio…vidal 2022) - decision diagrams are like trees but paths can rejoin</p></li>
<li><p>Extracting rules from neural networks as decision diagrams <a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/21335310/">(Chorowski, J., J.M. Zurada. 2011</a>)</p></li>
<li><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2013/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html">Decision Jungles: Compact and Rich Models for Classification</a> (shotton et al. 2013) - ensembles of decision diagrams</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8372043">Decision Stream: Cultivating Deep Decision Trees</a> (ignatov &amp; ignatov, 2017)</p>
<ul>
<li><p>merge nodes from different branches based on their similarity that is estimated with two-sample test statistics</p></li>
<li><p>nothing to do with deep learning</p></li>
</ul>
</li>
<li><p>misc</p>
<ul>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9474083">Optimizing Binary Decision Diagrams for Interpretable Machine Learning Classification</a> (cabodi et al. 2021)</p></li>
<li><p>proposing a SAT-based model for computing a decision tree as the smallest Reduced Ordered Binary Decision Diagram</p></li>
<li><p>exploring heuristic approaches for deriving sub-optimal (i.e., not minimal) ROBDDs, in order to improve the scalability of the proposed technique.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2203.11386">Optimizing Binary Decision Diagrams with MaxSAT for classification</a> (hu et al. 2022)</p></li>
<li><p>Deep Differentiable Logic Gate Networks (<a class="reference external" href="https://arxiv.org/pdf/2210.08277.pdf">peterson, …, deussen, 2022</a>)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="linear-algebraic-models">
<h3><span class="section-number">7.9.2.2. </span>linear (+algebraic) models<a class="headerlink" href="#linear-algebraic-models" title="Link to this heading">#</a></h3>
<section id="gams-generalized-additive-models">
<h4><span class="section-number">7.9.2.2.1. </span>gams (generalized additive models)<a class="headerlink" href="#gams-generalized-additive-models" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>gam takes form <span class="math notranslate nohighlight">\(g(\mu) = b + f_0(x_0) + f_1(x_1) + f_2(x_2) + ...\)</span></p>
<ul>
<li><p>usually assume some basis for the shape functions <span class="math notranslate nohighlight">\(f\)</span>, like splines, polynomials, or tree sums (and we select how many either manually or with some complexity penalty)</p></li>
<li><p><em>backfitting</em> - traditional way to fit - each <span class="math notranslate nohighlight">\(f_i\)</span> is fitted sequentially to the residuals of the previously fitted <span class="math notranslate nohighlight">\(f_0,...,f_{i-1}\)</span> (<a class="reference external" href="https://www.jstor.org/stable/2241560?seq=1#metadata_info_tab_contents">hastie &amp; tibshirani, 1989</a>)</p>
<ul>
<li><p>once all are fit, discard each shape function and re-fit to the residuals of all others one at a time</p></li>
</ul>
</li>
<li><p><em>boosting</em> - fit all <span class="math notranslate nohighlight">\(f\)</span> simultaneously, e.g. one tree for each <span class="math notranslate nohighlight">\(f_i\)</span> on each iteration</p></li>
<li><p>interpretability depends on (1) transparency of <span class="math notranslate nohighlight">\(f_i\)</span> and (2) number of terms</p></li>
<li><p>can also add in interaction terms (e.g. <span class="math notranslate nohighlight">\(f_i(x_1, x_2)\)</span>), but need a way to rank which interactions to add (see notes on interactions)</p></li>
</ul>
</li>
<li><p>Explainable boosting machine: tree-based shape functions trained with cyclical boosting</p>
<ul>
<li><p>Intelligible models for classification and regression (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/2339530.2339556">lou, caruana, &amp; gehrke, 2012</a>) - find that gradient boosting with shallow trees outperforms other models for <span class="math notranslate nohighlight">\(f_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(GA^2M\)</span> (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/2487575.2487579">lou, caruana, gehrke, &amp; hooker, 2013</a>) - select interactions using algorithm called FAST</p></li>
<li><p>Pneumonia risk (<a class="reference external" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf">caruana, lou, gehrke, koch, sturm, &amp; elhadad, 2015</a>) - application of GA2M finds interesting patterns (e.g. asthma decreases pneumonia risk)</p></li>
<li><p>InterpretML: A Unified Framework for Machine Learning Interpretability (<a class="reference external" href="https://arxiv.org/abs/1909.09223">nori…caruana 2019</a>) - software package mostly for EBM</p></li>
<li><p>Adding differential privacy to EBM (<a class="reference external" href="https://arxiv.org/abs/2106.09680">nori, caruana et al. 2021</a>)</p></li>
</ul>
</li>
<li><p>Neural Additive Models: Interpretable Machine Learning with Neural Nets (<a class="reference external" href="https://arxiv.org/abs/2004.13912">agarwal, …, caruana, &amp; hinton, 2021</a>) - shape functions are a DNN (also use ExU activation instead of ReLU to model sharp bumps)</p>
<ul>
<li><p>no interaction terms</p></li>
<li><p>NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2106.01613">chang, caruana, &amp; goldenberg, 2021</a>)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(NODE-GA^2M\)</span>: include interaction terms by initializing with all interaction terms and backprop decides which are kept</p></li>
<li><p>uses neural oblivious trees rather than standard DNN</p></li>
<li><p>idea dates back to <a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/312129.312228">Generalized Additive Neural Networks</a> (potts, 1999)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2202.12482">Sparse Neural Additive Model: Interpretable Deep Learning with Feature Selection via Group Sparsity</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2107.14417">Creating Powerful and Interpretable Models with Regression Networks</a> (2021) - generalizes neural GAM to include interaction terms</p>
<ul>
<li><p>train first-order functions</p></li>
<li><p>fix them and predict residuals with next order (and repeat for as many orders as desired)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Spline-based models</p>
<ul>
<li><p>Fast Stable Direct Fitting and Smoothness Selection for Generalized Additive Models (<a class="reference external" href="https://academic.oup.com/jrsssb/article/70/3/495/7109556">wood, 2008</a>)</p></li>
</ul>
</li>
<li><p>Other models</p>
<ul>
<li><p>Additive Models with Trend Filtering (<a class="reference external" href="https://arxiv.org/abs/1702.05037">sadhanala &amp; tibshirani, 2018</a>) - piecewise polynomial components with total variation regularization</p></li>
<li><p>Fused Lasso Additive Model (<a class="reference external" href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2015.1073155">petersen, weitten &amp; simon, 2014</a>) - shape functions are piecewise constant with a small number of knots</p></li>
</ul>
</li>
<li><p>Ensembling</p>
<ul>
<li><p>Ensemble classification based on generalized additive models (<a class="reference external" href="https://ideas.repec.org/p/rug/rugwps/09-625.html">bock…poel, 2009</a>) - use sample bagging, feature bagging, and combine both to find improved performance (use spline-based GAMs)</p></li>
<li><p>Efficiently Training Intelligible Models for Global Explanations (<a class="reference external" href="http://yinlou.github.io/papers/lou-cikm20.pdf">lou…dong, 2020</a>) - use subsample bagging (bags select from a subset of samples) and speed up fitting by sharing computation between bags that share points</p></li>
</ul>
</li>
<li><p>misc improvements</p>
<ul>
<li><p>Axiomatic Interpretability for Multiclass Additive Models (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3292500.3330898">zhang, tan, … caruana, 2019</a>) - extend GAM to multiclass and improve visualizations in that setting</p></li>
<li><p><a class="reference external" href="https://www.tandfonline.com/doi/full/10.1080/10618600.2015.1089775">Sparse Partially Linear Additive Models</a> (lou, bien, caruana &amp; gehrke, 2015) - some terms are linear and some use <span class="math notranslate nohighlight">\(f_i(x_i)\)</span></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.14120">Neural Basis Models for Interpretability</a> (2022)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.14108">Scalable Interpretability via Polynomials</a> (2022)</p></li>
<li><p>How Interpretable and Trustworthy are GAMs? (<a class="reference external" href="https://arxiv.org/abs/2006.06466">chang, tan, lengerich, goldenberg, &amp; caruana, 2021</a>) - different GAM algorithms provide different interpretations, tree-based GAMs appear best</p>
<ul>
<li><p>feature-sparse GAMs may perform worse on some data subsets</p></li>
<li><p>overly smooth GAMs may fail to identify sharp jumps that may be errors in data</p></li>
<li><p>bias-variance decomposition of different GAM algorithms (based on multiple refits and predictions) shows that some have less bias whereas others less variance</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="supersparse-models">
<h4><span class="section-number">7.9.2.2.2. </span>supersparse models<a class="headerlink" href="#supersparse-models" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>four main types of approaches to building scoring systems</p>
<ol class="arabic simple">
<li><p>exact solutions using optimization techniques (often use MIP)</p></li>
<li><p>approximation algorithms using linear programming (use L1 penalty instead of L0)</p>
<ol class="arabic simple">
<li><p>can also try sampling</p></li>
</ol>
</li>
<li><p>more sophisticated rounding techniques - e.g. random, constrain sum, round each coef sequentially</p></li>
<li><p>computeraided exploration techniques</p></li>
</ol>
</li>
<li><p>Supersparse linear integer models for optimized medical scoring systems (<a class="reference external" href="https://link.springer.com/content/pdf/10.1007/s10994-015-5528-6.pdf">ustun &amp; rudin 2016</a>)</p>
<ul>
<li><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pubmed/29052706">2helps2b paper</a></p></li>
<li><p><img alt="" src="../../_images/2helps2b.png" /></p></li>
<li><p>note: scoring systems map points to a risk probability</p></li>
</ul>
</li>
<li><p>An Interpretable Model with Globally Consistent Explanations for Credit Risk (<a class="reference external" href="https://arxiv.org/abs/1811.12615">chen et al. 2018</a>) - a 2-layer linear additive model</p></li>
<li><p>Fast Sparse Classification for Generalized Linear and Additive Models (<a class="reference external" href="https://arxiv.org/abs/2202.11389">liu, …, seltzer, rudin, 2022</a>)</p></li>
<li><p>Naive Feature Selection: Sparsity in Naive Bayes (<a class="reference external" href="http://proceedings.mlr.press/v108/askari20a.html">askari…el ghaoui, 2020</a>) - sparse naive bayes feature selection is on par with lasso but much faster</p></li>
<li><p>GroupFasterRisk: Fast and Interpretable Mortality Risk Scores for Critical Care Patients (<a class="reference external" href="https://arxiv.org/pdf/2311.13015.pdf">zhu…rudin, 2023</a>) - builds linear integer model over bins of features. Group sparsity penalty penalizes number of unique features, whereas a secondary penalty controls the number of bins used per feature</p></li>
</ul>
</section>
<section id="symbolic-regression">
<h4><span class="section-number">7.9.2.2.3. </span>symbolic regression<a class="headerlink" href="#symbolic-regression" title="Link to this heading">#</a></h4>
<p>Symbolic regression learns a symbolic expression for a function (e.g. a mathematical formula) given priors on what kinds of symbols (e.g. <code class="docutils literal notranslate"><span class="pre">addition</span></code> versus <code class="docutils literal notranslate"><span class="pre">sin</span></code>) are more “difficult” [see also notes on LLM-in-the-loop symbolic regression]</p>
<ul class="simple">
<li><p>Interpretable Scientific Discovery with Symbolic Regression: A Review (<a class="reference external" href="https://arxiv.org/abs/2211.10873">makke &amp; chawla, 2022</a>)</p></li>
<li><p>Genetic algorithms</p>
<ul>
<li><p>Search for tree that represents expression: maximize acc + minimize number of nodes in the tree (e.g. PySR package, <a class="reference external" href="https://arxiv.org/abs/2305.01582">cranmer 2023</a>)</p></li>
<li><p>Discovering Symbolic Models from Deep Learning with Inductive Biases (<a class="reference external" href="https://arxiv.org/abs/2006.11287">cranmer…ho, 2020</a>) - rather than learning entire function class at once, learn compositions (so second function takes first function as input), which allows functions to gradually become much more complex</p></li>
<li><p>Symbolic-Regression Boosting (<a class="reference external" href="https://arxiv.org/abs/2206.12082">sipper &amp; moore, 2022</a>)</p></li>
</ul>
</li>
<li><p>Post-hoc distillation</p>
<ul>
<li><p>Demystifying Black-box Models with Symbolic Metamodels (<a class="reference external" href="https://papers.nips.cc/paper/9308-demystifying-black-box-models-with-symbolic-metamodels.pdf">alaa, van der schaar, 2019</a>) - distill black-box model with Meijer G-functions (rather than pre-specifying some forms, as is done with symbolic regression)</p>
<ul>
<li><p>Symbolic Metamodels for Interpreting Black-boxes Using Primitive Functions (<a class="reference external" href="https://arxiv.org/abs/2302.04791">abroshan…khalili, 2023</a>) - use GP approach</p></li>
<li><p>Neural Symbolic Regression using Control Variables (<a class="reference external" href="https://arxiv.org/abs/2306.04718">chu…shao, 2023</a>)</p></li>
</ul>
</li>
<li><p>Discovering Symbolic Models from Deep Learning with Inductive Biases (<a class="reference external" href="https://arxiv.org/abs/2006.11287">cranmer…ho, 2020</a>) - focused on GNNs</p></li>
<li><p>Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Black Box Simulators (<a class="reference external" href="https://arxiv.org/abs/2002.01080">sreedharan et al. 2020</a>)</p></li>
</ul>
</li>
<li><p>neural networks</p>
<ul>
<li><p>2-step symbolic regr: first generate equation skeleton, then optimize constants with GD</p>
<ul>
<li><p>Neural Symbolic Regression that Scales (<a class="reference external" href="https://arxiv.org/abs/2106.06427">biggio et al. 2021</a>) - use large pretraining set</p></li>
<li><p>SymbolicGPT (<a class="reference external" href="https://arxiv.org/abs/2106.14131">valipour…ghodsi, 2021</a>) - similar but use point cloud rather than attention</p></li>
<li><p>Deep symbolic regression (<a class="reference external" href="https://arxiv.org/pdf/1912.04871.pdf">petersen…kim, 2021</a>) - RL-based</p></li>
</ul>
</li>
<li><p>End-to-End symbolic regression (still use final refinement step)</p>
<ul>
<li><p>AI Feynman: A physics-inspired method for symbolic regression (<a class="reference external" href="https://www.science.org/doi/10.1126/sciadv.aay2631">udresku &amp; tegmark, 2020</a>) - use a loop with many if-then checks to decompose the equations</p></li>
<li><p>End-to-end symbolic regression with transformers (<a class="reference external" href="https://arxiv.org/abs/2204.10532">kamienny…charton, 2022</a>)</p></li>
<li><p>SymFormer (<a class="reference external" href="https://arxiv.org/abs/2205.15764">vastl…babuska, 2022</a>)</p></li>
<li><p>Deep Generative Symbolic Regression (<a class="reference external" href="https://openreview.net/forum?id=o7koEEMA1bR">holt…van der schaar, 2023</a>) - use RL</p></li>
</ul>
</li>
<li><p>Building and Evaluating Interpretable Models using Symbolic Regression and Generalized Additive Models (<a class="reference external" href="https://openreview.net/pdf?id=BkgyvQzmW">sharif, 2017</a>)</p></li>
</ul>
</li>
<li><p>Logic Regression (<a class="reference external" href="https://amstat.tandfonline.com/doi/abs/10.1198/1061860032238?casa_token=WVNXGYsNPLsAAAAA:eCjYgsRw_WZ6g0GPG9x3CMyHyEV9kwcXvWCC1S0TTbLc7SDBiiyHiKLNtYsuC6WYOpto7xAi6tQ5eQ#.YJB3bGZKjzc">ruczinski, kooperberg &amp; leblanc, 2012</a>) - given binary input variables, automatically construct interaction terms and linear model (fit using simulated annealing)</p></li>
<li><p>Model Learning with Personalized Interpretability Estimation (<a class="reference external" href="https://arxiv.org/abs/2104.06060">virgolin…wahde, 2021</a>) - use HITL to decide which symbolic functions are most interpretable</p></li>
</ul>
</section>
</section>
<section id="interpretable-neural-nets">
<h3><span class="section-number">7.9.2.3. </span>interpretable neural nets<a class="headerlink" href="#interpretable-neural-nets" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Adaptive wavelet distillation from neural networks through interpretations (<a class="reference external" href="https://arxiv.org/abs/2107.09145">Ha et al. 2021</a>) - distill flexible wavelet using gradient-based interpretation scores from a neural net</p></li>
<li><p>PINN: Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0021999118307125">raissi et al. 2019</a>) - solve PDEs by constraining neural net to predict specific parameters / derivatives</p></li>
<li><p>Two Instances of Interpretable Neural Network for Universal Approximations (<a class="reference external" href="https://arxiv.org/abs/2112.15026">tjoa &amp; cuntai, 2021</a>) - each neuron responds to a training point</p></li>
<li><p>B-Cos Networks: Alignment Is All We Need for Interpretability (<a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2022/html/Bohle_B-Cos_Networks_Alignment_Is_All_We_Need_for_Interpretability_CVPR_2022_paper.html">bohle…schiele, 2022</a>) - promotes weight-input alignment during training</p></li>
</ul>
<section id="concepts">
<h4><span class="section-number">7.9.2.3.1. </span>concepts<a class="headerlink" href="#concepts" title="Link to this heading">#</a></h4>
<p><em>📌 see also notes on LLMs</em></p>
<ul class="simple">
<li><p>CBM: Concept Bottleneck Models (<a class="reference external" href="https://arxiv.org/pdf/2007.04612.pdf">koh et al. 2020</a>) - predict intermediate concepts before making final prediction</p></li>
<li><p>Post-hoc CBM (<a class="reference external" href="https://arxiv.org/abs/2205.15480">yuksekgonul…zou, 2022</a>) - automatically project embeddings to concepts and train linear model on those</p>
<ul>
<li><p>Label-Free CBM (<a class="reference external" href="https://arxiv.org/abs/2304.06129">oikarinen…lily weng, 2023</a>) - extend to learn to match text concepts extracted with embeddings of any vision model</p></li>
<li><p>uses CLIP-Dissect method (<a class="reference external" href="https://arxiv.org/abs/2204.10965">oikarinen &amp; lily weng, 2023</a>)</p></li>
</ul>
</li>
<li><p>CBMs with LLMs / VLMs</p>
<ul>
<li><p>Crafting Interpretable Embeddings by Asking LLMs Questions (<a class="reference external" href="https://arxiv.org/pdf/2405.16714">benara…gao, 2024</a>) - use LLM to generate and answer questions corresponding to concepts, then use LASSO to select relevant questions</p></li>
<li><p>BC-LLM: Bayesian Concept Bottleneck Models with LLM Priors (<a class="reference external" href="https://arxiv.org/abs/2410.15555">feng…tan, 2024</a>) - use LLM to generate questions from extracted keywords, then iterate on fitting predictive models and searching for new concepts with a Bayesian approach</p></li>
<li><p>LaBO: Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification (<a class="reference external" href="https://arxiv.org/pdf/2211.11158.pdf">yang…yatskar, 2022</a>) - generate prompt-based features using GPT-3 (e.g. “brown head with white stripes”) and use CLIP to check for the presence of those features, all before learning simple linear model</p>
<ul>
<li><p>Knowledge-enhanced Bottlenecks (KnoBo) - A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis (<a class="reference external" href="https://yueyang1996.github.io/papers/knobo.pdf">yang…yatskar, 2024</a>) - CBMs that incorporate knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed</p></li>
</ul>
</li>
<li><p>CB-LLM: Crafting Large Language Models for Enhanced Interpretability (<a class="reference external" href="https://lilywenglab.github.io/WengLab_2024_CBLLM.pdf">sun…lily weng, 2024</a>)</p>
<ul>
<li><p>compute embedding similarity of concepts and input, and train layer to predict each of these similarity scores as concept bottleneck</p>
<ul>
<li><p>before training bottleneck, use ChatGPT to help correct any concept scores that seem incorrect</p></li>
</ul>
</li>
<li><p>Human evaluation: agreement of concept scores and contribution of concept to output</p></li>
<li><p>Concept Bottleneck Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2412.07992">sun, oikarinen, ustun, &amp; lily weng, 2024</a>) - this updated version of the paper also has results for language modeling</p></li>
</ul>
</li>
<li><p>Towards Achieving Concept Completeness for Unsupervised Textual Concept Bottleneck Models (<a class="reference external" href="https://arxiv.org/abs/2502.11100">bhan…lesot, 2025</a>) - distill embeddings from a trained model</p></li>
</ul>
</li>
<li><p>Refining CBMs</p>
<ul>
<li><p>Tree-Based Leakage Inspection and Control in Concept Bottleneck Models (<a class="reference external" href="https://arxiv.org/abs/2410.06352">ragkousis &amp; parbhoo, 2024</a>) - investigate where soft version of a feature outperforms hard version of the feature</p></li>
<li><p>Stochastic Concept Bottleneck Models (<a class="reference external" href="https://arxiv.org/pdf/2406.19272">vandenhirtz…vogt, 2024</a>) - model covariance between concepts</p></li>
<li><p>Coarse-to-Fine Concept Bottleneck Models (<a class="reference external" href="https://arxiv.org/pdf/2310.02116">panousis…marcos, 2024</a>)</p></li>
</ul>
</li>
<li><p>MoIE: Route, Interpret, Repeat (<a class="reference external" href="https://arxiv.org/abs/2302.10289#">ghosh, …, batmangehelich, 2023</a>) - mixture of different interpretable models, with black-box routing</p></li>
<li><p>SASC (<a class="reference external" href="https://arxiv.org/abs/2305.09863">singh, …, gao, 2023</a>) - learn factors from BERT using dictionary learning, assign each factor a natural-language explanation, then build a sparse linear model of these factors</p></li>
<li><p>Towards Robust Interpretability with Self-Explaining Neural Networks (<a class="reference external" href="https://arxiv.org/pdf/1806.07538.pdf">alvarez-melis &amp; jaakkola 2018</a>) - use regularization to ensure model is aligned with concepts</p></li>
</ul>
</section>
<section id="localization">
<h4><span class="section-number">7.9.2.3.2. </span>localization<a class="headerlink" href="#localization" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation (<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.html">durand et al. 2017</a>) - constrains architecture: after extracting conv features, replace linear layers with special pooling layers, which helps with spatial localization</p>
<ul>
<li><ul>
<li><p>each class gets a pooling map</p></li>
<li><p>prediction for a class is based on top-k spatial regions for a class</p></li>
<li><p>finally, can combine the predictions for each class</p></li>
</ul>
</li>
</ul>
</li>
<li><p>BagNet: Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet (<a class="reference external" href="https://arxiv.org/abs/1904.00760">brendel &amp; bethge, 2019</a> - CNN is restricted to look at very local features only and still does well (and produces an inbuilt saliency measure)</p>
<ul>
<li><p>Models are generally biased to learn shapes not texture (<a class="reference external" href="https://openreview.net/pdf?id=Bygh9j09KX">geirhos…brendel, 2019</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="example-based-case-based-e-g-prototypes-nearest-neighbor">
<h4><span class="section-number">7.9.2.3.3. </span>example-based = case-based (e.g. prototypes, nearest neighbor)<a class="headerlink" href="#example-based-case-based-e-g-prototypes-nearest-neighbor" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>ProtoPNet: This looks like that (<a class="reference external" href="https://arxiv.org/abs/1806.10574">chen, …, rudin, 2018</a>; 2nd prototypes paper) - learn convolutional prototypes that are smaller than the original input size</p>
<ul>
<li><p>use L2 distance in repr space to measure distance between patches and prototypes</p></li>
<li><p>loss function</p>
<ul>
<li><p>require the filters to be identical to the latent representation of some training image patch</p></li>
<li><p>cluster image patches of a particular class around the prototypes of the same class, while separating image patches of different classes</p></li>
</ul>
</li>
<li><p>maxpool class prototypes so spatial location doesn’t matter</p>
<ul>
<li><p>also get heatmap of where prototype was activated (only max really matters)</p></li>
</ul>
</li>
<li><p>train in 3 steps</p>
<ul>
<li><p>train everything: classification + clustering around intraclass prototypes + separation between interclass prototypes (last layer fixed to 1s / -0.5s)</p></li>
<li><p>project prototypes to data patches</p></li>
<li><p>learn last (linear) layer</p></li>
</ul>
</li>
<li><p>ProtoNets (<a class="reference external" href="https://arxiv.org/pdf/1710.04806.pdf">li, …, rudin, 2017</a>; 1st prototypes paper)</p>
<ul>
<li><p>uses encoder/decoder setup</p></li>
<li><p>encourage every prototype to be similar to at least one encoded input</p></li>
<li><p>results: learned prototypes in fact look like digits &amp; correct class prototypes go to correct classes</p></li>
<li><p>loss: classification + reconstruction + distance to a training point</p></li>
</ul>
</li>
<li><p>Concept Whitening for Interpretable Image Recognition (<a class="reference external" href="https://arxiv.org/pdf/2002.01650.pdf">chen, bei, &amp; rudin, 2020</a>) - force network to separate “concepts” (like in TCAV) along different axes</p></li>
</ul>
</li>
<li><p>This Looks Like That, Because … Explaining Prototypes for Interpretable Image Recognition (<a class="reference external" href="https://arxiv.org/abs/2011.02863">nauta…seifert 2020</a>) - add textual quantitative information about visual characteristics deemed important by the classification model e.g. colour hue, shape, texture, contrast and saturation</p>
<ul>
<li><p>Neural Prototype Trees for Interpretable Fine-Grained Image Recognition (<a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html">nauta…seifert 2021</a>) - build decision trees on top of prototypes - performance is slightly poor until they use ensembles</p></li>
</ul>
</li>
<li><p>Concept transformers (<a class="reference external" href="https://openreview.net/pdf?id=kAa9eDS0RdO">rigotti… scotton, 2022</a>) - use human-given concepts and explain predictions as a function of these concepts</p></li>
<li><p>applications to medical imaging</p>
<ul>
<li><p>IAIA-BL: A Case-based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography (<a class="reference external" href="https://arxiv.org/abs/2103.12308">barnet…rudin, 2021</a>) - improve algorithm specifically for mammography</p></li>
<li><p>XProtoNet: Diagnosis in Chest Radiography With Global and Local Explanations (<a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2021/html/Kim_XProtoNet_Diagnosis_in_Chest_Radiography_With_Global_and_Local_Explanations_CVPR_2021_paper.html">eunji kim…yoon, 2021</a>) - alter ProtoPNet to use dynamically sized patches for prototype matching rather than fixed-size patches</p></li>
<li><p>PIP-NET (<a class="reference external" href="https://arxiv.org/pdf/2307.10404.pdf">nauta…seifert, 2023</a>) - applying prototypes to medical image classification</p></li>
<li><p>ProtoMIL: Multiple Instance Learning with Prototypical Parts for Fine-Grained Interpretability (<a class="reference external" href="https://arxiv.org/abs/2108.10612">Rymarczyk et al. 2021</a>)</p></li>
</ul>
</li>
<li><p>AutoProtoNet: Interpretability for Prototypical Networks (<a class="reference external" href="https://arxiv.org/abs/2204.00929">sandoval-segura &amp; lawson, 2022</a>)</p>
<ul>
<li><p>builds interpretability into Prototypical Networks by training an embedding space suitable for reconstructing inputs</p></li>
<li><p>also devise a prototype refinement method, which allows a human to debug inadequate classification parameters</p></li>
</ul>
</li>
<li><p>TesNet: Interpretable Image Recognition by Constructing Transparent Embedding Space (<a class="reference external" href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interpretable_Image_Recognition_by_Constructing_Transparent_Embedding_Space_ICCV_2021_paper.html">wang et al. 2021</a>) - alter ProtoPNet to get “orthogonal” basis concepts</p></li>
<li><p>ProtoPShare: Prototype Sharing for Interpretable Image Classification and Similarity Discovery (<a class="reference external" href="https://arxiv.org/abs/2011.14340">Rymarczyk et al. 2020</a>),- share some prototypes between classes with data-dependent merge pruning</p>
<ul>
<li><p>merge “similar” prototypes, where similarity is measured as dist of all training patches in repr. space</p></li>
<li><p>Interpretable Image Classification with Differentiable Prototypes Assignment (<a class="reference external" href="https://arxiv.org/abs/2112.02902">rmyarczyk et al. 2021</a>)</p></li>
</ul>
</li>
<li><p>These <em>do not</em> Look Like Those: An Interpretable Deep Learning Model for Image Recognition (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9373404">singh &amp; yow, 2021</a>) - all weights for prototypes are either 1 or -1</p></li>
<li><p>Towards Explainable Deep Neural Networks (<a class="reference external" href="https://arxiv.org/abs/1912.02523">angelov &amp; soares 2019</a>) - more complex version of using prototypes</p></li>
<li><p>Self-Interpretable Model with Transformation Equivariant Interpretation (<a class="reference external" href="https://arxiv.org/abs/2111.04927">wang &amp; wang, 2021</a>)</p>
<ul>
<li><p>generate data-dependent prototypes for each class and formulate the prediction as the inner product between each prototype and the extracted features</p>
<ul>
<li><p>interpretation is hadamard product of prototype and extracted features (prediction is sum of this product)</p></li>
</ul>
</li>
<li><p>interpretations can be easily visualized by upsampling from the prototype space to the input data space</p></li>
<li><p>regularization</p>
<ul>
<li><p>reconstruction regularizer - regularizes the interpretations to be meaningful and comprehensible</p>
<ul>
<li><p>for each image, enforce each prototype to be similar to its corresponding class’s latent repr.</p></li>
</ul>
</li>
<li><p>transformation regularizer - constrains the interpretations to be transformation equivariant</p></li>
</ul>
</li>
<li><p><em>self-consistency score</em> quantifies the robustness of interpretation by measuring the consistency of interpretations to geometric transformations</p></li>
</ul>
</li>
<li><p>ProtoAttend: Attention-Based Prototypical Learning (<a class="reference external" href="https://www.jmlr.org/papers/volume21/20-042/20-042.pdf">arik &amp; pfister, 2020</a>) - unlike ProtoPNet, each prediction is made as a weighted combination of similar input samples (like nearest-neighbor)</p></li>
<li><p>Explaining Latent Representations with a Corpus of Examples (<a class="reference external" href="https://arxiv.org/pdf/2110.15355.pdf">crabbe, …, van der schaar 2021</a>) - for an individual prediction,</p>
<ol class="arabic simple">
<li><p>Which corpus examples explain the prediction issued for a given test example?</p></li>
<li><p>What features of these corpus examples are relevant for the model to relate them to the test example?</p></li>
</ol>
</li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9346401">Interpreting Deep Neural Networks through Prototype Factorization</a> - posthoc convert DNN into having different factors</p></li>
<li><p>ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition (<a class="reference external" href="https://arxiv.org/abs/2208.10431">xue et al. 2022</a>)</p></li>
<li><p>Learning Optimally Sparse Support Vector Machines (<a class="reference external" href="http://proceedings.mlr.press/v28/cotter13.pdf">cotter, shalev-shwartz, &amp; srebro, 2013</a>) - minimize number of support vectors</p>
<ul>
<li><p>Counterfactual Explanations for Support Vector Machine Models (<a class="reference external" href="https://arxiv.org/abs/2212.07432">salazar et al. 2022</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="connecting-dnns-and-rules">
<h4><span class="section-number">7.9.2.3.4. </span>connecting dnns and rules<a class="headerlink" href="#connecting-dnns-and-rules" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>TAO: Alternating optimization of decision trees, with application to learning sparse oblique trees (<a class="reference external" href="https://proceedings.neurips.cc/paper/2018/hash/185c29dc24325934ee377cfda20e414c-Abstract.html">carreira-perpinan, tavallali, 2018</a>)</p>
<ul>
<li><p>Minimize loss function with sparsity of features at each node + predictive performance</p></li>
<li><p>Algorithm: update each node one at a time while keeping all others fixed (finds a local optimum of loss)</p></li>
<li><p>Fast for 2 reasons</p>
<ul>
<li><p>separability - nodes which aren’t on same path from root-to-leaf can be optimized separately</p></li>
<li><p>reduced problem - for any given node, <em>we only solve a binary classification</em> where the labels are the predictions that a point would be given if it were sent left/right</p>
<ul>
<li><p>in this work, solve binary classification by approximating it with sparse logistic regression</p></li>
</ul>
</li>
</ul>
</li>
<li><p>TAO trees with boosting performs well + works for regression (<a class="reference external" href="https://faculty.ucmerced.edu/mcarreira-perpinan/papers/icml20.pdf">zharmagambetov and carreira-perpinan, 2020</a>)</p></li>
<li><p>TAO trees with bagging performs well (<a class="reference external" href="http://graduatestudents.ucmerced.edu/azharmagambetov/files/papers/fods20.pdf">carreira-perpiñán &amp; zharmagambetov, 2020</a>)</p></li>
<li><p>Learning a Tree of Neural Nets (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9413718">zharmagambetov and carreira-perpinan, 2020</a>) - use neural net rather than binary classification at each node</p></li>
<li><p>Also use TAO trained on neural net features do speed-up/improve the network</p></li>
</ul>
</li>
<li><p>incorporating prior knowledge</p>
<ul>
<li><p>DeepCTRL: Controlling Neural Networks with Rule Representations (<a class="reference external" href="https://arxiv.org/abs/2106.07804">seo…pfister, 21</a>)</p>
<ul>
<li><p>one encoder for rules, one for data</p>
<ul>
<li><p>both are concatenated with stochastic parameter <span class="math notranslate nohighlight">\(\alpha\)</span> (which also weights the loss)</p></li>
<li><p>at test-time, can select <span class="math notranslate nohighlight">\(\alpha\)</span> to vary contribution of rule part (e.g. if rule doesn’t apply to a certain point)</p></li>
</ul>
</li>
<li><p>training</p>
<ul>
<li><p>normalize losses initially to ensure they are on the same scale</p></li>
<li><p>some rules can be made differentiable in a straightforward way: <span class="math notranslate nohighlight">\(r(x, \hat y) \leq \tau \to \max (r(x, \hat y ) - \tau, 0)\)</span>, but can’t do this for everything e.g. decision tree rules</p></li>
<li><p>rule-based loss is defined by looking at predictions for perturbations of the input</p></li>
</ul>
</li>
<li><p>evaluate the <code class="docutils literal notranslate"><span class="pre">verification</span> <span class="pre">ratio</span></code>= fraction of samples that satisfy the rule</p></li>
<li><p>see also Lagrangian Duality for Constrained Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2001.09394">fioretto et al. 2020</a>)</p></li>
</ul>
</li>
<li><p>RRL: A Scalable Classifier for Interpretable Rule-Based Representation Learning (<a class="reference external" href="https://openreview.net/forum?id=UwOMufsTqCy">wang et al. 2020</a>)</p>
<ul>
<li><p>Rule-based Representation Learner (RRL) - automatically learns interpretable non-fuzzy rules for data representation</p></li>
<li><p>project RRL it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent</p></li>
</ul>
</li>
<li><p>Harnessing Deep Neural Networks with Logic Rules (<a class="reference external" href="https://arxiv.org/pdf/1603.06318.pdf">hu, …, xing, 2020</a>) - iterative distillation method that transfers the structured information of logic rules into the weights of neural networks</p></li>
</ul>
</li>
<li><p>soft decision trees</p>
<ul>
<li><p>Neural Random Forests (<a class="reference external" href="https://link.springer.com/article/10.1007/s13171-018-0133-y">biau et al. 2018</a>) - convert RF to DNN</p>
<ul>
<li><p>first layer learns a node for each split</p></li>
<li><p>second layer learns a node for each leaf (by only connecting to nodes on leaves in the path)</p></li>
<li><p>finally map each leaf to a value</p></li>
<li><p>relax + retrain</p></li>
<li><p>Gradient Boosted Decision Tree Neural Network (<a class="reference external" href="https://arxiv.org/abs/1910.09340">saberian…raimond, 2019</a>) - build DNN based on decision tree ensemble - basically the same but with gradient-boosted trees</p></li>
</ul>
</li>
<li><p>Distilling a DNN Into a Soft Decision Tree (<a class="reference external" href="https://arxiv.org/pdf/1711.09784.pdf">frosst &amp; hinton 2017</a>) - distills DNN into DNN-like tree which uses sigmoid neuron to decide which path to follow</p>
<ul>
<li><p>training on distilled DNN predictions outperforms training on original labels</p></li>
<li><p>to make the decision closer to a hard cut, can multiply by a large scalar before applying sigmoid</p></li>
<li><p>parameters updated with backprop</p></li>
<li><p>regularization to ensure that all paths are taken equally likely</p></li>
</ul>
</li>
<li><p>these papers are basically learning trees via backprop, where splits are “smoothed” during training</p>
<ul>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007/s11263-019-01237-6">End-to-End Learning of Decision Trees and Forests</a>; <a class="reference external" href="https://ieeexplore.ieee.org/document/9393908">SDTR: Soft Decision Tree Regressor for Tabular Data</a></p></li>
<li><p>Deep Neural Decision Forests (<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf">2015</a>)</p>
<ul>
<li><p>dnn learns small intermediate representation, which outputs all possible splits in a tree</p></li>
<li><p>these splits are forced into a tree-structure and optimized via SGD</p>
<ul>
<li><p>neurons use sigmoid function</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Neural Decision Trees (<a class="reference external" href="https://arxiv.org/abs/1702.07360">balestriero, 2017</a>) - treat each neural net like a node in a tree</p></li>
<li><p>Differentiable Pattern Set Mining (<a class="reference external" href="http://eda.mmci.uni-saarland.de/pubs/2021/binaps-fischer,vreeken.pdf">fischer &amp; vreeken, 2021</a>)</p>
<ul>
<li><p>use neural autoencoder with binary activations + binarizing weights</p></li>
<li><p>optimizing a data-sparsity aware reconstruction loss, continuous versions of the weights are learned in small, noisy steps</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Extracting rules from (recurrent) DNNs</p>
<ul>
<li><p><a class="reference external" href="https://github.com/mateoespinosa/remix">set of methods</a> for extracting rules from DNNN</p></li>
<li><p>Automatic Rule Extraction from Long Short Term Memory Networks (<a class="reference external" href="https://arxiv.org/abs/1702.02540">murdoch &amp; szlam, 2017</a>) - extract out phrases using feature importance</p></li>
<li><p>A Comparative Study of Rule Extraction for Recurrent Neural Networks (<a class="reference external" href="https://arxiv.org/abs/1801.05420">wang et al. 2018</a>) - create automata based on interpretable states to track RNNs</p></li>
<li><p>Efficient Decompositional Rule Extraction for Deep Neural Networks (<a class="reference external" href="https://arxiv.org/abs/2111.12628">zarlenga…jamnik, 2021</a>)</p></li>
</ul>
</li>
<li><p>Learning Binary Decision Trees by Argmin Differentiation (<a class="reference external" href="https://arxiv.org/abs/2010.04627">zantedeschi et al. 2021</a>)</p>
<ul>
<li><p>argmin differentiation - solving an optimization problem as a differentiable module within a parent problem tackled with gradient-based optimization methods</p></li>
<li><p>relax hard splits into soft ones and learn via gradient descent</p></li>
</ul>
</li>
<li><p>Optimizing for Interpretability in DNNs with Tree Regularization (<a class="reference external" href="https://www.jair.org/index.php/jair/article/view/12558">wu…doshi-velez, 2021</a>) - regularize DNN prediction function towards tree (potentially only for some region)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1711.06178.pdf">Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</a> - regularize so that deep model can be closely modeled by tree w/ few nodes</p></li>
</ul>
</li>
<li><p>Adaptive Neural Trees (<a class="reference external" href="http://proceedings.mlr.press/v97/tanno19a.html?utm_campaign=piqcy&amp;amp;utm_medium=email&amp;amp;utm_source=Revue%20newsletter">tanno et al. 2019</a>) - adaptive neural tree mechanism with trainable nodes, edges, and leaves</p></li>
<li><p>mixture of experts (MOE) / hierarchical MOE</p></li>
<li><p>Oblique Decision Trees from Derivatives of ReLU Networks (<a class="reference external" href="https://arxiv.org/abs/1909.13488">lee &amp; jaakkola, 2020</a>)</p>
<ul>
<li><p>locally constant networks (which are derivatives of relu networks) are equivalent to trees</p></li>
<li><p>they perform well and can use DNN tools e.g. Dropconnect on them</p></li>
<li><p>note: deriv wrt to input can be high-dim</p>
<ul>
<li><p>they define locally constant network LCN scalar prediction as derivative wrt every parameter transposed with the activations of every corresponding neuron</p></li>
<li><p>approximately locally constant network ALCN: replace Relu <span class="math notranslate nohighlight">\(\max(0, x)\)</span> with softplus <span class="math notranslate nohighlight">\(1+\exp(x)\)</span></p></li>
<li><p>ensemble these with boosting to improve performance</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="constrained-models-e-g-monotonicity">
<h3><span class="section-number">7.9.2.4. </span>constrained models (e.g. monotonicity)<a class="headerlink" href="#constrained-models-e-g-monotonicity" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>different constraints in <a class="reference external" href="https://www.tensorflow.org/lattice/overview">tensorflow lattice</a></p>
<ul>
<li><p>e.g. monoticity, convexity, unimodality (unique peak), pairwise trust (model has higher slope for one feature when another feature is in particular value range)</p></li>
<li><p>e.g. regularizers = laplacian (flatter), hessian (linear), wrinkle (smoother), torsion (independence between feature contributions)</p></li>
<li><p>lattice regression (<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.215.849&amp;amp;rep=rep1&amp;amp;type=pdf">garcia &amp; gupta, 2009</a>) - learn keypoints of look-up table and at inference time interpolate the table</p>
<ul>
<li><p>to learn, view as kernel method and then learn linear function in the kernel space</p></li>
</ul>
</li>
<li><p>Monotonic Calibrated Interpolated Look-Up Tables (<a class="reference external" href="https://www.jmlr.org/papers/volume17/15-243/15-243.pdf">gupta et al. 2016</a>)</p>
<ul>
<li><p>speed up <span class="math notranslate nohighlight">\(D\)</span>-dimensional interpolation to <span class="math notranslate nohighlight">\(O(D \log D)\)</span></p></li>
<li><p>follow-up work: Deep Lattice Networks and Partial Monotonic Functions (<a class="reference external" href="https://proceedings.neurips.cc//paper/2017/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">you…gupta, 2017</a>) - use many layers</p></li>
</ul>
</li>
</ul>
</li>
<li><p>neural nets</p>
<ul>
<li><p>Sparse Epistatic Regularization of Deep Neural Networks for Inferring Fitness Functions (<a class="reference external" href="https://www.biorxiv.org/content/10.1101/2020.11.24.396994v1">aghazadeh et al. 2020</a>) - directly regularize interactions / high-order freqs in DNNs</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1909.13611">MonoNet: Towards Interpretable Models by Learning Monotonic Features</a> - enforce output to be a monotonic function of individual features</p></li>
</ul>
</li>
<li><p>monotonicity constraints in histogram-based gradient boosting (<a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting">see sklearn</a>)</p></li>
<li><p>Capsules: How to represent part-whole hierarchies in a neural network (<a class="reference external" href="https://arxiv.org/abs/2102.12627">hinton, 2021</a>)</p>
<ul>
<li><p>The idea is simply to use islands of identical vectors to represent the nodes in the parse tree (parse tree would be things like wheel-&gt; cabin -&gt; car)</p></li>
<li><p>each patch / pixel gets representations at different levels (e.g. texture, parrt of wheel, part of cabin, etc.)</p>
<ul>
<li><p>each repr. is a vector - vector for high-level stuff (e.g. car) will agree for different pixels but low level (e.g. wheel) will differ</p></li>
<li><p>during training, each layer at each location gets information from nearby levels</p>
<ul>
<li><p>hinton assumes weights are shared between locations (maybe don’t need to be)</p></li>
<li><p>also attention mechanism across other locations in same layer</p></li>
</ul>
</li>
<li><p>each location also takes in its positional location (x, y)</p></li>
<li><p>could have the lowest-level repr start w/ a convnet</p></li>
</ul>
</li>
<li><p>iCaps: An Interpretable Classifier via Disentangled Capsule Networks (<a class="reference external" href="https://arxiv.org/abs/2008.08756">jung et al. 2020</a>)</p>
<ul>
<li><p>the class capsule also includes classification-irrelevant information</p>
<ul>
<li><p>uses a novel class-supervised disentanglement algorithm</p></li>
</ul>
</li>
<li><p>entities represented by the class capsule overlap</p>
<ul>
<li><p>adds additional regularizer</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="misc-models">
<h3><span class="section-number">7.9.2.5. </span>misc models<a class="headerlink" href="#misc-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1905.09688.pdf">The Convolutional Tsetlin Machine</a> - uses easy-to-interpret conjunctive clauses</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1804.01508.pdf">The Tsetlin Machine</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.perimeterinstitute.ca/research/research-initiatives/tensor-networks-initiative">Tensor networks</a> - like DNN that only takes boolean inputs and deals with interactions explicitly</p>
<ul>
<li><p>widely used in physics</p></li>
</ul>
</li>
</ul>
<section id="bayesian-models">
<h4><span class="section-number">7.9.2.5.1. </span>bayesian models<a class="headerlink" href="#bayesian-models" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>e.g. naive bayes</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.09358">Making Bayesian Predictive Models Interpretable: A Decision Theoretic Approach</a></p></li>
</ul>
</section>
<section id="programs">
<h4><span class="section-number">7.9.2.5.2. </span>programs<a class="headerlink" href="#programs" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>program synthesis</strong> - automatically find a program in an underlying programming language that satisfies some user intent</p>
<ul>
<li><p><strong>ex. program induction</strong> - given a dataset consisting of input/output pairs, generate a (simple?) program that produces the same pairs</p></li>
</ul>
</li>
<li><p>Programs as Black-Box Explanations (<a class="reference external" href="https://arxiv.org/pdf/1611.07579.pdf">singh et al. 2016</a>)</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Probabilistic_programming">probabilistic programming</a> - specify graphical models via a programming language</p></li>
</ul>
</section>
</section>
</section>
<section id="posthoc-interpretability-i-e-how-can-we-interpret-a-fitted-model">
<h2><span class="section-number">7.9.3. </span>posthoc interpretability (i.e. how can we interpret a fitted model)<a class="headerlink" href="#posthoc-interpretability-i-e-how-can-we-interpret-a-fitted-model" title="Link to this heading">#</a></h2>
<p>Note that in this section we also include <em>dataset interpretations</em> that work directly on data (rather than first fitting a model)</p>
<section id="model-agnostic">
<h3><span class="section-number">7.9.3.1. </span>model-agnostic<a class="headerlink" href="#model-agnostic" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature Removal Is a Unifying Principle for Model Explanation Methods (<a class="reference external" href="https://arxiv.org/abs/2011.03623">cover, lundberg, &amp; lee, 2020</a>) - many different methods, e.g. SHAP, LIME, meaningful perturbations, permutation tests, RISE can be viewed through feature removal</p></li>
<li><p>LIME: local surrogate (<a class="reference external" href="https://arxiv.org/abs/1602.04938">ribeiro et al. 2016</a>) - fit a simple model locally to on point and interpret that</p>
<ul>
<li><p>select data perturbations and get new predictions</p>
<ul>
<li><p>for tabular data, this is just varying the values around the prediction</p></li>
<li><p>for images, this is turning superpixels on/off</p></li>
<li><p>superpixels determined in unsupervised way</p></li>
</ul>
</li>
<li><p>weight the new samples based on their proximity</p></li>
<li><p>train a kernel-weighted, interpretable model on these points</p></li>
<li><p>LEMNA - like lime but uses lasso + small changes</p></li>
</ul>
</li>
<li><p>anchors: (<a class="reference external" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982/15850">ribeiro et al. 2018</a>) - find biggest square region of input space that contains input and preserves same output (with high precision)</p>
<ul>
<li><p>does this search via iterative rules</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1810.03805.pdf">Sufficient input subsets</a> - seek the smallest subsets of features which produce the prediction</p>
<ul>
<li><p>other features are masked or imputed</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://www.jmlr.org/papers/v11/baehrens10a.html">local-gradient</a> (bahrens et al. 2010) - direction of highest slope towards a particular class / other class</p></li>
<li><p><a class="reference external" href="https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10618-014-0368-8&amp;amp;casa_token=AhKnW6Xx4L0AAAAA:-SEMsMjDX3_rU5gyGx6plcmF5A_ufXvsWJHzjCUIGWHGW0fqOe50yhWKYOK6UIPDHQaUwEkE3RK17XOByzo">golden eye</a> (henelius et al. 2014) - randomize different groups of features and search for groups which interact</p></li>
<li><p><strong><a class="reference external" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti">shapley value</a></strong> - average marginal contribution of a feature value across all possible sets of feature values</p>
<ul>
<li><p>“how much does prediction change on average when this feature is added?”</p></li>
<li><p>tells us the difference between the actual prediction and the average prediction</p></li>
<li><p>estimating: all possible sets of feature values have to be evaluated with and without the j-th feature</p>
<ul>
<li><p>this includes sets of different sizes</p></li>
<li><p>to evaluate, take expectation over all the other variables, fixing this variables value</p></li>
</ul>
</li>
<li><p>shapley sampling value - sample instead of exactly computing</p>
<ul>
<li><p>quantitative input influence is similar to this…</p></li>
</ul>
</li>
<li><p>satisfies 3 properties</p>
<ul>
<li><p>local accuracy - basically, explanation scores sum to original prediction</p></li>
<li><p>missingness - features with <span class="math notranslate nohighlight">\(x'_i=0\)</span> have 0 impact</p></li>
<li><p>consistency - if a model changes so that some simplified input’s contribution increases or stays the same regardless of the other inputs, that input’s attribution should not decrease.</p></li>
</ul>
</li>
<li><p>interpretation: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value</p></li>
<li><p>recalculate via sampling other features in expectation</p></li>
<li><p>propagating shapley values (<a class="reference external" href="https://arxiv.org/pdf/1911.11888.pdf">chen, lundberg, &amp; lee 2019</a>) - can work with stacks of different models</p></li>
<li><p>averaging these across dataset can be misleading (<a class="reference external" href="https://aokeson.github.io/files/global_feature_attributions.pdf">okeson et al. 2021</a>)</p></li>
<li><p>Understanding Global Feature Contributions Through Additive Importance Measures (<a class="reference external" href="https://arxiv.org/abs/2004.00668">covert, lundberg, &amp; lee 2020</a>)</p>
<ul>
<li><p>SAGE score looks at reduction in predictive accuracy due to subsets of features</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/~johnhew/interpreting-probes.html">probes</a> - check if a representation (e.g. BERT embeddings) learned a certain property (e.g. POS tagging) by seeing if we can predict this property (maybe linearly) directly from the representation</p>
<ul>
<li><p>problem: if the post-hoc probe is a complex model (e.g. MLP), it can accurately predict a property even if that property isn’t really contained in the representation</p></li>
<li><p>potential solution: benchmark against control tasks, where we construct a new random task to predict given a representation, and see how well the post-hoc probe can do on that task</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1903.10464">Explaining individual predictions when features are dependent: More accurate approximations to Shapley values</a> (aas et al. 2019) - tries to more accurately compute conditional expectation</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.13413">Feature relevance quantification in explainable AI: A causal problem</a> (janzing et al. 2019) - argues we should just use unconditional expectation</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/7546525">quantitative input influence</a> - similar to shap but more general</p></li>
<li><p>permutation importance - increase in the prediction error after we permuted the feature’s values</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbb E[Y] - \mathbb E[Y\vert X_{\sim i}]\)</span></p></li>
<li><p>If features are correlated, the permutation feature importance can be biased by unrealistic data
instances (PDP problem)</p></li>
<li><p>not the same as model variance</p></li>
<li><p>Adding a correlated feature can decrease the importance of the associated feature</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1802.07814.pdf">L2X: information-theoretical local approximation</a> (chen et al. 2018) - locally assign feature importance based on mutual information with function</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1906.10670">Learning Explainable Models Using Attribution Priors + Expected Gradients</a> - like doing integrated gradients in many directions (e.g. by using other points in the training batch as the baseline)</p>
<ul>
<li><p>can use this prior to help improve performance</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1904.00045">Interpreting Black Box Models via Hypothesis Testing</a></p></li>
</ul>
<section id="variable-importances-vis">
<h4><span class="section-number">7.9.3.1.1. </span>variable importances (VIs)<a class="headerlink" href="#variable-importances-vis" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>variable importance definitions</p>
<ol class="arabic simple">
<li><p>a quantitative indicator that quantifies the change of model output value w.r.t. the change or permutation of one or a set of input variables</p></li>
<li><p>an indicator that quantifies the contribution of the uncertainties of one or a set of input variables to the uncertainty of model output variable</p></li>
<li><p>an indicator that quantifies the strength of dependence between the model output variable and one or a set of input variables.</p></li>
</ol>
</li>
<li><p>difference-based - deriv=based methods, local importance measure, morris’ screening method</p>
<ul>
<li><p><strong>LIM</strong> (local importance measure) - like LIME</p>
<ul>
<li><p>can normalize weights by values of x, y, or ratios of their standard deviations</p></li>
<li><p>can also decompose variance to get the covariances between different variables</p></li>
<li><p>can approximate derivative via adjoint method or smth else</p></li>
</ul>
</li>
<li><p><strong>morris’ screening method</strong></p>
<ul>
<li><p>take a grid of local derivs and look at the mean / std of these derivs</p></li>
<li><p>can’t distinguish between nonlinearity / interaction</p></li>
</ul>
</li>
<li><p>using the squared derivative allows for a close connection w/ sobol’s total effect index</p>
<ul>
<li><p>can extend this to taking derivs wrt different combinations of variables</p></li>
</ul>
</li>
</ul>
</li>
<li><p>parametric regression</p>
<ul>
<li><p>correlation coefficient, linear reg coeffeicients</p></li>
<li><p><strong>partial correlation coefficient</strong> (PCC) - wipe out correlations due to other variables</p>
<ul>
<li><p>do a linear regression using the other variables (on both X and Y) and then look only at the residuals</p></li>
</ul>
</li>
<li><p>rank regression coefficient - better at capturing nonlinearity</p></li>
<li><p>could also do polynomial regression</p></li>
<li><p>more techniques (e.g. relative importance analysis RIA)</p>
<ul>
<li><p>nonparametric regression</p>
<ul>
<li><p>use something like LOESS, GAM, projection pursuit</p></li>
<li><p>rank variables by doing greedy search (add one var at a time) and seeing which explains the most variance</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>hypothesis test</p>
<ul>
<li><p><strong>grid-based hypothesis tests</strong>: splitting the sample space (X, Y) into grids and then testing whether the patterns of sample distributions across different grid cells are random</p>
<ul>
<li><p>ex. see if means vary</p></li>
<li><p>ex. look at entropy reduction</p></li>
</ul>
</li>
<li><p>other hypothesis tests include the squared rank difference, 2D kolmogorov-smirnov test, and distance-based tests</p></li>
</ul>
</li>
<li><p>variance-based VI(sobol’s indices)</p>
<ul>
<li><p><strong>ANOVA decomposition</strong> - decompose model into conditional expectations <span class="math notranslate nohighlight">\(Y = g_0 + \sum_i g_i (X_i) + \sum_i \sum_{j &gt; i} g_{ij} (X_i, X_j) + \dots + g_{1,2,..., p}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(g_0 = \mathbf E (Y)\\ g_i = \mathbf E(Y \vert X_i) - g_0 \\ g_{ij} = \mathbf E (Y \vert X_i, X_j) - g_i - g_j - g_0\\...\)</span></p></li>
<li><p>take variances of these terms</p></li>
<li><p>if there are correlations between variables some of these terms can misbehave</p></li>
<li><p>note: <span class="math notranslate nohighlight">\(V(Y) = \sum_i V (g_i) + \sum_i \sum_{j &gt; i} V(g_{ij}) + ... V(g_{1,2,...,p})\)</span> - variances are orthogonal and all sum to total variance</p></li>
<li><p><a class="reference external" href="https://statweb.stanford.edu/~owen/mc/A-anova.pdf">anova decomposition basics</a> - factor function into means, first-order terms, and interaction terms</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(S_i\)</span>: <strong>Sobol’s main effect</strong> index: <span class="math notranslate nohighlight">\(=V(g_i)=V(E(Y \vert X_i))=V(Y)-E(V(Y \vert X_i))\)</span></p>
<ul>
<li><p>small value indicates <span class="math notranslate nohighlight">\(X_i\)</span> is non-influential</p></li>
<li><p>usually used to select important variables</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(S_{Ti}\)</span>: <strong>Sobol’s total effect</strong> index - include all terms (even interactions) involving a variable</p></li>
<li><p>equivalently, <span class="math notranslate nohighlight">\(V(Y) - V(E[Y \vert X_{\sim i}])\)</span></p>
<ul>
<li><p>usually used to screen unimportant variables</p>
<ul>
<li><p>it is common to normalize these indices by the total variance <span class="math notranslate nohighlight">\(V(Y)\)</span></p></li>
</ul>
</li>
<li><p>three methods for computation - Fourier amplitude sensitivity test, meta-model, MCMC</p></li>
<li><p>when features are correlated, these can be strange (often inflating the main effects)</p>
<ul>
<li><p>can consider <span class="math notranslate nohighlight">\(X_i^{\text{Correlated}} = E(X_i \vert X_{\sim i})\)</span> and <span class="math notranslate nohighlight">\(X_i^{\text{Uncorrelated}} = X_i - X_i^{\text{Correlated}}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>this can help us understand the contributions that come from different features, as well as the correlations between features (e.g. <span class="math notranslate nohighlight">\(S_i^{\text{Uncorrelated}} = V(E[Y \vert X_i^{\text{Uncorrelated}}])/V(Y)\)</span></p>
<ul>
<li><p><a class="reference external" href="https://epubs.siam.org/doi/pdf/10.1137/130936233">sobol indices connected to shapley value</a></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(SHAP_i = \underset{S, i \in S}{\sum} V(g_S) / \vert S \vert\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>efficiently compute SHAP values directly from data (<a class="reference external" href="http://proceedings.mlr.press/v119/williamson20a/williamson20a.pdf">williamson &amp; feng, 2020 icml</a>)</p></li>
</ul>
</li>
<li><p>moment-independent VI</p>
<ul>
<li><p>want more than just the variance ot the output variables</p></li>
<li><p>e.g. <strong>delta index</strong> = average dist. between <span class="math notranslate nohighlight">\(f_Y(y)\)</span> and <span class="math notranslate nohighlight">\(f_{Y \vert X_i}(y)\)</span> when <span class="math notranslate nohighlight">\(X_i\)</span> is fixed over its full distr.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\delta_i = \frac 1 2 \mathbb E \int \vert f_Y(y) - f_{Y\vert X_i} (y) \vert dy = \frac 1 2 \int \int \vert f_{Y, X_i}(y, x_i) - f_Y(y) f_{X_i}(x_i) \vert dy \,dx_i\)</span></p></li>
<li><p>moment-independent because it depends on the density, not just any moment (like measure of dependence between <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(X_i\)</span></p></li>
</ul>
</li>
<li><p>can also look at KL, max dist..</p></li>
</ul>
</li>
<li><p>graphic VI - like curves</p>
<ul>
<li><p>e.g. scatter plot, meta-model plot, regional VIMs, parametric VIMs</p></li>
<li><p>CSM - relative change of model ouput mean when range of <span class="math notranslate nohighlight">\(X_i\)</span> is reduced to any subregion</p></li>
<li><p>CSV - same thing for variance</p></li>
</ul>
</li>
<li><p>Sparse and Faithful Explanations Without Sparse Models (<a class="reference external" href="https://arxiv.org/pdf/2402.09702.pdf">sun…wang, rudin, 2024</a>) - introduce sparse explanation value (SEV) - that measure the decision sparsity of a model (defined using movements over a hypercube)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1805.04755.pdf">A Simple and Effective Model-Based Variable Importance Measure</a></p>
<ul>
<li><p>measures the feature importance (defined as the variance of the 1D partial dependence function) of one feature conditional on different, fixed points of the other feature. When the variance is high, then the features interact with each other, if it is zero, they don’t interact</p></li>
</ul>
</li>
<li><p>Learning to Explain: Generating Stable Explanations Fast (<a class="reference external" href="https://aclanthology.org/2021.acl-long.415/">situ et al. 2021</a>) - train a model on “teacher” importance scores (e.g. SHAP) and then use it to quickly predict importance scores on new examples</p></li>
<li><p>Guarantee Regions for Local Explanations (<a class="reference external" href="https://arxiv.org/abs/2402.12737v1">havasi…doshi-velez, 2024</a>) - use anchor points to find regions for which local interp methods reliably fit the full model</p></li>
</ul>
</section>
<section id="importance-curves">
<h4><span class="section-number">7.9.3.1.2. </span>importance curves<a class="headerlink" href="#importance-curves" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>pdp plots</strong> - marginals (force value of plotted var to be what you want it to be)</p>
<ul>
<li><p>separate into <strong>ice plots</strong> - marginals for instance</p>
<ul>
<li><p>average of ice plots = pdp plot</p></li>
<li><p>sometimes these are centered, sometimes look at derivative</p></li>
</ul>
</li>
<li><p>both pdp ice suffer from many points possibly not being real</p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007/s42979-021-00560-5">totalvis: A Principal Components Approach to Visualizing Total Effects in Black Box Models</a> - visualize pdp plots along PC directions</p></li>
</ul>
</li>
<li><p>possible solution: <strong>Marginal plots M-plots</strong> (bad name - uses conditional, not marginal)</p>
<ul>
<li><p>only use points conditioned on certain variable</p></li>
<li><p>problem: this bakes things in (e.g. if two features are correlated and only one important, will say both are important)</p></li>
</ul>
</li>
<li><p><strong>ALE-plots</strong> - take points conditioned on value of interest, then look at differences in predictions around a window</p>
<ul>
<li><p>this gives pure effect of that var and not the others</p></li>
<li><p>needs an order (i.e. might not work for caterogical)</p></li>
<li><p>doesn’t give you individual curves</p></li>
<li><p>recommended very highly by the book…</p></li>
<li><p>they integrate as you go…</p></li>
</ul>
</li>
<li><p>summary: To summarize how each type of plot (PDP, M, ALE) calculates the effect of a feature at a certain grid value v:</p>
<ul>
<li><p>Partial Dependence Plots: “Let me show you what the model predicts on average when each data instance has the value v for that feature. I ignore whether the value v makes sense for all data instances.”</p></li>
</ul>
</li>
<li><p>M-Plots: “Let me show you what the model predicts on average for data instances that have values close to v for that feature. The effect could be due to that feature, but also due to correlated features.”</p>
<ul>
<li><p>ALE plots: “Let me show you how the model predictions change in a small “window” of the feature around v for data instances in that window.”</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="tree-ensembles">
<h3><span class="section-number">7.9.3.2. </span>tree ensembles<a class="headerlink" href="#tree-ensembles" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>MDI = <strong>mean decrease impurity</strong> = Gini importance</p></li>
<li><p>MDA = mean decrease accuracy = <strong>permutation tests</strong>: (breiman, 20010</p>
<ul>
<li><p>conditional variable importance for random forests (<a class="reference external" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307">strobl et al. 2008</a>)</p>
<ul>
<li><p>propose permuting conditioned on the values of variables not being permuted</p>
<ul>
<li><p>to find region in which to permute, define the grid within which the values of <span class="math notranslate nohighlight">\(X_j\)</span> are permuted for each tree by means of the partition of the feature space induced by that tree</p></li>
</ul>
</li>
<li><p>many scores (such as MDI, MDA) measure marginal importance, not conditional importance</p>
<ul>
<li><p>as a result, correlated variables get importances which are too high</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Extracting Optimal Explanations for Ensemble Trees via Logical Reasoning (<a class="reference external" href="https://arxiv.org/abs/2103.02191">zhang et al. ‘21</a>) - OptExplain: extracts global explanation of tree ensembles using logical reasoning, sampling, + optimization</p></li>
<li><p>treeshap (<a class="reference external" href="https://arxiv.org/abs/1802.03888">lundberg, erion &amp; lee, 2019</a>): prediction-level</p>
<ul>
<li><p>individual feature attribution: want to decompose prediction into sum of attributions for each feature</p>
<ul>
<li><p>each thing can depend on all features</p></li>
</ul>
</li>
<li><p>Saabas method: basic thing for tree</p>
<ul>
<li><p>you get a pred at end</p></li>
<li><p>count up change in value at each split for each variable</p></li>
</ul>
</li>
<li><p>three properties</p>
<ul>
<li><p>local acc - decomposition is exact</p></li>
<li><p>missingness - features that are already missing are attributed no importance</p>
<ul>
<li><p>for missing feature, just (weighted) average nodes from each split</p></li>
</ul>
</li>
<li><p>consistency - if F(X) relies more on a certain feature j, <span class="math notranslate nohighlight">\(F_j(x)\)</span> should</p>
<ul>
<li><p>however Sabaas method doesn’t change <span class="math notranslate nohighlight">\(F_j(X)\)</span> for <span class="math notranslate nohighlight">\(F'(x) = F(x) + x_j\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>these 3 things iply we want shap values</p></li>
<li><p>average increase in func value when selecting i (given all subsets of other features)</p></li>
<li><p>for binary features with totally random splits, same as Saabas</p></li>
<li><p><strong>can cluster based on explanation similarity</strong> (fig 4)</p>
<ul>
<li><p>can quantitatively evaluate based on clustering of explanations</p></li>
</ul>
</li>
<li><p>their fig 8 - qualitatively can see how different features alter outpu</p></li>
<li><p>gini importance is like weighting all of the orderings</p></li>
</ul>
</li>
<li><p>Explainable AI for Trees: From Local Explanations to Global Understanding (<a class="reference external" href="https://arxiv.org/abs/1905.04610">lundberg et al. 2019</a>)</p>
<ul>
<li><p>shap-interaction scores - distribute among pairwise interactions + local effects</p></li>
<li><p>plot lots of local interactions together - helps detect trends</p></li>
<li><p>propose doing shap directly on loss function (identify how features contribute to loss instead of prediction)</p></li>
<li><p>can run supervised clustering (where SHAP score is the label) to get meaningful clusters</p>
<ul>
<li><p>alternatively, could do smth like CCA on the model output</p></li>
</ul>
</li>
</ul>
</li>
<li><p>understanding variable importances in forests of randomized trees (<a class="reference external" href="http://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-tre">louppe et al. 2013</a>) = consider fully randomized trees (all categorical, randomly pick feature at each depth, split on all possibilities)</p></li>
<li><p>Optimizable Counterfactual Explanations for Tree Ensembles (<a class="reference external" href="https://arxiv.org/pdf/1911.12199v1.pdf">lucic et al. 2019</a>)</p></li>
</ul>
</section>
<section id="neural-networks-dnns">
<h3><span class="section-number">7.9.3.3. </span>neural networks (dnns)<a class="headerlink" href="#neural-networks-dnns" title="Link to this heading">#</a></h3>
<section id="dnn-visualization">
<h4><span class="section-number">7.9.3.3.1. </span>dnn visualization<a class="headerlink" href="#dnn-visualization" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://distill.pub/2017/feature-visualization/">good summary on distill</a></p></li>
<li><p><strong>visualize intermediate features</strong></p>
<ol class="arabic simple">
<li><p>visualize filters by layer
- doesn’t really work past layer 1</p></li>
<li><p><em>decoded filter</em> - rafegas &amp; vanrell 2016
- project filter weights into the image space
- pooling layers make this harder</p></li>
<li><p><em>deep visualization</em> - yosinski 15</p></li>
<li><p>Understanding Deep Image Representations by Inverting Them (<a class="reference external" href="https://arxiv.org/abs/1412.0035">mahendran &amp; vedaldi 2014</a> ) - generate image given representation</p></li>
<li><p><a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html">pruning for identifying critical data routing paths</a> - prune net (while preserving prediction) to identify neurons which result in critical paths</p></li>
</ol>
</li>
<li><p>penalizing activations</p>
<ul>
<li><p><a class="reference external" href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0490.pdf">interpretable cnns</a> (zhang et al. 2018) - penalize activations to make filters slightly more intepretable</p>
<ul>
<li><p>could also just use specific filters for specific classes…</p></li>
</ul>
</li>
<li><p>teaching compositionality to cnns - mask features by objects</p></li>
</ul>
</li>
<li><p>approaches based on maximal activation</p>
<ul>
<li><p>images that maximally activate a feature</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1311.2901.pdf">deconv nets</a> - Zeiler &amp; Fergus (2014) use deconvnets (zeiler et al. 2011) to map features back to pixel space</p>
<ul>
<li><p>given one image, get the activations (e.g. maxpool indices) and use these to get back to pixel space</p></li>
<li><p>everything else does not depend on the original image</p></li>
<li><p>might want to use optimization to generate image that makes optimal feature instead of picking from training set</p></li>
</ul>
</li>
<li><p>before this, erhan et al. did this for unsupervised features</p></li>
<li><p>dosovitskiy et al 16 - train generative deconv net to create images from neuron activations</p></li>
<li><p>aubry &amp; russel 15 do similar thing</p></li>
<li><p><a class="reference external" href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">deep dream</a> - reconstruct image from feature map</p></li>
<li><p>could use natural image prior</p></li>
<li><p>could train deconvolutional NN</p></li>
<li><p>also called <em>deep neuronal tuning</em> - GD to find image that optimally excites filters</p></li>
</ul>
</li>
<li><p><em>neuron feature</em> - weighted average version of a set of maximum activation images that capture essential properties - rafegas_17</p>
<ul>
<li><p>can also define <em>color selectivity index</em> - angle between first PC of color distribution of NF and intensity axis of opponent color space</p></li>
<li><p><em>class selectivity index</em> - derived from classes of images that make NF</p></li>
</ul>
</li>
<li><p>saliency maps for each image / class</p>
<ul>
<li><p>simonyan et al 2014</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1812.04604.pdf">Diagnostic Visualization for Deep Neural Networks Using Stochastic Gradient Langevin Dynamics</a> - sample deep dream images generated by gan</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a> (olah et al. 2020)</p>
<ul>
<li><p>study of inceptionV1 (GoogLeNet)</p></li>
<li><p>some interesting neuron clusters: curve detectors, high-low freq detectors (useful for finding background)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://distill.pub/2020/circuits/early-vision/#group_conv2d1_complex_gabor">an overview of early vision</a> (olah et al. 2020)</p>
<ul>
<li><p>many groups</p>
<ul>
<li><p>conv2d0: gabor, color-contrast, other</p></li>
<li><p>conv2d1: low-freq, gabor-like, color contrast, multicolor, complex gabor, color, hatch, other</p></li>
<li><p>conv2d2: color contrast, line, shifted line, textures, other, color center-surround, tiny curves, etc.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://distill.pub/2020/circuits/curve-detectors/#visualizing-attribution">curve-detectors</a> (cammarata et al. 2020)</p></li>
<li><p><a class="reference external" href="https://distill.pub/2020/circuits/curve-circuits/">curve-circuits</a> (cammarata et al. 2021)</p>
<ul>
<li><p>engineering curve circuit from scratch</p></li>
<li><p><img alt="" src="../../_images/curves.png" /></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=r1xyx3R9tQ">posthoc prototypes</a></p>
<ul>
<li><p><strong>counterfactual explanations</strong> - like adversarial, counterfactual explanation describes smallest change to feature vals that changes the prediction to a predefined output</p>
<ul>
<li><p>maybe change fewest number of variables not their values</p></li>
<li><p>counterfactual should be reasonable (have likely feature values)</p></li>
<li><p>human-friendly</p></li>
<li><p>usually multiple possible counterfactuals (Rashomon effect)</p></li>
<li><p>can use optimization to generate counterfactual</p></li>
<li><p><strong>anchors</strong> - opposite of counterfactuals, once we have these other things won’t change the prediction</p></li>
</ul>
</li>
<li><p>prototypes (assumed to be data instances)</p>
<ul>
<li><p>prototype = data instance that is representative of lots of points</p></li>
<li><p>criticism = data instances that is not well represented by the set of prototypes</p></li>
<li><p>examples: k-medoids or MMD-critic</p>
<ul>
<li><p>selects prototypes that minimize the discrepancy between the data + prototype distributions</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2003.13268">Architecture Disentanglement for Deep Neural Networks</a> (hu et al. 2021) - “NAD learns to disentangle a pre-trained DNN into sub-architectures according to independent tasks”</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1906.10671">Explaining Deep Learning Models with Constrained Adversarial Examples</a></p></li>
<li><p><a class="reference external" href="http://bmvc2018.org/papers/0794.pdf">Understanding Deep Architectures by Visual Summaries</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.09085">Semantics for Global and Local Interpretation of Deep Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.07373">Iterative augmentation of visual evidence for weakly-supervised lesion localization in deep interpretability frameworks</a></p></li>
<li><p>FIDO: <a class="reference external" href="https://arxiv.org/pdf/1807.08024.pdf">explaining image classifiers by counterfactual generation</a>  - generate changes (e.g. with GAN in-filling) and see if pred actually changes</p>
<ul>
<li><p>can search for smallest sufficient region and smallest destructive region</p></li>
</ul>
</li>
</ul>
</section>
<section id="dnn-concept-based-explanations">
<h4><span class="section-number">7.9.3.3.2. </span>dnn concept-based explanations<a class="headerlink" href="#dnn-concept-based-explanations" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>TCAV: concept activation vectors (<a class="reference external" href="https://arxiv.org/abs/1711.11279">kim et al. 2018</a>)</p>
<ul>
<li><p>Given: a user-defined set of examples for a concept (e.g., ‘striped’), and random examples, labeled training-data examples for the studied class (zebras), trained DNN</p>
<ul>
<li><p>CAV - vector orthogonal to the linear classification boundary in activation space</p></li>
<li><p>TCAV uses the derivative of the CAV direction wrt input</p></li>
</ul>
</li>
<li><p>automated concept activation vectors (<a class="reference external" href="https://arxiv.org/abs/1902.03129">ghorbani, …, zhou, kim, 2019</a>) - Given a set of concept discovery images, each image is segmented with different resolutions to find concepts that are captured best at different sizes. (b) After removing duplicate segments, each segment is resized tothe original input size resulting in a pool of resized segments of the discovery images. (c) Resized segments are mapped to a model’s activation space at a bottleneck layer. To discover the concepts associated with the target class, clustering with outlier removal is performed. (d) The output of our method is a set of discovered concepts for each class, sorted by their importance in prediction</p></li>
<li><p>Discover and Cure: Concept-aware Mitigation of Spurious Correlation (<a class="reference external" href="https://www.semanticscholar.org/reader/6f0d9e2aa3f40aee571c7e35958598d4f82cdab5">wu…zou, 2023</a>)</p></li>
</ul>
</li>
<li><p>Concept Gradient: Concept-based Interpretation Without Linear Assumption (<a class="reference external" href="https://arxiv.org/pdf/2208.14966.pdf">bai…ravikumar..hsieh, 2022</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.07969">On Completeness-aware Concept-Based Explanations in Deep Neural Networks</a></p></li>
<li><p><a class="reference external" href="https://openaccess.thecvf.com/content_ECCV_2018/html/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.html">Interpretable Basis Decomposition for Visual Explanation</a> (zhou et al. 2018) - decompose activations of the input image into semantically interpretable components pre-trained from a large concept corpus</p></li>
</ul>
</section>
<section id="dnn-causal-motivated-attribution">
<h4><span class="section-number">7.9.3.3.3. </span>dnn causal-motivated attribution<a class="headerlink" href="#dnn-causal-motivated-attribution" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Explaining The Behavior Of Black-Box Prediction Algorithms With Causal Learning (<a class="reference external" href="https://arxiv.org/abs/2006.02482">sani et al. 2021</a>) - specify some interpretable features and learn a causal graph of how the classifier uses these features</p>
<ul>
<li><p><strong>partial ancestral graph (PAG)</strong> (<a class="reference external" href="https://www.jmlr.org/papers/volume9/zhang08a/zhang08a.pdf">zhang 08</a>) is a graphical representation which includes</p>
<ul>
<li><p>directed edges (X <span class="math notranslate nohighlight">\(\to\)</span> Y means X is a causal ancestor of Y)</p></li>
<li><p>bidirected edges (X <span class="math notranslate nohighlight">\(\leftrightarrow\)</span> Y means X and Y are both caused by some unmeasured common factor(s), e.g., X ← U → Y )</p></li>
<li><p>partially directed edges (X <span class="math notranslate nohighlight">\(\circ \to\)</span> Y or X <span class="math notranslate nohighlight">\(\circ-\circ\)</span> Y ) where the circle marks indicate ambiguity about whether the endpoints are arrows or tails</p></li>
<li><p>PAGs may also include additional edge types to represent selection bias</p></li>
</ul>
</li>
<li><p>given a model’s predictions <span class="math notranslate nohighlight">\(\hat Y\)</span> and some potential causes <span class="math notranslate nohighlight">\(Z\)</span>, learn a PAGE among them all</p>
<ul>
<li><p>assume <span class="math notranslate nohighlight">\(\hat Y\)</span> is a causal non-ancestor of <span class="math notranslate nohighlight">\(Z\)</span> (there is no directed path from <span class="math notranslate nohighlight">\(\hat Y\)</span> into any element of <span class="math notranslate nohighlight">\(Z\)</span>)</p></li>
<li><p>search for a PAG and not DAG bc <span class="math notranslate nohighlight">\(Z\)</span> might not include all possibly relevant variables</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Neural Network Attributions: A Causal Perspective (<a class="reference external" href="http://proceedings.mlr.press/v97/chattopadhyay19a.html">Chattopadhyay et al. 2019</a>)</p>
<ul>
<li><p>the neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented</p></li>
</ul>
</li>
<li><p>CXPlain: Causal Explanations for Model Interpretation under Uncertainty (<a class="reference external" href="https://arxiv.org/abs/1910.12336">schwab &amp; karlen, 2019</a>)</p>
<ul>
<li><p>model-agnostic - efficiently query model to figure out which inputs are most important</p></li>
<li><p>pixel-level attributions</p></li>
</ul>
</li>
<li><p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals (<a class="reference external" href="https://arxiv.org/abs/2006.00995">elezar…goldberg, 2020</a>)</p>
<ul>
<li><p>instead of simple probing, generate counterfactuals in representations and see how final prediction changes</p>
<ul>
<li><p>remove a property (e.g. part of speech) from the repr. at a layer using Iterative Nullspace Projection (INLP) (<a class="reference external" href="https://arxiv.org/abs/2004.07667">Ravfogel et al., 2020</a>)</p>
<ul>
<li><p>iteratively tries to predict the property linearly, then removes these directions</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Bayesian Interpolants as Explanations for Neural Inferences (<a class="reference external" href="https://arxiv.org/abs/2004.04198">mcmillan 20</a>)</p>
<ul>
<li><p>if <span class="math notranslate nohighlight">\(A \implies B\)</span>, <em>interpolant</em> <span class="math notranslate nohighlight">\(I\)</span> satisfies <span class="math notranslate nohighlight">\(A\implies I\)</span>, <span class="math notranslate nohighlight">\(I \implies B\)</span> and <span class="math notranslate nohighlight">\(I\)</span> expressed only using variables common to <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span></p>
<ul>
<li><p>here, <span class="math notranslate nohighlight">\(A\)</span> is model input, <span class="math notranslate nohighlight">\(B\)</span> is prediction, <span class="math notranslate nohighlight">\(I\)</span> is activation of some hidden layer</p></li>
</ul>
</li>
<li><p><em>Bayesian interpolant</em> show <span class="math notranslate nohighlight">\(P(A|B) \geq \alpha^2\)</span> when <span class="math notranslate nohighlight">\(P(I|A) \geq \alpha\)</span> and <span class="math notranslate nohighlight">\(P(B|I) \geq \alpha\)</span></p></li>
</ul>
</li>
<li><p>Towards Axiomatic, Hierarchical, and Symbolic Explanation for Deep Models (<a class="reference external" href="https://arxiv.org/abs/2111.06206">ren…zhang, 2021</a>) - summarize DNN predictions as a DAG</p>
<ul>
<li><p>DAG can be further summarized into set of AND nodes followed by OR nodes</p></li>
</ul>
</li>
</ul>
</section>
<section id="dnn-feature-importance">
<h4><span class="section-number">7.9.3.3.4. </span>dnn feature importance<a class="headerlink" href="#dnn-feature-importance" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>saliency maps</p>
<ol class="arabic simple">
<li><p>occluding parts of the image
- sweep over image and remove patches
- which patch removals had highest impact on change in class?</p></li>
<li><p>text usually uses attention maps
- ex. karpathy et al LSTMs
- ex. lei et al. - most relevant sentences in sentiment prediction</p></li>
<li><p>class-activation map (CAM) (<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html">zhou et al. 2016</a>)</p>
<ol class="arabic simple">
<li><p>sum the activations across channels (weighted by their weight for a particular class)</p></li>
<li><p>weirdness: drop negative activations (can be okay if using relu), normalize to 0-1 range</p>
<ol class="arabic simple">
<li><p>CALM (<a class="reference external" href="https://arxiv.org/pdf/2106.07861.pdf">kim et al. 2021</a>) - fix issues with normalization before by introducing a latent variable on the activations</p></li>
</ol>
</li>
</ol>
</li>
<li><p>RISE (<a class="reference external" href="https://arxiv.org/pdf/1806.07421.pdf">petsiuk et al. 2018</a>) - randomized input sampling</p>
<ol class="arabic simple">
<li><p>randomly mask the images, get prediction</p></li>
<li><p>saliency map = sum of masks weighted by the produced predictions</p></li>
</ol>
</li>
<li><p>Quantifying Attention Flow in Transformers (<a class="reference external" href="https://arxiv.org/abs/2005.00928">abnar &amp; zuidema</a>) - solves the problem where the attention of the previous layer is  combined nonlinearly with the attention of the next layer by introducing <strong>attention rollout</strong> - combining attention maps and assuming linearity</p></li>
</ol>
</li>
<li><p>gradient-based methods - visualize what in image would change class label</p>
<ul>
<li><p>gradient * input</p></li>
<li><p>integrated gradients (<a class="reference external" href="http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf">sundararajan et al. 2017</a>) - just sum up the gradients from some baseline to the image times the differ  (in 1d, this is just <span class="math notranslate nohighlight">\(\int_{x'=baseline}^{x'=x} (x-x') \cdot (f(x) - f(x'))\)</span></p>
<ul>
<li><p>in higher dimensions, such as images, we pick the path to integrate by starting at some baseline (e.g. all zero) and then get gradients as we interpolate between the zero image and the real image</p></li>
<li><p>if we picture 2 features, we can see that integrating the gradients will not just yield <span class="math notranslate nohighlight">\(f(x) - f(baseline)\)</span>, because each time we evaluate the gradient we change both features</p></li>
<li><p><a class="reference external" href="https://distill.pub/2020/attribution-baselines/">explanation distill article</a></p>
<ul>
<li><p>ex. any pixels which are same in original image and modified image will be given 0 importance</p></li>
<li><p>lots of different possible choices for baseline (e.g. random Gaussian image, blurred image, random image from the training set)</p></li>
<li><p>multiplying by <span class="math notranslate nohighlight">\(x-x'\)</span> is strange, instead can multiply by distr. weight for <span class="math notranslate nohighlight">\(x'\)</span></p>
<ul>
<li><p>could also average over distributions of baseline (this yields <strong>expected gradients</strong>)</p></li>
</ul>
</li>
<li><p>when we do a Gaussian distr., this is very similar to smoothgrad</p></li>
</ul>
</li>
</ul>
</li>
<li><p>lrp</p></li>
<li><p>taylor decomposition</p></li>
<li><p>deeplift</p></li>
<li><p>smoothgrad - average gradients around local perturbations of a point</p></li>
<li><p>guided backpropagation - springenberg et al</p></li>
<li><p>lets you better create maximally specific image</p></li>
<li><p>selvaraju 17 - <em>grad-CAM</em></p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1710.11063">grad-cam++</a></p></li>
</ul>
</li>
<li><p>competitive gradients (<a class="reference external" href="https://arxiv.org/pdf/1905.12152.pdf">gupta &amp; arora 2019</a>)</p>
<ul>
<li><p>Label  “wins” a pixel if either (a) its map assigns that pixel a positive score higher than the scores assigned by every other label ora negative score lower than the scores assigned by every other label.</p></li>
<li><p>final saliency map consists of scores assigned by the chosen label to each pixel it won, with the map containing a score 0 for any pixel it did not win.</p></li>
<li><p>can be applied to any method which satisfies completeness (sum of pixel scores is exactly the logit value)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1705.05598">Learning how to explain neural networks: PatternNet and PatternAttribution</a> - still gradient-based</p></li>
</ul>
</li>
</ul>
</section>
<section id="dnn-language-models-transformers">
<h4><span class="section-number">7.9.3.3.5. </span>dnn language models / transformers<a class="headerlink" href="#dnn-language-models-transformers" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>attention is not explanation (<a class="reference external" href="https://arxiv.org/abs/1902.10186">jain &amp; wallace, 2019</a>)</p>
<ul>
<li><p>attention is not <strong>not</strong> explanation (<a class="reference external" href="https://arxiv.org/abs/1908.04626">wiegreffe &amp; pinter, 2019</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.09379">Staying True to Your Word: (How) Can Attention Become Explanation?</a></p></li>
</ul>
</li>
<li><p>influence = pred with a word - pred with a word masked</p></li>
<li><p>attention corresponds to this kind of influence</p></li>
<li><p>deceptive attention - we can successfully train a model to make similar predictions but have different attention</p></li>
<li><p>An Empirical Examination of Local Composition in Language Models (<a class="reference external" href="https://arxiv.org/pdf/2210.03575.pdf">liu &amp; neubig, 2022</a>)</p>
<ul>
<li><p>predict phrase embedding given embeddings of children</p></li>
<li><p>phrase embedding accuracy isn’t correlated with actual semantic compositionality</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="interactions">
<h3><span class="section-number">7.9.3.4. </span>interactions<a class="headerlink" href="#interactions" title="Link to this heading">#</a></h3>
<section id="model-agnostic-interactions">
<h4><span class="section-number">7.9.3.4.1. </span>model-agnostic interactions<a class="headerlink" href="#model-agnostic-interactions" title="Link to this heading">#</a></h4>
<p>How interactions are defined and summarized is a very difficult thing to specify. For example, interactions can change based on monotonic transformations of features (e.g. <span class="math notranslate nohighlight">\(y= a \cdot b\)</span>, <span class="math notranslate nohighlight">\(\log y = \log a + \log b\)</span>). Nevertheless, when one has a specific question it can make sense to pursue finding and understanding interactions.</p>
<ul>
<li><p>basic methods</p>
<ul class="simple">
<li><p>occlusion = context-dependent = “break-down” , more faithful: score is contribution of variable of interest given all other variables (e.g. permutation test - randomize var of interest from right distr.)</p></li>
<li><p>context-free = “build-up”, less faithful: score is contribution of only variable of interest ignoring other variables</p></li>
</ul>
</li>
<li><p><em>H-statistic</em>: 0 for no interaction, 1 for complete interaction</p>
<ul>
<li><p>how much of the variance of the output of the joint partial dependence is explained by the interaction instead of the individuals</p></li>
<li><div class="math notranslate nohighlight">
\[H^2_{jk} = \underbrace{\sum_i [\overbrace{PD(x_j^{(i)}, x_k^{(i)})}^{\text{interaction}} \overbrace{- PD(x_j^{(i)}) - PD(x_k^{(i)})}^{\text{individual}}]^2}_{\text{sum over data points}} \: / \: \underbrace{\sum_i [PD(x_j^{(i)}, x_k^{(i)})}_{\text{normalization}}]^2\]</div>
</li>
<li><p>alternatively, using ANOVA decomp: <span class="math notranslate nohighlight">\(H_{jk}^2 = \sum_i g_{ij}^2 / \sum_i (\mathbb E [Y \vert X_i, X_j])^2\)</span></p></li>
<li><p>same assumptions as PDP: features need to be independent</p></li>
</ul>
</li>
<li><p>alternatives</p>
<ul class="simple">
<li><p>variable interaction networks (<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.7500&amp;amp;rep=rep1&amp;amp;type=pdf">hooker 2004</a>) - decompose pred into main effects + feature interactions</p></li>
<li><p>PDP-based feature interaction (greenwell et al. 2018)</p></li>
</ul>
</li>
<li><p>feature-screening (<a class="reference external" href="https://arxiv.org/abs/2011.12215">li, feng ruan, 2020</a>)</p>
<ul class="simple">
<li><p>want to find beta which is positive when a variable is important</p></li>
<li><p>idea: maximize difference between (distances for interclass) and (distances for intraclass)</p></li>
<li><p>using an L1 distance yields better gradients than an L2 distance</p></li>
</ul>
</li>
<li><p>ANOVA - factorial method to detect feature interactions based on differences among group means in a dataset</p>
<ul class="simple">
<li><p>Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models (<a class="reference external" href="http://proceedings.mlr.press/v108/lengerich20a.html">lengerich, tan, …, hooker, caruana, 2020</a>)</p>
<ul>
<li><p><em>pure interaction effects</em> - variance in the outcome which cannot be represented by any subset of features</p>
<ul>
<li><p>has an equivalence with the Functional ANOVA decomposition</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Automatic Interaction Detection (AID) - detects interactions by subdividing data into disjoint exhaustive subsets to model an outcome based on categorical features</p></li>
<li><p>SPEX: Scaling Feature Interaction Explanations for LLMs (<a class="reference external" href="https://arxiv.org/pdf/2502.13870#page=1.00">kang, butler, agarwal…ramachandran, yu, 2025</a>) - efficient search over binary interactions, applied to inputs of LLMs / VLMs</p></li>
<li><p>Shapley Taylor Interaction Index (STI) (Dhamdhere et al., 2019) - extends shap to all interactions</p></li>
<li><p>Faith-Shap: The Faithful Shapley Shapley Interaction Index (<a class="reference external" href="https://arxiv.org/abs/2203.00870#:~:text=Shapley%20values%2C%20which%20were%20originally,black%2Dbox%20machine%20learning%20models.">tsai, yeh, &amp; ravikumar, 2019</a>)</p>
<ul class="simple">
<li><p>SHAP axioms for interactions no longer specify a unique interaction index</p></li>
<li><p>here, adopt the viewpoint of Shapley values as coefficients of the most faithful linear approximation to the pseudo-Boolean coalition game value function</p></li>
</ul>
</li>
<li><p>gradient-based methods (originally Friedman and Popescu, 2008 then later used with many models such as logit)</p>
<ul class="simple">
<li><p>test if partial derivatives for some subset (e.g. <span class="math notranslate nohighlight">\(x_1, ..., x_p\)</span>) are nonzero $<span class="math notranslate nohighlight">\(\mathbb{E}_{\mathbf{x}}\left[\frac{\partial^p f(\mathbf{x})}{\partial x_{i_{1}} \partial x_{i_{2}} \ldots \partial x_{i_p}}\right]^{2}&gt;0\)</span>$</p></li>
<li><p>doesn’t work well for piecewise functions (e.g. Relu) and computationally expensive</p></li>
</ul>
</li>
<li><p>include interactions explicitly then run lasso (e.g. bien et al. 2013)</p></li>
<li><p>might first limit interaction search using frequent itemsets (📌 see notes on rules)</p></li>
</ul>
</section>
<section id="tree-based-interactions">
<h4><span class="section-number">7.9.3.4.2. </span>tree-based interactions<a class="headerlink" href="#tree-based-interactions" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>iterative random forest (<a class="reference external" href="https://www.pnas.org/content/115/8/1943">basu et al. 2018</a>)</p>
<ul>
<li><p>interaction scoring - find interactions as features which co-occur on paths (using RIT algorithm)</p>
<ul>
<li><p>signed iterative Random Forests (<a class="reference external" href="https://arxiv.org/abs/1810.07287">kumbier et al. 2018</a>)</p></li>
</ul>
</li>
<li><p>repeated refitting</p>
<ul>
<li><p>fit RF and get MDI importances</p></li>
<li><p>iteratively refit RF, weighting probability of feature being selected by its previous MDI</p></li>
</ul>
</li>
</ul>
</li>
<li><p>detecting interactions with additive groves (<a class="reference external" href="https://www.ccs.neu.edu/home/mirek/papers/2008-ICML-Interactions.pdf">sorokina, caruana, riedewald, &amp; fink, 2008</a>) - use random forest with and without an interaction (forcibly removed) to detect feature interactions - very slow</p></li>
<li><p>GUIDE: Regression trees with unbiased variable selection and interaction detection (<a class="reference external" href="https://www.jstor.org/stable/24306967?casa_token=c5imnuT3UTcAAAAA%3ARq78kxZpxxwcqL4DyuMwb5PHsQmDgQcsEnOlTwzvvS7xRCJGrcr-ABWR9XDuoP_d3D2puv7HwzEBZCibKyytW4iwuYFXIaBGEnrY4gKT90E3aavH0w">loh, 2002</a>) - tests pairwise interactions based on the <span class="math notranslate nohighlight">\(\chi^2\)</span> test</p></li>
<li><p>FAST (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/2487575.2487579">lou, caruana, gehrke, &amp; hooker, 2013</a>) - given model of marginal curves, test adding a pairwise interaction using a restricted tree that makes exacttly one split on each of the two interacting features</p></li>
<li><p>SBART: Bayesian regression tree ensembles that adapt to smoothness and sparsity (<a class="reference external" href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12293?casa_token=Pj1jdf_Cdy0AAAAA%3AWLdBFv0Y9oC5i8CUfAFDWuXR9WCww4Vne9pQazuUhIq7cFiaUZhUe8g3NlzydWch1WHtMtcSa95Q66lqGw">linero &amp; yang, 2018</a>) - adapts BART to sparsity</p></li>
<li><p>DP-Forests: bayesian decision tree ensembles for interaction detection (<a class="reference external" href="https://arxiv.org/abs/1809.08524">du &amp; linero, 2018</a>)</p>
<ul>
<li><p>Bayesian tree ensembles (e.g. BART) generally detect too many (high-order) interactions</p></li>
<li><p>Dirichlet-process forests (DP-Forests) emphasize low-order interactions</p>
<ul>
<li><p>create groups of trees which each use non-overlapping features</p></li>
<li><p>hopefully, each group learns a single low-order interaction</p></li>
<li><p>dirichlet process prior is used to pick the number of groups</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> parameter for Dirichlet process describes how strongly to enforce this grouping</p></li>
</ul>
</li>
</ul>
</li>
<li><p>interaction definition: <span class="math notranslate nohighlight">\(x_{j}\)</span> and <span class="math notranslate nohighlight">\(x_{k}\)</span> are interact if <span class="math notranslate nohighlight">\(f_{0}(x)\)</span> cannot be written as <span class="math notranslate nohighlight">\(f_{0}(x)=f_{0 \backslash j}(x)+f_{0 \backslash k}(x)\)</span> where <span class="math notranslate nohighlight">\(f_{0 \backslash j}\)</span> and <span class="math notranslate nohighlight">\(f_{0 \backslash k}\)</span> do not depend on <span class="math notranslate nohighlight">\(x_{j}\)</span> and <span class="math notranslate nohighlight">\(x_{k}\)</span> respectively</p></li>
</ul>
</li>
</ul>
</section>
<section id="dnn-interactions">
<h4><span class="section-number">7.9.3.4.3. </span>dnn interactions<a class="headerlink" href="#dnn-interactions" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>ACD: hierarchical interpretations for neural network predictions (<a class="reference external" href="https://arxiv.org/abs/1806.05337">singh et al. 2019</a>)</p>
<ul>
<li><p>contextual decomposition (<a class="reference external" href="https://arxiv.org/abs/1801.05453">murdoch et al. 2018</a>)</p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=BkxRRkSKwr">Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models</a></p></li>
<li><p>Compositional Explanations for Image Classifiers (<a class="reference external" href="https://arxiv.org/abs/2103.03622">chockler et al. 21</a>) - use perturbation-based interpretations to greedily search for pixels which increase prediction the most (simpler version of ACD)</p></li>
</ul>
</li>
<li><p>Interpretable Artificial Intelligence through the Lens of Feature Interaction (<a class="reference external" href="https://arxiv.org/abs/2103.03103">tsang et al. 2021</a>)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">interaction</span></code>= any non-additive effect between multiple features on an outcome (i.e. cannot be decomposed into a sum of subfunctions of individual variables)</p></li>
<li><p>Detecting Statistical Interactions from Neural Network Weights (<a class="reference external" href="https://arxiv.org/abs/1705.04977">tsang et al. 2018</a>) - interacting inputs must follow strongly weighted connections to a common hidden unit before the final output</p>
<ul>
<li><p>Neural interaction transparency (NIT) (<a class="reference external" href="https://dl.acm.org/citation.cfm?id=3327482">tsang et al. 2017</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Explaining Explanations: Axiomatic Feature Interactions for Deep Networks (<a class="reference external" href="https://arxiv.org/abs/2002.04138">janizek et al. 2020</a>) - integrated hessians - makes the distinction between main &amp; interaction effects unclear</p></li>
<li><p>Learning Global Pairwise Interactions with Bayesian Neural Networks (<a class="reference external" href="https://arxiv.org/abs/1901.08361">cui et al. 2020</a>) - Bayesian Group Expected Hessian (GEH) - train bayesian neural net and analyze hessian to understand interactions</p></li>
<li><p>Feature Interactions Reveal Linguistic Structure in Language Models (<a class="reference external" href="https://arxiv.org/pdf/2306.12181.pdf">jumelet &amp; zuidema, 2023</a>) - evaluate ability to find interactions using synthetic data / models</p></li>
</ul>
</section>
<section id="linear-interactions">
<h4><span class="section-number">7.9.3.4.4. </span>linear interactions<a class="headerlink" href="#linear-interactions" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>often use “hierarchy” or “heredity” constraint - means interaction term is only added when both main effects are also included</p></li>
<li><p>hierarchical group lasso (<a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/26759522/">lim &amp; hastie, 2015</a>) - learns pairwise interactions in a linear model using a hierarchy constraint</p></li>
<li><p>VANISH: Variable Selection Using Adaptive Nonlinear Interaction Structures in High Dimensions (<a class="reference external" href="https://www.tandfonline.com/doi/abs/10.1198/jasa.2010.tm10130?casa_token=HhKY3HXj0fYAAAAA:vTRgqAqWy3DZ9r9vXEinQOZbWuctLPA3J9bACTbrnKIkUPV19yqaDV5zr9dD6IiTrYXsj6HT_kDYNN8">radchenko &amp; james, 2012</a>) - learns pairwise interactions via basis function expansions</p>
<ul>
<li><p>uses hierarchiy constraint</p></li>
</ul>
</li>
<li><p>Coefficient tree regression: fast, accurate and interpretable predictive modeling (<a class="reference external" href="https://link-springer-com.libproxy.berkeley.edu/article/10.1007/s10994-021-06091-7">surer, apley, &amp; malthouse, 2021</a>) - iteratively group linear terms with similar coefficients into a bigger term</p></li>
</ul>
</section>
</section>
<section id="finding-influential-examples">
<h3><span class="section-number">7.9.3.5. </span>finding influential examples<a class="headerlink" href="#finding-influential-examples" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>influential instances - want to find important data points</p></li>
<li><p>deletion diagnostics - delete a point and see how much it changed</p></li>
<li><p>influence funcs (<a class="reference external" href="https://arxiv.org/abs/1703.04730">koh &amp; liang, 2017</a>): use <strong>Hessian</strong> (<span class="math notranslate nohighlight">\(\in \mathbb{R}^{\theta   \times \theta}\)</span>) to estimate the effect of upweighting a point</p>
<ul>
<li><p>influence functions = inifinitesimal approach - upweight one point by infinitesimally small weight and see how much estimate changes (e.g. calculate first derivative)</p></li>
<li><p>influential instance - when data point removed, has a strong effect on the model (not necessarily same as an outlier)</p></li>
<li><p>requires access to gradient (e.g. nn, logistic regression)</p></li>
<li><p>take single step with Newton’s method after upweighting loss</p></li>
<li><p>measure change in parameters by removing one point</p></li>
<li><p>measure change in loss at one point by removing a different point (by multiplying above by chain rule)</p></li>
<li><p>measure change in parameters by modifying one point</p></li>
</ul>
</li>
<li><p>Representer Point Selection for Explaining Deep Neural Networks (<a class="reference external" href="https://arxiv.org/abs/1811.09720">yeh, … , ravikumar, 2018</a>)</p>
<ul>
<li><p><strong>representer values</strong> - decompose pre-activation single prediction of a DNN into linear combinations of training points - weights are the representer values</p>
<ul>
<li><p>much faster than influence funcs &amp; gives signed values</p></li>
</ul>
</li>
<li><p>a <strong>representer theorem</strong> is any of several related results stating that a minimizer <span class="math notranslate nohighlight">\(f^*\)</span> of a regularized <a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">empirical risk functional</a> defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data</p></li>
</ul>
</li>
<li><p>Revisiting Methods for Finding Influential Examples (<a class="reference external" href="https://arxiv.org/pdf/2111.04683.pdf">k &amp; sogaard, 2021</a>)</p>
<ul>
<li><p>argue that influence should be estimated relative to model states and data samples = <code class="docutils literal notranslate"><span class="pre">expected</span> <span class="pre">influence</span> <span class="pre">scores</span></code></p></li>
<li><p>different ways to compute the influence of a training example <span class="math notranslate nohighlight">\(x_{train}\)</span> on a test example <span class="math notranslate nohighlight">\(x_{test}\)</span></p></li>
<li><p><em>leave-one-out</em>: calculate loss on <span class="math notranslate nohighlight">\(x_{test}\)</span> when trained on all points except <span class="math notranslate nohighlight">\(x_{\text{train}}\)</span> - loss when trained on all points</p></li>
<li><p><em>influence functions</em>: <span class="math notranslate nohighlight">\(I\left(x_{\text {train }}, x_{\text {test }}\right)=\)</span> <span class="math notranslate nohighlight">\(\nabla_{\theta} L\left(x_{\text {test }}, \hat{\theta}\right)^{T} H_{\hat{\theta}}^{-1} \nabla_{\theta} L\left(x_{\text {train }}, \hat{\theta}\right)\)</span>, where <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the weights of the trained model and <span class="math notranslate nohighlight">\(H_{\hat{\theta}}=\frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta}^{2} L\left(x_{i}, \hat{\theta}\right)\)</span></p></li>
<li><p><em>TraceInIdeal</em> (pruthi et al. 2020): train with batch size 1, measure change in loss of <span class="math notranslate nohighlight">\(x_{test}\)</span> after taking a training step with <span class="math notranslate nohighlight">\(x_{train}\)</span></p>
<ul>
<li><p><em>TraceInCP</em> approximates this using larger batch sizes</p></li>
</ul>
</li>
<li><p><em>representer point selection</em>: <span class="math notranslate nohighlight">\(I\left(x_{\text {train }}, x_{\text {test }}\right)=\frac{-1}{2 \lambda n} \frac{\partial L\left(x_{\text {train }}, \hat{\theta}\right)}{\partial \Phi\left(x_{\text {train }}, \hat{\theta}\right)} f\left(x_{\text {train }}\right)^{T} f\left(x_{\text {test }}\right)\)</span></p></li>
<li><p><em>Grad-dot</em>: <span class="math notranslate nohighlight">\(I\left(x_{\text {train }}, x_{t e s t}\right)=\nabla_{\theta} L\left(x_{\text {train }}, \hat{\theta}\right)^{T} \nabla_{\theta} L\left(x_{\text {test }}, \hat{\theta}\right) .\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="model-summarization-distillation">
<h3><span class="section-number">7.9.3.6. </span>model summarization / distillation<a class="headerlink" href="#model-summarization-distillation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1801.08640">Considerations When Learning Additive Explanations for Black-Box Models</a></p>
<ul>
<li><p>when we have a nonlinear model or correlated components, impossible to uniquely distill it into an additive surrogate model</p></li>
<li><p>4 potential additive surrogate models:</p>
<ul>
<li><p>distilled additive - tend to be the most faithful</p></li>
<li><p>partial-dependence</p></li>
<li><p>Shapley explanations (averaged)</p></li>
<li><p>gradient-based</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1806.10270.pdf">piecewise linear interp</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1908.06214">Computing Linear Restrictions of Neural Networks</a> - calculate function of neural network restricting its points to lie on a line</p></li>
<li><p>Interpreting CNN Knowledge via an Explanatory Graph (<a class="reference external" href="https://arxiv.org/abs/1708.01785">zhang et al. 2017</a>) - create a graph that responds better to things like objects than individual neurons</p></li>
<li><p>ModelDiff: A Framework for Comparing Learning Algorithms (<a class="reference external" href="https://arxiv.org/abs/2211.12491">shah, …, ilyas, madry, 2022</a>) - find the difference between 2 learned models by searching for feature transformations where they differ</p></li>
</ul>
<p><em>usually distillation refers to training a surrogate model on the predictions of the original model, but it is sometimes used more loosely to refer to using information from an original model to inform a surrogate/student model</em></p>
<ul class="simple">
<li><p>model-agnostic model distillation</p>
<ul>
<li><p>Trepan - approximate model w/ a decision tree</p></li>
<li><p>BETA (<a class="reference external" href="https://arxiv.org/abs/1707.01154">lakkaraju et al. 2017</a>) - approximate model with a rule list</p></li>
</ul>
</li>
<li><p>exact distillation</p>
<ul>
<li><p>Born-again tree ensembles (<a class="reference external" href="https://arxiv.org/pdf/2003.11132.pdf">vidal et al. 2020</a>) - efficient algorithm to exactly find a minimal tree which reproduces the predictions of a tree ensemble</p></li>
</ul>
</li>
<li><p>Knowledge Distillation as Semiparametric Inference (<a class="reference external" href="https://arxiv.org/abs/2104.09732">dao…mackey, 2021</a>)</p>
<ul>
<li><p>when KD should succeed</p>
<ul>
<li><p>probabilities more informative than labels (hinton, vinyals, &amp; dean, 2015)</p></li>
<li><p>linear students exactly mimic linear teachers (phuong &amp; lampert, 2019)</p></li>
<li><p>students can learn at a faster rate given knowledge of datapoint difficulty (lopez-paz et al. 2015)</p></li>
<li><p>regularization for kernel ridge regression (<a class="reference external" href="https://arxiv.org/pdf/2002.05715.pdf">mobahi farajtabar, &amp; bartlett, 2020</a>)</p></li>
<li><p>teacher class probabilities are proxies for the true bayes class probabilities <span class="math notranslate nohighlight">\(\mathbb E [Y|x]\)</span></p></li>
</ul>
</li>
<li><p>adjustments</p>
<ul>
<li><p>teacher underfitting <span class="math notranslate nohighlight">\(\to\)</span> loss correction</p></li>
<li><p>teacher overfitting <span class="math notranslate nohighlight">\(\to\)</span> cross-fitting (chernozhukov et al. 2018) - like cross-validation, fit student only to held-out predictions</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="different-problems-perspectives">
<h2><span class="section-number">7.9.4. </span>different problems / perspectives<a class="headerlink" href="#different-problems-perspectives" title="Link to this heading">#</a></h2>
<section id="improving-models">
<h3><span class="section-number">7.9.4.1. </span>improving models<a class="headerlink" href="#improving-models" title="Link to this heading">#</a></h3>
<p><em>nice reference list <a class="reference external" href="https://github.com/stefanoteso/awesome-explanatory-supervision">here</a></em></p>
<ul class="simple">
<li><p>Interpretations are useful: penalizing explanations to align neural networks with prior knowledge (<a class="reference external" href="https://arxiv.org/abs/1909.13584">rieger et al. 2020</a>)</p>
<ul>
<li><p>Refining Neural Networks with Compositional Explanations (<a class="reference external" href="https://arxiv.org/abs/2103.10415">yao et al. 21</a>) - human looks at saliency maps of interactions, gives natural language explanation, this is converted back to interactions (defined using IG), and then regularized</p></li>
<li><p>Dropout as a Regularizer of Interaction Effects (<a class="reference external" href="https://arxiv.org/abs/2007.00823">lengerich…caruana, 21</a>) - dropout regularizes interaction effects</p></li>
<li><p>Sparse Epistatic Regularization of Deep Neural Networks for Inferring Fitness Functions (<a class="reference external" href="https://www.biorxiv.org/content/10.1101/2020.11.24.396994v1">aghazadeh … listgarten, ramchandran, 2020</a>) - penalize DNNs spectral representation to limit learning noisy high-order interactions</p></li>
</ul>
</li>
<li><p>Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations (<a class="reference external" href="https://arxiv.org/abs/1703.03717">ross et al. 2017</a>) - selectively penalize input gradients</p></li>
<li><p>Explain and improve: LRP-inference fine-tuning for image captioning models (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1566253521001494">sun et al. 2022</a>)</p>
<ul>
<li><p>Pruning by explaining: A novel criterion for deep neural network pruning (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0031320321000868">yeom et al. 2021</a>) - use LRP-based score to prune DNN</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.07416">Understanding Misclassifications by Attributes</a></p></li>
<li><p>Improving VQA and its Explanations by Comparing Competing Explanations (<a class="reference external" href="https://arxiv.org/abs/2006.15631">wu et al. 2020</a>) - train to distinguish correct human explanations from competing explanations supporting incorrect answers</p>
<ul>
<li><p>VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions (<a class="reference external" href="https://arxiv.org/abs/1803.07464">li et al. 2018</a>) - train to jointly predict answer + generate an explanation</p></li>
<li><p>Self-Critical Reasoning for Robust Visual Question Answering (<a class="reference external" href="https://proceedings.neurips.cc/paper/2019/hash/33b879e7ab79f56af1e88359f9314a10-Abstract.html">wu &amp; mooney, 2019</a>) - use textual explanations to extract a set of important visual objects</p></li>
</ul>
</li>
</ul>
<p><strong>complementarity</strong> - ML should focus on points hard for humans + seek human input on points hard for ML</p>
<ul class="simple">
<li><p>note: goal of ML isn’t to learn categories but learn things that are associated with actions</p></li>
<li><p>Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer (<a class="reference external" href="http://papers.nips.cc/paper/7853-predict-responsibly-improving-fairness-and-accuracy-by-learning-to-defer">madras et al. 2018</a>) - adaptive rejection learning - build on rejection learning considering the strengths/weaknesses of humans</p></li>
<li><p>Learning to Complement Humans (<a class="reference external" href="https://arxiv.org/abs/2005.00582">wilder et al. 2020</a>) - 2 approaches for how to incorporate human input</p>
<ul>
<li><p>discriminative approach - jointly train predictive model and policy for deferring to human (with a cost for deferring)</p></li>
<li><p>decision-theroetic approach - train predictive model + policy jointly based on value of information (VOI)</p></li>
<li><p>do real-world experiments w/ humans to validate:  scientific discovery (a galaxy classification task) &amp; medical diagnosis (detection of breast cancer metastasis)</p></li>
</ul>
</li>
<li><p>Gaining Free or Low-Cost Transparency with Interpretable Partial Substitute (<a class="reference external" href="https://arxiv.org/pdf/1802.04346.pdf">wang, 2019</a>) - given a black-box model, find a subset of the data for which predictions can be made using a simple rule-list (<a class="reference external" href="https://scholar.google.com/citations?hl=en&amp;amp;user=KB6A0esAAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate">tong wang</a> has a few nice papers like this)</p>
<ul>
<li><p>Interpretable Companions for Black-Box Models (<a class="reference external" href="https://arxiv.org/abs/2002.03494">pan, wang, et al. 2020</a> ) - offer an interpretable, but slightly less acurate model for each decision</p>
<ul>
<li><p>human experiment evaluates how much humans are able to tolerate</p></li>
</ul>
</li>
<li><p>Hybrid Predictive Models: When an Interpretable Model Collaborates with a Black-box Model (<a class="reference external" href="https://www.jmlr.org/papers/volume22/19-325/19-325.pdf">wang &amp; lin, 2021</a>) - use interpretable model on subset where it works</p>
<ul>
<li><p>objective function considers predictive accuracy, model interpretability, and model transparency (defined as the percentage of data processed by the interpretable substitute)</p></li>
</ul>
</li>
<li><p>Partially Interpretable Estimators (PIE): Black-Box-Refined Interpretable Machine Learning (<a class="reference external" href="https://arxiv.org/abs/2105.02410">wang et al. 2021</a>) - interpretable model for individual features and black-box model captures feature interactions (on residuals)</p></li>
</ul>
</li>
<li><p>Three Maxims for Developing Human-Centered AI for Decision Making (<a class="reference external" href="https://www.proquest.com/docview/2628490400?pq-origsite=gscholar&amp;amp;fromopenview=true">bansal et al. 2022</a>; gagan bansal has a few nice papers like this)</p>
<ul>
<li><p>help users understand <em>when to trust</em> AI recommendations</p></li>
<li><p><em>preserve user’s mental model of AI’s trustworthiness</em></p></li>
<li><p>train AI to <em>optimize for team performance</em>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="human-in-the-loop-hitl">
<h3><span class="section-number">7.9.4.2. </span>human-in-the-loop (HITL)<a class="headerlink" href="#human-in-the-loop-hitl" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>rethinking human-ai interaction (<a class="reference external" href="https://jessylin.com/2020/06/08/rethinking-human-ai-interaction/">jessy lin blog post, 2020</a>)</p>
<ul>
<li><p>humans-as-backup - most common hitl system - human checks prediction if it is low confidence</p>
<ul>
<li><p>e.g. driving, content moderation, machine translation</p></li>
<li><p>this system can behave worse if humans (e.g. drivers, doctors) expect it to work autonomously</p>
<ul>
<li><p>e.g. translators in conjunction w/ google translate are worse than raw translations</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>change the loop</strong>: <em>how can humans direct where / when machine aid is helpful?</em></p>
<ul>
<li><p>e.g. instead of post-editing a machine translation, can provide auto fill-ins as a translator types</p></li>
<li><p>e.g. restrict interacting with chat bots to a set of choices</p></li>
<li><p>innteractive optimization procedures like <a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/1869086.1869098">human-guided search</a> - provide mechanisms for people to provide “soft” constraints and knowledge by adding constraints as the search evolves or intervening by manually modifying computer-generated solutions</p></li>
</ul>
</li>
<li><p><strong>change the inputs</strong>: <em>how can we make it more natural for humans to specify what they want?</em></p>
<ul>
<li><p>refinement tools (e.g. <a class="reference external" href="https://arxiv.org/abs/1902.02960">Human-Centered Tools for Coping with Imperfect Algorithms during Medical Decision-Making</a>) - easily compare pathology images with images that have other similar TCAV concepts</p></li>
<li><p>heuristics (e.g. some decision rules)</p></li>
</ul>
</li>
<li><p><strong>change the outputs</strong>: <em>how can we help humans understand and solve their own problems?</em></p>
<ul>
<li><p>e.g. use ML in agent-based modeling</p></li>
</ul>
</li>
</ul>
</li>
<li><p>A Survey of Human-in-the-loop for Machine Learning (<a class="reference external" href="https://arxiv.org/abs/2108.00941">wu…he, 2021</a>)</p>
<ul>
<li><p>HITL data processing, ex. identify key examples to annotate to improve performance / reduce discriminatory bias</p></li>
<li><p>HITL during training</p></li>
</ul>
</li>
<li><p>Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9552218">zhao et al. 2021</a>)</p>
<ul>
<li><p>human knowledge and feedback are combined to train a concept extractor</p></li>
<li><p>by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance</p></li>
</ul>
</li>
<li><p>Making deep neural networks right for the right scientific reasons by interacting with their explanations (<a class="reference external" href="https://www.nature.com/articles/s42256-020-0212-3">schramowski…kersting, 2020</a>) - scientist iteratively provides feedback on DNN’s explanation</p>
<ul>
<li><p>Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations (<a class="reference external" href="https://arxiv.org/abs/2112.02290">stammer…scharmowski, kersting, 2021</a>) - humans provide weak supervision, tested on toy datasets</p></li>
</ul>
</li>
<li><p>POTATO: exPlainable infOrmation exTrAcTion framewOrk (<a class="reference external" href="https://arxiv.org/abs/2201.13230">kovacs et al. 2022</a>) - humans select rules using graph-based feature for text classification</p></li>
<li><p>Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for Learned Systems (<a class="reference external" href="https://arxiv.org/abs/2006.12453">bayan &amp; mitsch, 2022</a>) - a framework for combining formal verification techniques, heuristic search, and user interaction to explore explanations at the desired level of granularity and fidelity</p>
<ul>
<li><p>asks questions about sets e.g. “when do you”, or “what do you do when”</p></li>
</ul>
</li>
</ul>
</section>
<section id="interpretable-automl">
<h3><span class="section-number">7.9.4.3. </span>(interpretable) Automl<a class="headerlink" href="#interpretable-automl" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/csinva/imodels/blob/master/notebooks/imodels_automl.ipynb">imodels_automl</a></p></li>
<li><p>Responsible AI toolbox (<a class="reference external" href="https://github.com/microsoft/responsible-ai-toolbox">github</a>)</p></li>
<li><p>auto-sklearn interpretability subset (<a class="reference external" href="https://automl.github.io/auto-sklearn/master/examples/40_advanced/example_interpretable_models.html">link</a>)</p></li>
<li><p>Automatic Componentwise Boosting: An Interpretable AutoML System (<a class="reference external" href="https://arxiv.org/abs/2109.05583">coors…rugamer, 2021</a>) - restrict AutoML to GAMs</p></li>
<li><p><strong>coreset algorithms</strong> - summarize datasets with smaller dataset that accurately represent the full dataset on downstream tasks</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.08707">Introduction to Coresets: Accurate Coresets</a></p></li>
<li><p>Efficient Dataset Distillation Using Random Feature Approximation (<a class="reference external" href="https://arxiv.org/abs/2210.12067">loo…rus, 2022</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="recourse">
<h3><span class="section-number">7.9.4.4. </span>recourse<a class="headerlink" href="#recourse" title="Link to this heading">#</a></h3>
<p><strong>recourse</strong> - can a person obtain desired prediction from fixed mode by changing actionable input variables (not just standard explainability)</p>
<ul class="simple">
<li><p>actionable recourse in linear classification (<a class="reference external" href="https://arxiv.org/pdf/1809.06514.pdf">ustun et al. 2019</a>)</p>
<ul>
<li><p>want model to provide actionable inputs (e.g. income) rather than immutable variables (e.g. age, marital status)</p>
<ul>
<li><p>drastic changes in actionable inputs are basically immutable</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="interp-for-rl">
<h3><span class="section-number">7.9.4.5. </span>interp for rl<a class="headerlink" href="#interp-for-rl" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>heatmaps</p></li>
<li><p>visualize most interesting states / rollouts</p></li>
<li><p>language explanations</p></li>
<li><p>interpretable intermediate representations (e.g. bounding boxes for autonomous driving)</p></li>
<li><p>policy extraction - distill a simple model from a bigger model (e.g. neural net -&gt; tree)</p></li>
<li><p><a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-030-10928-8_25">Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees</a></p>
<ul>
<li><p>Linear Model tree allows leaf nodes to contain a linear model, rather than simple constants</p>
<ul>
<li><p>introduce novel on-line learning algo for this which uses SGD to update the linear models</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.3832&amp;amp;rep=rep1&amp;amp;type=pdf">U-tree</a> is a classic online reinforcement learning method which represents a Q function using a tree structure</p></li>
</ul>
</li>
<li><p>Programmatically Interpretable Reinforcement Learning(<a class="reference external" href="http://proceedings.mlr.press/v80/verma18a.html">verma et al. 2018</a>) - represent policies as interpretable programs</p>
<ul>
<li><p>first learn neural policy DRL, then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”</p></li>
</ul>
</li>
<li><p>Neural-to-Tree Policy Distillation with Policy Improvement Criterion (<a class="reference external" href="https://arxiv.org/abs/2108.06898">li et al. 2021</a>)</p>
<ul>
<li><p>typical policy distillation that clones model behaviors with even a small error could bring a data distribution shift</p></li>
<li><p>propose DPIC - changes distillation objective from behavior cloning to maximizing an advantage evaluation (maximizes an approximated cumulative reward)</p>
<ul>
<li><p>hard to apply policy optimization because no gradients</p></li>
</ul>
</li>
<li><p>initialize by modifying criterion to optimize the policy improvement criterion</p></li>
<li><p>then, use viper imitation-learning approach</p></li>
</ul>
</li>
<li><p>Towards Generalization and Simplicity in Continuous Control (<a class="reference external" href="https://arxiv.org/abs/1703.02660">rajeswaran, .., kakade, 2018</a>) - simple policies (e.g. linear or RBF kernel) - work fairly well for many continuous control tasks</p></li>
<li><p>historical</p>
<ul>
<li><p>Tree-Based Batch Mode Reinforcement Learning (<a class="reference external" href="https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf">ernst et al. 2005</a>) - one of the first fitted Q iteration papers actually used trees</p></li>
</ul>
</li>
</ul>
</section>
<section id="interpretation-over-sets-perturbations">
<h3><span class="section-number">7.9.4.6. </span>interpretation over sets / perturbations<a class="headerlink" href="#interpretation-over-sets-perturbations" title="Link to this heading">#</a></h3>
<p>These papers don’t quite connect to prediction, but are generally about finding stable interpretations across a set of models / choices.</p>
<ul class="simple">
<li><p>Exploring the cloud of variable importance for the set of all good models (<a class="reference external" href="https://www.nature.com/articles/s42256-020-00264-0">dong &amp; rudin, 2020</a>)</p></li>
<li><p>All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously (<a class="reference external" href="https://www.jmlr.org/papers/volume20/18-760/18-760.pdf">fisher, rudin, &amp; dominici, 2019</a>) - also had title <em>Model class reliance: Variable importance measures for any machine learning model class, from the “Rashomon” perspective</em></p>
<ul>
<li><p><strong>model reliance</strong> = MR - like permutation importance, measures how much a model relies on covariates of interest for its accuracy</p>
<ul>
<li><p>defined (for a feature) as the ratio of expected loss after permuting (with all possible permutation pairs) to before permuting</p>
<ul>
<li><p>could also be defined as a difference or using predictions rather than loss</p></li>
</ul>
</li>
<li><p>connects to U-statistics - can shows unbiased etc.</p></li>
<li><p>related to <em>Algorithm Reliance (AR)</em> - fitting with/without a feature and measuring the difference in loss (see <a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S0304380002002570">gevrey et al. 03</a>)</p></li>
</ul>
</li>
<li><p><strong>model-class reliance</strong> = MCR = highest/lowest degree of MR within a class of well-performing models</p>
<ul>
<li><p>with some assumptions on model class complexity (in the form of a <em>covering number</em>), can create uniform bounds on estimation error</p></li>
<li><p>MCR can be efficiently computed for (regularized) linear / kernel linear models</p></li>
</ul>
</li>
<li><p><strong>Rashomon set</strong> = class of well-performing models</p>
<ul>
<li><p>“Rashomon” effect of statistics - many prediction models may fit the data almost equally well (breiman 01)</p></li>
<li><p>“This set can be thought of as representing models that might be arrived at due to differences in data measurement, processing, filtering, model parameterization, covariate selection, or other analysis choices”</p></li>
<li><p>can study these tools for describing rank of risk predictions, variance of predictions, e.g. confidence intervals</p></li>
<li><p><img alt="Screen Shot 2020-09-27 at 8.20.35 AM" src="../../_images/mcr_depiction.png" /></p></li>
</ul>
</li>
<li><p><strong>confidence intervals</strong> - can get finite-sample interval for anything, not just loss (e.g. norm of coefficients, prediction for a specific point)</p></li>
<li><p>connections to causality</p>
<ul>
<li><p>when function is conditional expectation, then MR is similar to many things studies in causal literature</p></li>
<li><p>conditional importance measures a different notion (takes away things attributed to spurious variables)</p>
<ul>
<li><p>can be hard to do conditional permutation well when some feature pairs are rare so can use weighting, matching, or imputation</p></li>
</ul>
</li>
</ul>
</li>
<li><p>here, application is to see on COMPAS dataset whether one can build an accurate model which doesn’t rely on race / sex (in order to audit black-box COMPAS models)</p></li>
</ul>
</li>
<li><p>A Theory of Statistical Inference for Ensuring the Robustness of Scientific Results (<a class="reference external" href="https://arxiv.org/abs/1804.08646">coker, rudin, &amp; king, 2018</a>)</p>
<ul>
<li><p>Inference = process of using facts we know to learn about facts we do not know</p></li>
<li><p><strong>hacking intervals</strong> - the range of a summary statistic one may obtain given a class of possible endogenous manipulations of the data</p>
<ul>
<li><p><strong>prescriptively constrained</strong> hacking intervals - explicitly define reasonable analysis perturbations</p>
<ul>
<li><p>e.g. hyperparameters (such as k in kNN), matching algorithm, adding a new feature</p></li>
</ul>
</li>
<li><p><strong>tethered hacking intervals</strong> - take any model with small enough loss on the data</p>
<ul>
<li><p>rather than choosing <span class="math notranslate nohighlight">\(\alpha\)</span>, we choose error tolerance</p></li>
<li><p>for MLE, equivalent to profile likelihood confidence intervals</p></li>
<li><p>e.g. SVM distance from point to boundary, Kernel regression prediction for a specific new point, feature selection</p></li>
<li><p>e.g. linear regression ATE, individual treatment effect</p></li>
</ul>
</li>
<li><p>PCS intervals could be seen as slightly broader, including data cleaning and problem translations</p></li>
</ul>
</li>
<li><p>different theories of inference have different counterfactual worlds</p>
<ul>
<li><p>p-values - data from a superpopulation</p></li>
<li><p>Fisher’s exact p-values - fix the data and randomize counterfactual treatment assignments</p></li>
<li><p>Causal sensitivity analysis - unmeasured confounders from a defined set</p></li>
<li><p>bayesian credible intervals - redrawing the data from the same data generating process, given the observed data and assumed prior and likelihood model</p></li>
<li><p>hacking intervals - counterfactual researchers making counterfactual analysis choices</p></li>
</ul>
</li>
<li><p>2 approaches to replication</p>
<ul>
<li><p>replicating studies - generally replication is very low</p></li>
<li><p><em>p</em>-curve approach: look at distr. of p-values, check if lots of things are near 0.05</p></li>
</ul>
</li>
</ul>
</li>
<li><p>On the Existence of Simpler Machine Learning Models (<a class="reference external" href="https://arxiv.org/pdf/1908.01755.pdf">semenova, rudin, &amp; parr, 2020</a>)</p>
<ul>
<li><p><strong>rashomon ratio</strong> - ratio of the volume of the set of accurate models to the volume of the hypothesis space</p>
<ul>
<li><p>can use this to perform model selection over different hypothesis spaces using empirical risk v. rashomon ratio (<em>rashomon curve</em>)</p></li>
</ul>
</li>
<li><p><strong>pattern Rashomon ratio</strong> - considers unique predictions on the data (called “patterns”) rather than the count of functions themselves.</p></li>
</ul>
</li>
<li><p>Underspecification Presents Challenges for Credibility in Modern Machine Learning (<a class="reference external" href="https://arxiv.org/pdf/2011.03395.pdf">D’Amour et al. 2020</a>)</p>
<ul>
<li><p>different models can achieve the same validation accuracy but perform differently wrt different data perturbations</p></li>
<li><p>shortcuts = spurious correlations cause failure because of ambiguity in the data</p></li>
<li><p><em>stress tests</em> probe a broader set of requirements</p>
<ul>
<li><p>ex. subgroup analyses, domain shift, contrastive evaluations (looking at transformations of an individual example, such as counterfactual notions of fairness)</p></li>
</ul>
</li>
<li><p>suggestions</p>
<ul>
<li><p>need to test models more thoroughly</p></li>
<li><p>need criteria to select among good models (e.g. explanations)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Predictive Multiplicity in Classification (<a class="reference external" href="https://arxiv.org/pdf/1909.06677.pdf">marx et al. 2020</a>)</p>
<ul>
<li><p>predictive multiplicity = ability of a prediction problem to admit competing models with conflicting predictions</p></li>
</ul>
</li>
<li><p>A general framework for inference on algorithm-agnostic variable importance (<a class="reference external" href="https://www-tandfonline-com.libproxy.berkeley.edu/doi/full/10.1080/01621459.2021.2003200?needAccess=true">williamson et al. 2021</a>)</p></li>
<li><p>An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference? (<a class="reference external" href="https://arxiv.org/abs/2011.14999">broderick et al. 2021</a>)</p>
<ul>
<li><p>Approximate Maximum Influence Perturbation -  method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data</p></li>
<li><p>results of several economics papers can be overturned by removing less than 1% of the sample</p></li>
</ul>
</li>
<li><p>Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models (<a class="reference external" href="https://arxiv.org/abs/2303.16047">chen, zhong, seltzer, &amp; rudin, 2023</a>) - algorithm produces many GAMs from which to choose, instead of just one</p></li>
</ul>
</section>
<section id="ai-safety">
<h3><span class="section-number">7.9.4.7. </span>ai safety<a class="headerlink" href="#ai-safety" title="Link to this heading">#</a></h3>
<p><strong>agi definitions</strong></p>
<ul class="simple">
<li><p>AI systems are fully substitutable for human labor, or have a comparably large impact (from Future Fund prize)</p>
<ul>
<li><p>when will [AGI be developed](<a class="reference external" href="https://ftxfuturefund.org/announcing-the-future-funds-ai-worldview-prize/">Announcing the Future Fund’s AI Worldview Prize – Future Fund</a>) (future fund competition)</p></li>
</ul>
</li>
<li><p>APS properties (carlsmith, 2022)</p>
<ul>
<li><p><strong>A</strong>dvanced capability: they outperform the best humans on some set of tasks which, when performed at advanced levels, grant significant power in today’s world (e.g. scientific research, business/military/political strategy, engineering, and persuasion/manipulation)</p>
<ul>
<li><p>Agentic <strong>p</strong>lanning: they make and execute plans, in pursuit of objectives, on the basis of models of the world</p></li>
<li><p><strong>S</strong>trategic awareness: the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining power over humans and the real-world environment.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>papers</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1606.06565">Concrete Problems in AI Safety</a></p></li>
<li><p><strong>Robustness to distributional shift.</strong> <em>Can ML be robust to changes in the data distribution, or at least fail gracefully?</em></p></li>
<li><p><strong>Safe exploration.</strong> <em>Can RL agents learn about their environment without executing catastrophic actions?</em></p></li>
<li><p><strong>Avoiding negative side effects.</strong> <em>Can we transform an RL agent’s <a class="reference external" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node9.html">reward function</a> to avoid undesired effects on the environment?</em></p></li>
<li><p><strong>Avoiding “reward hacking” and “<a class="reference external" href="http://www.agroparistech.fr/mmip/maths/laurent_orseau/papers/ring-orseau-AGI-2011-delusion.pdf">wireheading</a>”.</strong> <em>Can we prevent agents from “gaming” their reward functions, such as by distorting their observations?</em></p></li>
<li><p><strong>Scalable oversight.</strong> <em>Can RL agents efficiently achieve goals for which feedback is very expensive?</em></p></li>
<li><p>Is Power-Seeking AI an Existential Risk? (<a class="reference external" href="https://arxiv.org/abs/2206.13353">carlsmith, 2022</a>)</p></li>
<li><p>6 points</p>
<ol class="arabic simple">
<li><p>it will become possible and financially feasible to build relevantly powerful and agentic AI systems</p></li>
<li><p>there will be strong incentives to do so</p></li>
<li><p>it will be much harder to build aligned (and relevantly powerful/agentic) AI systems than to build misaligned (and relevantly powerful/agentic) AI systems that are still superficially attractive to deploy</p></li>
<li><p>some such misaligned systems will seek power over humans in high-impact ways</p></li>
<li><p>this problem will scale to the full disempowerment of humanity</p></li>
<li><p>such disempowerment will constitute an existential catastrophe</p></li>
</ol>
</li>
<li><p>backdrop</p>
<ul>
<li><p>no reason to believe brain is anywhere near a hard limit (e.g. limitations on speed, memory)</p></li>
<li><p>AI systems that don’t seek to gain or maintain power may cause a lot of harm, but this harm is more easily limited by the power they already have (so wouldn’t count as existential risk)</p></li>
</ul>
</li>
<li><p>Why I Think More NLP Researchers Should Engage with AI Safety Concerns (<a class="reference external" href="https://wp.nyu.edu/arg/why-ai-safety/">sam bowman, 2022</a>)</p></li>
<li><p>Large Language Model Alignment: A Survey (<a class="reference external" href="https://arxiv.org/pdf/2309.15025.pdf">shen…xiong, 2023</a>)</p>
<ul>
<li><p><strong>Outer Alignment</strong>  - choose the right loss functions or reward fuctions and ensure that the training objectives of AI systems match human values.</p>
<ul>
<li><p>Outer alignment attempts to align the specified training objective to the goal of its designer</p></li>
</ul>
</li>
<li><p><strong>Inner Alignment</strong> - This is to ensure that AI systems are actually trained to achieve the goals set by their designers.</p>
<ul>
<li><p>Once we have specified training objectives, we need to ensure that the behaviors of AI systems actually align with those specifications (e.g. they don’t use shortcuts)cc</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/research_ovws"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ovw_transfer_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7.8. </span>transfer learning</p>
      </div>
    </a>
    <a class="right-next"
       href="ovw_complexity.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.10. </span>complexity</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-interpretability">7.9.1. evaluating interpretability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-centric">7.9.1.1. human-centric</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-weaknesses-sanity-checks">7.9.1.2. interpretation weaknesses &amp; sanity checks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intrinsic-interpretability-i-e-how-can-we-fit-a-simpler-model-transparent-models">7.9.2. intrinsic interpretability (i.e. how can we fit a simpler model) = transparent models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rules-overview">7.9.2.1. decision rules overview</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-sets">7.9.2.1.1. rule sets</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-lists">7.9.2.1.2. rule lists</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trees">7.9.2.1.3. trees</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-diagrams">7.9.2.1.4. decision diagrams</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebraic-models">7.9.2.2. linear (+algebraic) models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gams-generalized-additive-models">7.9.2.2.1. gams (generalized additive models)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#supersparse-models">7.9.2.2.2. supersparse models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-regression">7.9.2.2.3. symbolic regression</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-neural-nets">7.9.2.3. interpretable neural nets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#concepts">7.9.2.3.1. concepts</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#localization">7.9.2.3.2. localization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-based-case-based-e-g-prototypes-nearest-neighbor">7.9.2.3.3. example-based = case-based (e.g. prototypes, nearest neighbor)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#connecting-dnns-and-rules">7.9.2.3.4. connecting dnns and rules</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constrained-models-e-g-monotonicity">7.9.2.4. constrained models (e.g. monotonicity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#misc-models">7.9.2.5. misc models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models">7.9.2.5.1. bayesian models</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#programs">7.9.2.5.2. programs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posthoc-interpretability-i-e-how-can-we-interpret-a-fitted-model">7.9.3. posthoc interpretability (i.e. how can we interpret a fitted model)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-agnostic">7.9.3.1. model-agnostic</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-importances-vis">7.9.3.1.1. variable importances (VIs)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-curves">7.9.3.1.2. importance curves</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-ensembles">7.9.3.2. tree ensembles</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-dnns">7.9.3.3. neural networks (dnns)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-visualization">7.9.3.3.1. dnn visualization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-concept-based-explanations">7.9.3.3.2. dnn concept-based explanations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-causal-motivated-attribution">7.9.3.3.3. dnn causal-motivated attribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-feature-importance">7.9.3.3.4. dnn feature importance</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-language-models-transformers">7.9.3.3.5. dnn language models / transformers</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interactions">7.9.3.4. interactions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-agnostic-interactions">7.9.3.4.1. model-agnostic interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-based-interactions">7.9.3.4.2. tree-based interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-interactions">7.9.3.4.3. dnn interactions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-interactions">7.9.3.4.4. linear interactions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-influential-examples">7.9.3.5. finding influential examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-summarization-distillation">7.9.3.6. model summarization / distillation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-problems-perspectives">7.9.4. different problems / perspectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-models">7.9.4.1. improving models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-in-the-loop-hitl">7.9.4.2. human-in-the-loop (HITL)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-automl">7.9.4.3. (interpretable) Automl</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recourse">7.9.4.4. recourse</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interp-for-rl">7.9.4.5. interp for rl</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-over-sets-perturbations">7.9.4.6. interpretation over sets / perturbations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-safety">7.9.4.7. ai safety</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>