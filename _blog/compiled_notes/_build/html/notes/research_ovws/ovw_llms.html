
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.7. llms</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/research_ovws/ovw_llms';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7.8. transfer learning" href="ovw_transfer_learning.html" />
    <link rel="prev" title="7.6. ml in medicine" href="ovw_ml_medicine.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ai/ai.html">1. ai</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ai/knowledge_rep.html">1.1. representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/psychology.html">1.2. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/fairness_sts.html">1.3. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/philosophy.html">1.4. philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/ai_futures.html">1.5. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/cogsci.html">1.6. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/llms.html">1.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/logic.html">1.8. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/search.html">1.9. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/decisions_rl.html">1.10. decisions, rl</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/math.html">2. math</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/linear_algebra.html">2.1. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/optimization.html">2.2. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/differential_equations.html">2.3. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/chaos.html">2.4. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/math_basics.html">2.5. math basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/signals.html">2.6. signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/calculus.html">2.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/proofs.html">2.8. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/analysis.html">2.9. real analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml/ml.html">3. ml</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml/unsupervised.html">3.1. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/structure_ml.html">3.2. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/learning_theory.html">3.3. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/deep_learning.html">3.4. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/comp_vision.html">3.5. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/kernels.html">3.6. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/nlp.html">3.7. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/feature_selection.html">3.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/evaluation.html">3.9. evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/classification.html">3.10. classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stat/stat.html">4. stat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stat/time_series.html">4.1. time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/graphical_models.html">4.2. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/causal_inference.html">4.3. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/game_theory.html">4.4. game theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/data_analysis.html">4.7. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/testing.html">4.8. testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">5. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">5.1. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">5.2. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">5.3. development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">5.4. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">5.5. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">5.6. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">5.7. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">5.8. brain basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">6. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">6.1. cs theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">6.2. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">6.3. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">6.4. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">6.5. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">6.6. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">6.7. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">6.8. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">6.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">6.10. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">6.11. reproducibility</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="research_ovws.html">7. research_ovws</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ovw_disentanglement.html">7.1. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_uncertainty.html">7.2. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_generalization.html">7.3. generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_causal_inference.html">7.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_omics.html">7.5. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_ml_medicine.html">7.6. ml in medicine</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_transfer_learning.html">7.8. transfer learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_interp.html">7.9. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_complexity.html">7.10. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_dl_theory.html">7.11. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_scat.html">7.12. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="ovw_interesting_science.html">7.13. interesting science</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/research_ovws/ovw_llms.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>llms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">7.7.1. prompting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-prompting">7.7.1.1. (auto)prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-chaining-decoding">7.7.1.2. llm chaining / decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-querying-causal-inference">7.7.1.3. llm querying / causal inference</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty">7.7.1.3.1. uncertainty</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-compression-compiling">7.7.1.4. prompt compression / compiling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-guided-generation">7.7.1.5. classifier-guided generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-engineering-vetting">7.7.2. architecture engineering &amp; vetting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-attention-variants">7.7.2.1. architecture/attention variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixture-of-experts-moe-routing">7.7.2.2. mixture of experts (MoE) / routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning">7.7.2.3. pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptation-transfer">7.7.2.4. adaptation / transfer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-time-training">7.7.2.5. test-time training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mech-interp">7.7.3. (mech) interp</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-merging">7.7.3.1. model merging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#editing">7.7.3.2. editing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#direct-weight-inspection">7.7.3.3. direct weight inspection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-interpretation">7.7.3.4. debugging / interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-models">7.7.3.5. interpretable models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings-related">7.7.4. embeddings-related</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-models">7.7.4.1. embedding models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-embeddings">7.7.4.2. explainable embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">7.7.4.3. retrieval-augmented generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#external-memory">7.7.4.4. external memory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-discovery">7.7.5. explanation / discovery</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-module-explanation">7.7.5.1. dataset / module explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#directly-learning-algorithms">7.7.5.2. directly learning algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automated-assistants-teaching-hitl">7.7.5.3. automated assistants, teaching, HITL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clinical-nlp">7.7.5.4. clinical nlp</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clinical-bio-image-segmentation">7.7.5.5. clinical/bio image segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cool-tasks">7.7.5.6. cool tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-explanations-oldschool">7.7.5.7. text explanations (oldschool)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modalities-domains">7.7.6. other modalities / domains</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data">7.7.6.1. tabular data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#audio-time-series">7.7.6.2. audio / time-series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#education">7.7.6.3. education</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">7.7.7. misc</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#security">7.7.7.1. security</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy">7.7.7.2. privacy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-reasoning">7.7.7.3. symbolic reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-use-agents">7.7.7.4. tool use / agents</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multilingual-stuff">7.7.7.5. multilingual stuff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-context-learning">7.7.7.6. in-context learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-limitations-critiques">7.7.7.7. llm limitations / critiques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-llms">7.7.7.8. evaluating with LLMs</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llms">
<h1><span class="section-number">7.7. </span>llms<a class="headerlink" href="#llms" title="Link to this heading">#</a></h1>
<p>See related papers in the <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_interp.html">📌 interpretability</a> page.</p>
<section id="prompting">
<h2><span class="section-number">7.7.1. </span>prompting<a class="headerlink" href="#prompting" title="Link to this heading">#</a></h2>
<ul>
<li><p><a class="github reference external" href="https://github.com/dair-ai/Prompt-Engineering-Guide">dair-ai/Prompt-Engineering-Guide</a></p></li>
<li><p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing (<a class="reference external" href="https://arxiv.org/pdf/2107.13586.pdf">liu…neubig, 2021</a>)</p>
<ul>
<li><p>from <em>feature-engineering</em> -&gt; <em>architecture engineering</em> -&gt; <em>prompt engineering</em> (nowadays, it’s <em>data engineering</em>)</p></li>
<li><details>
<summary>Overview figure</summary>
<p>	<img class="medium_image" src="../assets/prompting_typology.png"/>
</p>
</details>
</li>
</ul>
</li>
<li><p>early prompting papers</p>
<ul class="simple">
<li><p>LAMA: Language Models as Knowledge Bases? (<a class="reference external" href="https://arxiv.org/abs/1909.01066">petroni…riedel, 2019</a>) - use fill-in-the-blank (cloze) prompts for extracting knowledge from LLMs</p>
<ul>
<li><p>create LAMA probe - dataset of (subject, relation, object) triplets with templates – find that BERT can recall these relations</p></li>
<li><p>How to Query Language Models? (<a class="reference external" href="https://arxiv.org/abs/2108.01928">adolphs et al. 2021</a>) - query LLMs by example (e.g. “Ronaldo plays for Portugal. Who does Neuer play for?”)</p></li>
<li><p>How Can We Know What Language Models Know? (<a class="reference external" href="https://arxiv.org/abs/1911.12543">jiang … neubig, 2020</a>)</p>
<ul>
<li><p>mining-based and paraphrasing-based methods to automatically generate high-quality diverse prompts</p></li>
<li><p>ensemble methods to combine answers from different prompts (e.g. avg logits and more)</p></li>
</ul>
</li>
<li><p>Noisy Channel Language Model Prompting for Few-Shot Text Classification (<a class="reference external" href="https://arxiv.org/pdf/2108.04106.pdf">min et al. 2022</a>)</p></li>
<li><p>Querying <span class="math notranslate nohighlight">\(P(question|answer)\)</span> with Bayes rule outperforms standard querying <span class="math notranslate nohighlight">\(P(answer|question)\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="auto-prompting">
<h3><span class="section-number">7.7.1.1. </span>(auto)prompting<a class="headerlink" href="#auto-prompting" title="Link to this heading">#</a></h3>
<p><img alt="prompting_hierarchy" src="../../_images/prompting_hierarchy.png" /></p>
<ul class="simple">
<li><p>natural-language prompting</p>
<ul>
<li><p>iPrompt: Explaining Patterns in Data with Language Models via Interpretable Autoprompting (<a class="reference external" href="https://arxiv.org/abs/2210.01848">singh, morris, …gao, 2022</a>)</p></li>
<li><p>APE: LLMs Are Human-Level Prompt Engineers (<a class="reference external" href="https://arxiv.org/abs/2211.01910">zhou…ba, 2022</a>)</p>
<ul>
<li><p>similar to iPrompt, (1) propose prompt candidates with an LLM, (2) score the prompts by the accuracy they yield when using another LLM and (3) regenerate similar prompt candidates</p></li>
<li><p>experiments on instruction induction datasets + truthful QA</p></li>
</ul>
</li>
<li><p>FluentPrompt: Toward Human Readable Prompt Tuning (<a class="reference external" href="https://arxiv.org/abs/2212.10539">shi, …, zettlemoyer, 2022</a>) - use langevin sampling + fluency constraint to generate prompt</p>
<ul>
<li><p>experiments relatively weak: 3 sentiment datasets + autoprompt is the only baseline</p></li>
</ul>
</li>
<li><p>APO: Automatic Prompt Optimization with “Gradient Descent” and Beam Search (<a class="reference external" href="https://arxiv.org/pdf/2305.03495.pdf">pryzant…zeng, 2023</a>) - update prompts based on errors made by previous prompts</p></li>
<li><p>OPRO: LLMs as Optimizers (<a class="reference external" href="https://arxiv.org/abs/2309.03409">yang…quoc le, zhou, &amp; chen , 2023</a>) - add in past prompts with their scores during optimization</p></li>
<li><p>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution (<a class="reference external" href="https://arxiv.org/abs/2309.16797">fernando…rocktaschel, 2023</a>) - simultaneously improve prompts with LLM + improve the <em>mutation-prompts</em> the LLM uses to mutate the prompts</p></li>
<li><p>Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers (<a class="reference external" href="https://arxiv.org/abs/2309.08532">guo…yang, 2023</a>)</p></li>
<li><p>PromptAgent: Strategic Planning with LMs Enables Expert-level Prompt Optimization (<a class="reference external" href="https://arxiv.org/abs/2310.16427">wang…hu, 2023</a>) - iterate on prompt errors using MC tree search</p></li>
<li><p>Language Models as Black-Box Optimizers for Vision-Language Models (<a class="reference external" href="https://arxiv.org/pdf/2309.05950v1.pdf">yu…pathak, &amp; ramanan, 2023</a>)</p></li>
<li><p>Automatic Prompt Optimization with “Gradient Descent” and Beam Search (<a class="reference external" href="https://arxiv.org/abs/2305.03495">pryzant…zeng, 2023</a>) - LLM computes “gradient” by describing error</p></li>
<li><p>Are Large Language Models Good Prompt Optimizers? (<a class="reference external" href="https://arxiv.org/abs/2402.02101">ma…huang, 2024</a>) - critique that models often struggle</p></li>
</ul>
</li>
<li><p>discrete prompting</p>
<ul>
<li><p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts (<a class="reference external" href="https://aclanthology.org/2020.emnlp-main.346/">shin…sameer singh, 2020</a>)</p>
<ul>
<li><p>select prompts from a fixed set of tokens (resulting prompts are not coherent)</p></li>
<li><p>Universal Adversarial Triggers for Attacking and Analyzing NLP (<a class="reference external" href="https://arxiv.org/abs/1908.07125">wallace…sameer singh, 2019</a> ) - find input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset</p></li>
</ul>
</li>
<li><p>RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning (<a class="reference external" href="https://arxiv.org/abs/2205.12548">deng…hu, 2022</a>)</p></li>
<li><p>LM-BFF: Making Pre-trained Language Models Better Few-shot Learners (<a class="reference external" href="https://arxiv.org/abs/2012.15723">gao et al. 2020</a>) - uses T5 to generate (i) template for the task (which might include a whole example or two) + (i) appropropriate label tokens in the vocabulary for the task (suffers from computationally intensive search + sub-optimal discrete space search)</p></li>
<li><p>PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains (<a class="reference external" href="https://arxiv.org/abs/2102.12206">ben-david, …, reichart, 2022</a>)</p></li>
</ul>
</li>
<li><p>continuous prompt optimization</p>
<ul>
<li><p>Prefix-Tuning: Optimizing Continuous Prompts for Generation (<a class="reference external" href="https://arxiv.org/abs/2101.00190">li &amp; percy liang, 2021</a>) – optimizes in continuous space for language generation tasks</p>
<ul>
<li><p>learn to map some parameters <span class="math notranslate nohighlight">\(\theta\)</span> through and MLP to generate a starting hidden state <span class="math notranslate nohighlight">\(h_i\)</span> – never actually sends the prefix through the network</p></li>
</ul>
</li>
<li><p>P-Tuning: GPT Understands, Too (<a class="reference external" href="https://arxiv.org/abs/2103.10385">liu et al. 2021</a>) – use LSTM to generate prompt embeddings (don’t map to tokens)</p></li>
<li><p>Control Prefixes for Parameter-Efficient Text Generation (<a class="reference external" href="https://arxiv.org/abs/2110.08329">clive, cao, &amp; rei, 2022</a>) - allow for adapting the prefix to each input example</p>
<ul>
<li><p>DART: Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners (<a class="reference external" href="https://arxiv.org/abs/2108.13161">zhang…chen, 2022</a>)</p>
<ul>
<li><p>reformulating NLP task into differentially optimizing the prompt template + target label (given a pre-trained model)</p></li>
<li><p>focus on smaller models (Roberta-large + GPT-2) + few training shots</p></li>
<li><p>fluency constraint to ensure association among prompt embeddings</p></li>
</ul>
</li>
</ul>
</li>
<li><p>WARP: Word-level Adversarial ReProgramming (<a class="reference external" href="https://arxiv.org/abs/2101.00121">Hambardzumyan et al. 2021</a>) - add continous tokens + some task-specific parameters for better generalization</p></li>
<li><p>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction (<a class="reference external" href="https://arxiv.org/abs/2104.07650">Chen et al. 2021</a>) – incorporate relations, visualize learned prompt vectors with t-SNE</p></li>
</ul>
</li>
<li><p>misc</p>
<ul>
<li><p>Context-faithful Prompting for LLMs (<a class="reference external" href="https://arxiv.org/pdf/2303.11315.pdf">zhou, shang, poon &amp; chen, 2023</a>) - ask question in clever way to force LLM to follow it</p></li>
<li><p>SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis (<a class="reference external" href="https://arxiv.org/abs/2109.08306">Zhang et al. 2021</a>) - use sentiment knowledge penalties in the prompt</p></li>
<li><p>Meta-learning via Language Model In-context Tuning (<a class="reference external" href="https://arxiv.org/abs/2110.07814">yanda chen et al. 2022</a>) - given new task with new instruction</p></li>
<li><p>Prompt Programming for LLMs: Beyond the Few-Shot Paradigm (<a class="reference external" href="https://arxiv.org/abs/2102.07350">Reynolds &amp; McDonell, 2021</a>) - define metaprompts as general wrappers around tasks e.g. “This problem asks us to”</p></li>
<li><p>Re3: Generating Longer Stories With Recursive Reprompting and Revision (<a class="reference external" href="https://arxiv.org/abs/2210.06774">yang et al. 2022</a>) - generate summaries, then expand and revise with prompts</p></li>
<li><p>Directional Stimulus Prompting (<a class="reference external" href="https://arxiv.org/abs/2302.11520">li, baoling peng, …jianfeng gao, xifeng yan, 2023</a>) - generate hint keywords using small LLM that are put into the prompt when calling large LLM</p></li>
<li><p>memory-assisted prompt-editing (<a class="reference external" href="https://arxiv.org/abs/2201.06009">madaan…yang, 2022</a>) - allows model to “save things to memory” that get added to prompt when needed</p></li>
<li><p>Prompting Is Programming: A Query Language For LLMs (<a class="reference external" href="https://arxiv.org/abs/2212.06094">Beurer-Kellner, Fischer, &amp; Vechev, 2022</a>)</p></li>
</ul>
</li>
<li><p>can benefit from training for promptability</p>
<ul>
<li><p>Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections (<a class="reference external" href="https://arxiv.org/abs/2104.04670">zhong…klein, 2021</a>)</p></li>
<li><p>Continued Pretraining for Better Zero- and Few-Shot Promptability (<a class="reference external" href="https://arxiv.org/abs/2210.10258">wu…sameer singh, beltagy, 2022</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="llm-chaining-decoding">
<h3><span class="section-number">7.7.1.2. </span>llm chaining / decoding<a class="headerlink" href="#llm-chaining-decoding" title="Link to this heading">#</a></h3>
<p><strong>many notes are from this <a class="reference external" href="https://twitter.com/iraphas13/status/1551959289023016967">thread</a> on chaining models together</strong></p>
<ul class="simple">
<li><p>overviews</p>
<ul>
<li><p>Ai chains: Transparent and controllable human-ai interaction by chaining LLM prompts (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3491102.3517582">wu, terry, &amp; cai, 2022</a>) - chaining LLM steps together: output of one step becomes the input for the next</p>
<ul>
<li><p>interactive system where users can modify chains + their intermediate results – improves performance + human experience</p></li>
</ul>
</li>
<li><p>Language Model Cascades (<a class="reference external" href="https://arxiv.org/abs/2207.10342">dohan…sutton, 2022</a>) - treat chaining models as probabilistic programs</p>
<ul>
<li><p>use a probabilistic-programming language (PPL) to define a joint probability model on string-valued random variables, parameterized using LMs, and then condition this model on string-valued observations in order to compute a posterior over string-valued unknowns</p></li>
<li><p>self-PPLs extend probabilistic graphical models to support more complex joint distributions whose size and “shape” can itself be stochastic</p>
<ul>
<li><p>e.g., a graph unrolled for a random number of iterations, until a data-dependent stopping criterion is met</p></li>
<li><p>variables are all text: questions <span class="math notranslate nohighlight">\(Q\)</span>, answers <span class="math notranslate nohighlight">\(A\)</span>, and intermediate thoughts <span class="math notranslate nohighlight">\(T\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Prover-Verifier Games improve legibility of LLM outputs (<a class="reference external" href="https://arxiv.org/abs/2407.13692">kirchner, chen, … leike, mcaleese, &amp; burda, 2024</a>) - trained strong LMs to produce text that is easy for weak LMs to verify and found that this training also made the text easier for humans to evaluate</p></li>
</ul>
</li>
<li><p>posthoc</p>
<ul>
<li><p>understanding chain-of-thought and its faithfulness</p>
<ul>
<li><p>Faithful Chain-of-Thought Reasoning (<a class="reference external" href="https://arxiv.org/abs/2301.13379">yu et al. 2023</a>)</p></li>
<li><p>Contrastive Chain-of-Thought Prompting (<a class="reference external" href="https://arxiv.org/abs/2311.09277">chia…bing, 2023</a>)</p></li>
<li><p>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks (<a class="reference external" href="https://arxiv.org/abs/2211.12588">chen et al. 2022</a>)</p></li>
<li><p>Critiques</p>
<ul>
<li><p>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations (<a class="reference external" href="https://arxiv.org/abs/2307.08678">yanda chen, zhong, …, steinhardt, yu, mckeown, 2023</a>)</p>
<ul>
<li><p>Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning (<a class="reference external" href="https://arxiv.org/abs/2401.13986">chen…gao, 2024</a>)</p></li>
<li><p>Benchmarking and Improving Generator-Validator Consistency of Language Models (<a class="reference external" href="https://arxiv.org/abs/2310.01846">lisa li…liang, 2023</a>)</p></li>
</ul>
</li>
<li><p>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c402501846f9fe03e2cac015b3f0e6b1-Paper-Conference.pdf">ye &amp; durrett, 2022</a>)</p></li>
<li><p>Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting (<a class="reference external" href="https://arxiv.org/abs/2305.04388">turpin, …, bowman, 2023</a>)</p>
<ul>
<li><p>CoT explanations can be heavily influenced by biasing the model towards certain answers, thereby yielding invalid explanations</p></li>
<li><p>try biasing in 2 ways: answer is always (A), or setting where prompt suggests a certain answer</p></li>
</ul>
</li>
<li><p>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs (<a class="reference external" href="https://arxiv.org/abs/2305.14279">chen, …, bowman, cho, 2023</a>) - models fail at these 2 tasks:</p>
<ul>
<li><p>hypothetical consistency (the ability for a model to predict what its output would be in a hypothetical other context)</p></li>
<li><p>compositional consistency (consistency of a model’s outputs for a compositional task even when an intermediate step is replaced with the model’s output for that step)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>faithfulness metric = model sensitivity to removing some of the explanation</p>
<ul>
<li><p>Question Decomposition Improves the Faithfulness of Model-Generated Reasoning (<a class="reference external" href="https://www-files.anthropic.com/production/files/question-decomposition-improves-the-faithfulness-of-model-generated-reasoning.pdf">anthropic, 2023</a>) - introduce factored decomposition to improve faithfulness metric</p></li>
<li><p>Measuring Faithfulness in Chain-of-Thought Reasoning (<a class="reference external" href="https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf">anthropic, 2023</a>) - in addition to just removing some of the explanation, also add mistakes to it / paraphrase it</p>
<ul>
<li><p>larger models become less faithful by this metric</p></li>
</ul>
</li>
<li><p>Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI (<a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/26174">sia…zettlemoyer, mathias, 2023</a>)</p></li>
</ul>
</li>
<li><p>Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals (<a class="reference external" href="https://arxiv.org/pdf/2311.09605.pdf">elazar…sameer singh, noah smith, 2023</a>)</p></li>
<li><p>Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals (<a class="reference external" href="https://arxiv.org/abs/2310.00603">gat…reichart, 2023</a>)</p></li>
<li><p>Counterfactually Aware Fair Text Generation (<a class="reference external" href="https://arxiv.org/abs/2311.05451">banerjee…bhatia, 2023</a>)</p></li>
<li><p>Causal Proxy Models for Concept-based Model Explanations (<a class="reference external" href="https://proceedings.mlr.press/v202/wu23b.html">wu…potts, 2023</a>)</p></li>
<li><p>Evaluating Models’ Local Decision Boundaries via Contrast Sets (<a class="reference external" href="https://arxiv.org/abs/2004.02709">gardner…zhou, 2020</a>)</p></li>
<li><p>Are LLMs Post Hoc Explainers? (<a class="reference external" href="https://arxiv.org/abs/2310.05797">kroeger…lakkaraju, 2023</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Chain-of-Thought Prompting (<a class="reference external" href="https://arxiv.org/abs/2201.11903">wei et al. 2022</a>): in few-shot prompts, don’t just provide answer but also reasoning</p>
<ul>
<li><p>model outputs reasoning + answer, leading to improved performance</p></li>
<li><p>Self-Discover: LLMs Self-Compose Reasoning Structures (<a class="reference external" href="https://arxiv.org/abs/2402.03620">zhou…le…zheng, 2024</a>) - LLMs come up with their own step-by-step structure for a task</p></li>
<li><p>Self-Consistency Improves Chain of Thought Reasoning in Language Models (<a class="reference external" href="https://arxiv.org/abs/2203.11171">wang, wei, schuurmans, quoc le, … zhou, 2022</a>) - use output samples rather than greedy and return the most consistent final answer in the set</p></li>
<li><p>Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them (<a class="reference external" href="https://arxiv.org/abs/2210.09261">suzgun, …, quoc le, …, jason wei, 2022</a>)</p></li>
<li><p>self-ask (<a class="reference external" href="https://arxiv.org/pdf/2210.03350.pdf">Press et al., 2022</a>) - LLM asks itself (and then answers) follow-up questions before answering the initial question</p></li>
<li><p>Text Classification via LLMs (<a class="reference external" href="https://arxiv.org/pdf/2305.08377.pdf">sun…wang, 2023</a>) - add clues to the prompt</p></li>
<li><p>Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning (<a class="reference external" href="https://arxiv.org/abs/2306.14308">ma, …, chen, 2023</a>) - counterfactuals help improve CoT</p></li>
<li><p>RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought (<a class="reference internal" href="#"><span class="xref myst">xue et al. 2023</span></a>)</p></li>
<li><p>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning (<a class="reference external" href="https://arxiv.org/abs/2308.00436">miao, teh, &amp; rainforth, 2023</a>)</p></li>
<li><p>EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning (<a class="reference external" href="https://arxiv.org/pdf/2309.10687.pdf">mekala…sameer singh, 2023</a>) - replace <em>let’s think step by step</em> with <em>Let’s repeat the question and also think step by step</em></p></li>
<li><p>Let’s Think Dot by Dot: Hidden Computation in Transformer Language Models (<a class="reference external" href="https://arxiv.org/abs/2404.15758">pfau, merrill, &amp; bowman, 2024</a>)</p></li>
<li><p>Coconut: Training Large Language Models to Reason in a Continuous Latent Space (<a class="reference external" href="https://arxiv.org/abs/2412.06769">hao…weston, tian, 2024</a>) - requires some extra finetuning</p></li>
<li><p>Show Your Work: Scratchpads for Intermediate Computation with Language Models (<a class="reference external" href="https://arxiv.org/abs/2112.00114">nye et al. 2021</a>)</p></li>
<li><p>selection inference (<a class="reference external" href="https://arxiv.org/abs/2205.09712">creswell et al. 2022</a>) - generate set of facts, then iteratively generate inferences from the facts to yield the final answer</p></li>
<li><p>least-to-most prompting (<a class="reference external" href="https://arxiv.org/abs/2205.10625">zhou…quoc le et al. 2022</a>) - prompt LLM with context showing how to reduce into subproblems; then LLM sequentially solves the subproblems, using the previous answers</p></li>
<li><p>Generated Knowledge Prompting for Commonsense Reasoning (<a class="reference external" href="https://arxiv.org/abs/2110.08387">liu…hasjishirzi, 2021</a>) - generate knowledge from an LLM then provide it as additional input when answering a question</p></li>
<li><p>maieutic prompting (<a class="reference external" href="https://arxiv.org/abs/2205.11822">jung et al. 2022</a>) - generate a tree of all explanation of the form “True, because…”, “False, because…” then query LLM with these as prompts</p>
<ul>
<li><p>then use Max-SAT to try to satisfy as many relations between the model explanations as possible to come up with the true answer</p></li>
</ul>
</li>
</ul>
</li>
<li><p>using verifiers</p></li>
<li><p>LM vs LM: Detecting Factual Errors via Cross Examination (<a class="reference external" href="https://arxiv.org/abs/2305.13281">cohen et al. 2023</a>)</p>
<ul>
<li><p><a class="reference external" href="https://twitter.com/ChengleiSi/status/1664023767373299715">Thread</a> of papers combating hallucination</p></li>
<li><p>verifiers (<a class="reference external" href="https://arxiv.org/abs/2110.14168">cobbe et al. 2021</a>) - train model to judge whether an answer and thought are likely to be “valid”</p></li>
<li><p>subgoal search (<a class="reference external" href="https://t.co/PCR4yexHti">czechowski et al. 2021</a>) - train model to generate subgoals then solve them in a graph</p></li>
<li><p>STaR “Self-taught reasoner” (<a class="reference external" href="https://arxiv.org/abs/2203.14465">zelikman…goodman, 2022</a>)</p>
<ul>
<li><p>first, finetune on observed <span class="math notranslate nohighlight">\((Q, T, A)\)</span> triplets, where <span class="math notranslate nohighlight">\(T\)</span> is a rationale</p></li>
<li><p>then, impute unknown <span class="math notranslate nohighlight">\(T_i\)</span> given dataset of pairs <span class="math notranslate nohighlight">\((Q_i, A_i)\)</span> by sampling until finding a <span class="math notranslate nohighlight">\(T_i\)</span> which leads to the correct answer</p></li>
<li><p>robotics-specific</p>
<ul>
<li><p>zero-shot planning (<a class="reference external" href="https://arxiv.org/abs/2201.07207">huang, abbeel, pathak, &amp; mordatch, 2022</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.00598">socratic models</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.05608">Inner Monologue</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2103.01197">global workspace</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>tree-related</p>
<ul>
<li><p>Aug-tree (<a class="reference external" href="https://arxiv.org/abs/2209.11799">singh, askari, caruana, &amp; gao, 2023</a>)</p></li>
<li><p>Tree-prompting (<a class="reference external" href="https://arxiv.org/abs/2310.14034">morris, singh, rush, gao, &amp; deng, 2023</a>)</p>
<ul>
<li><p>Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck (<a class="reference external" href="https://arxiv.org/abs/2310.19660">ludan…callison-burch, 2023</a>)</p></li>
</ul>
</li>
<li><p>tree of thoughts (<a class="reference external" href="https://arxiv.org/abs/2305.10601">yao et al. 2023</a>) - LLM generates a tree of intermediate answers and perform steps such as backtracking</p>
<ul>
<li><p>Graph of Thoughts: Solving Elaborate Problems with LLMs (<a class="reference external" href="https://arxiv.org/pdf/2308.09687.pdf">besta, .., hoefler, 2023</a>) - allows merging/looping in the tree, e.g. for sorting</p></li>
</ul>
</li>
</ul>
</li>
<li><p>optimizing cost efficiency</p>
<ul>
<li><p>frugalGPT (<a class="reference external" href="https://arxiv.org/pdf/2305.05176.pdf">chen, zaharia, &amp; zou, 2023</a>)</p>
<ul>
<li><p>3 components</p>
<ol class="arabic simple">
<li><p>prompt adaptation - identify effective / shorter prompts (e.g. less demonstrations)</p></li>
<li><p>LLM approximation - create simpler/cheaper LLMs</p></li>
<li><p>LLM cascade - adaptively choose LLM based on query</p>
<ol class="arabic simple">
<li><p>train “generation scoring function” - returns reliability score from 0 to 1 for each (question, answer)</p></li>
<li><p>router sequentially proceeds through LLM APIs, returning the answer if the reliability score is high enough</p></li>
</ol>
</li>
</ol>
</li>
<li><p>frugalML (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/789ba2ae4d335e8a2ad283a3f7effced-Abstract.html">chen, zaharia, zou, 2020</a>) - tradeoff performance with budget for sequential cascade of API calls for single label</p>
<ul>
<li><p>FrugalMCT (<a class="reference external" href="https://proceedings.mlr.press/v162/chen22ad.html">chen, zaharia, zou, 2022</a>) - extends to multilabel</p></li>
</ul>
</li>
</ul>
</li>
<li><p>EcoAssistant: Using LLM Assistant More Affordably and Accurately (<a class="reference external" href="https://arxiv.org/abs/2310.03046">zhang…awadallah, wang, 2023</a>) - answer code-driven queries efficiently using code executor + cascade of increasingly complex LLMs</p></li>
</ul>
</li>
<li><p>decoding</p>
<ul>
<li><p>Greedy - iteratively pick highest-probability token</p></li>
<li><p>Nucleus sampling: <a class="reference external" href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration</a> (holtzman…choi, 2019)</p>
<ul>
<li><p>Contrastive decoding (<a class="reference external" href="https://arxiv.org/abs/2210.15097">li et al. 2022</a>) - decode based on the difference between a large and small LLM</p>
<ul>
<li><p>Context-aware decoding (<a class="reference external" href="https://arxiv.org/pdf/2305.14739.pdf">shi, …zettlemoyer, yih, 2023</a>) - the difference between the output probabilities when a model is used with and without context</p></li>
<li><p>DoLa: Decoding by Contrasting Layers Improves Factuality in LLMs (<a class="reference external" href="https://arxiv.org/abs/2309.03883">chuang…he, 2023</a>) - contasting later layers with early layers can improve truthfulness</p></li>
<li><p>Calibrate Before Use: Improving Few-Shot Performance of Language Models (<a class="reference external" href="https://arxiv.org/abs/2102.09690">zhao, …, dan klein, sameer singh, 2021</a>) - to make prompting easier, first calibrate output distr by making it uniform when given null inputs, e.g. “N/A”</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Minimum Bayes Risk Decoding (<a class="reference external" href="https://arxiv.org/abs/2211.07634">suzgun, …, jurafsky, 2022</a>) or (<a class="reference external" href="https://arxiv.org/pdf/2111.09388.pdf">freitag et al. 2022</a>)</p></li>
<li><p>A Frustratingly Simple Decoding Method for Neural Text Generation (<a class="reference external" href="https://arxiv.org/abs/2305.12675">yang, …, shi, 2023</a>) - build an anti-LM based on previously generated text and use this anti-LM to penalize future generation of what has been generated</p></li>
<li><p>fast decoding</p>
<ul>
<li><p><a class="reference external" href="https://kipp.ly/transformer-inference-arithmetic/">KV caching</a> + some other tricks - if repeatedly using the same tokens at the beginning of the context, can cache the KV vectors for those tokens</p>
<ul>
<li><p>KV caching trades off speed with memory</p></li>
</ul>
</li>
<li><p>speculative decoding (<a class="reference external" href="https://arxiv.org/abs/2211.17192">leviathan, kalma, &amp; matias, 2022</a>)  - decode multiple tokens in parallel with small model, potentially skipping steps for the large model</p></li>
</ul>
</li>
<li><p>early exit - popular way to speed up inference</p>
<ul>
<li><p>Multi-exit vision transformer for dynamic inference (<a class="reference external" href="https://arxiv.org/abs/2106.15183">Bakhtiarnia, A., Zhang, Q. and Iosifidis, A., 2021</a>)</p>
<ul>
<li><p>early layers have large activation map so early exist classifier must be complex</p></li>
<li><p>solution: ViT class token allows early-exit classifier to have constant complexity</p></li>
</ul>
</li>
<li><p>DeeBERT: Dynamic early exiting for accelerating BERT inference (<a class="reference external" href="https://arxiv.org/abs/2004.12993">xin…lin, 2020</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>prompt ensembles</p>
<ul>
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3560815">liu…neubig, 2023</a> review discusses different strategies for ensembling prompts, e.g. averaging, weighted averaging</p></li>
<li><p>black-box querying</p>
<ul>
<li><p>Tree-Prompting (<a class="reference external" href="https://arxiv.org/abs/2310.14034">morris…deng, 2023</a>)</p></li>
<li><p>PromptBoosting: Black-Box Text Classification with Ten Forward Passes (<a class="reference external" href="https://arxiv.org/abs/2212.09257">hou, …, jacob andreas, …, zhang, 2022</a>) - get a small pool of prompts, learn a verbalizer (final classification layer) for each, then ensemble them with AdaBoost on LLM output</p></li>
<li><p>people have studied many works on prompt ensembling (e.g. <a class="reference external" href="https://arxiv.org/abs/2104.08691">lester et al. 2021</a>)</p></li>
<li><p>Boosted Prompt Ensembles for LLMs (<a class="reference external" href="https://arxiv.org/abs/2304.05970">pitis…ba, 2023</a>) - similar but use CoT-style prompts and tasks, e.g. GSM8k</p></li>
<li><p>PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine (<a class="reference external" href="https://arxiv.org/abs/2308.12033">zhang…cai, 2023</a>) - builds set of prompts dynamically rather than assuming they’re fixed</p></li>
<li><p>PTR: Prompt Tuning with Rules for Text Classification (<a class="reference external" href="https://arxiv.org/abs/2105.11259">han et al. 2021</a>) – use logic rules to construct prompts with sub-prompts for many-class text classification (prompt is constructed hierarchically, but only one call is made to the LLM for inference)</p></li>
</ul>
</li>
<li><p>soft prompts</p>
<ul>
<li><p>Learning How to Ask: Querying LMs with Mixtures of Soft Prompts (<a class="reference external" href="https://arxiv.org/abs/2104.06599">Qin &amp; Eisner, 2021</a>) - learn a mixture of soft prompts using gradient descent</p></li>
</ul>
</li>
<li><p>require model retraining</p>
<ul>
<li><p>PRBOOST: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning (<a class="reference external" href="https://arxiv.org/abs/2203.09735">zhang…zhang, 2022</a>) - iteratively (1) select high-error examples, (2) have human label them as rules, and (3) use boosting to train model on the new rules + ensemble</p></li>
<li><p>typical rule generation</p>
<ul>
<li><p>Snuba (<a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/31777681/">Varma and Ré, 2018</a>) generates heuristics based on a small labeled dataset with pre-defined rule types</p></li>
<li><p>TALLOR (<a class="reference external" href="https://arxiv.org/pdf/2107.02282.pdf">Li et al. 2021a</a>) &amp; GLaRA (<a class="reference external" href="https://arxiv.org/pdf/2104.06230.pdf">Zhao et al. 2021</a>) study rule expansion for NER problem based on lexical information and then select rules based on a hand-tuned threshold</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Prompt ensembling / selection without labels</p>
<ul>
<li><p>Zero-Label Prompt Selection (<a class="reference external" href="https://arxiv.org/abs/2211.04668">liao, zheng, &amp; yang, 2022</a>) - use prompts to label unlabeled data and then select prompts using these labels</p></li>
<li><p>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models (<a class="reference external" href="https://proceedings.mlr.press/v202/allingham23a.html">alingham…lakshminarayanan, 2023</a>) - use confidence (max output logit) after appropriate normalization as weight</p></li>
</ul>
</li>
<li><p>few-shot text classification</p>
<ul>
<li><p>FastFit (<a class="reference external" href="https://arxiv.org/pdf/2404.12365.pdf">yehudai &amp; bandel, 2024</a>) - fit few-shot batch with contrastive examples then predict using similarities to shots rather than a classification head (base model is roberta)</p>
<ul>
<li><p>SetFit (<a class="reference external" href="https://arxiv.org/abs/2209.11055">tunstal…pereg, 2022</a>) - finetune stentence transformer with contrastive loss, then train classification head</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>self-verification</p>
<ul>
<li><p>review on self-verification (<a class="reference external" href="https://arxiv.org/pdf/2308.03188.pdf">pan…wang, 2023</a>)</p></li>
<li><p>Self-Refine: Iterative Refinement with Self-Feedback (<a class="reference external" href="https://arxiv.org/abs/2303.17651">madaan, …, clark, 2023</a>)</p></li>
<li><p>Self-Verification Improves Few-Shot Clinical Information Extraction (<a class="reference external" href="https://arxiv.org/abs/2306.00024">gero et al. 2023</a>)</p></li>
<li><p>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative LLMs (<a class="reference external" href="https://arxiv.org/abs/2303.08896">manakul…gales, 2023</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="llm-querying-causal-inference">
<h3><span class="section-number">7.7.1.3. </span>llm querying / causal inference<a class="headerlink" href="#llm-querying-causal-inference" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Can LLMs Infer Causation from Correlation? (<a class="reference external" href="https://arxiv.org/abs/2306.05836">jin…scholkopf, 2023</a>) - introduce Corr2Cause dataset (must infer causal graph from correlational statements), doesn’t test pre-existing knowledge</p></li>
<li><p>Causal Reasoning and LLMs: Opening a New Frontier for Causality (<a class="reference external" href="https://arxiv.org/abs/2305.00050">kiciman…tan, 2023</a>)</p>
<ul>
<li><p>LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis</p>
<ul>
<li><p>cause-effect pairs, LLM has to discover from graph (tubingen benchmark, neuropathic pain, etc.)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond (<a class="reference external" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00511/113490/Causal-Inference-in-Natural-Language-Processing">feder…vetich, diyi yang, 2022</a>)</p></li>
<li><p>Zero-shot causal learning (<a class="reference external" href="https://arxiv.org/abs/2301.12292">nilforoshan…leskovec, 2023</a>)</p></li>
<li><p>Discovering Latent Knowledge in Language Models Without Supervision (<a class="reference external" href="https://arxiv.org/abs/2212.03827">burns, ye, klein, &amp; steinhardt, 2022</a>) - identify whether text is true or false directly from a model’s <em>unlabeled activations</em></p>
<ul>
<li><p>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model (<a class="reference external" href="https://arxiv.org/abs/2306.03341">li…pfister, wattenberg, 2023</a>)</p></li>
</ul>
</li>
<li><p>InferBERT: A Transformer-Based Causal Inference Framework for Enhancing Pharmacovigilance (<a class="reference external" href="https://www.frontiersin.org/articles/10.3389/frai.2021.659622/full">wang…liu, 2021</a>) - learn + test feature relationships from attention weights</p></li>
<li><p>CausaLM: Causal Model Explanation Through Counterfactual Language Models (<a class="reference external" href="https://direct.mit.edu/coli/article/47/2/333/98518/CausaLM-Causal-Model-Explanation-Through">2021</a>) - produce example-level causal model explanations using models finetuned on auxiliary adversarial tasks derived from the causal graph of the problem</p></li>
<li><p>Investigating Gender Bias in Language Models Using Causal Mediation Analysis (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf">vig, …, shieber, 2020</a>)</p>
<ul>
<li><p>Applies causal mediation analysis to identify decisive neurons and attention heads responsible for gender bias in LLMs</p></li>
<li><p>Identifies a small handful of decisive attention heads in this case</p></li>
</ul>
</li>
<li><p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals (<a class="reference external" href="https://arxiv.org/pdf/2006.00995.pdf">elazar, …, goldberg, 2021</a>) - measure the importance of specific info within a model by introducing a causal intervention to erase that information, then observing the causal effects</p></li>
<li><p>TrustLLM (<a class="reference external" href="https://arxiv.org/abs/2401.05561">sun…zhao, 2024</a>) - evaluation and benchmark of many aspects of trustworthiness (<a class="reference external" href="https://github.com/HowieHwong/TrustLLM">github</a>)</p></li>
<li><p>What Evidence Do Language Models Find Convincing? (<a class="reference external" href="https://arxiv.org/abs/2402.11782">wan, wallace, &amp; klein, 2024</a>) - rather than relying on facts, LLMs largely rely on textual similarities in evidence to decide whether it’s important</p></li>
<li><p>Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability (<a class="reference external" href="https://arxiv.org/abs/2401.08574">aykurek…andreas, 2024</a>) - LMs generate additional text implied by documents, reason about the generated text, and finetune on the correct text</p>
<ul>
<li><p>LMs’ reasoning capabilities during inference can be leveraged during training to improve their reliability</p></li>
</ul>
</li>
</ul>
<section id="uncertainty">
<h4><span class="section-number">7.7.1.3.1. </span>uncertainty<a class="headerlink" href="#uncertainty" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Semantic Uncertainty (<a class="reference external" href="https://arxiv.org/abs/2302.09664">kuhn, gal, &amp; farquhar, 2023</a>) - instead of calculating entropy over tokens, first generate set of answers, then cluster them base on semantic equivalence, before computing entropy</p>
<ul>
<li><p>clustering is done via an LM that tests entailment e.g. E.g., “The capital of France is Paris.” entails “Paris is the capital of France.” because they mean the same thing</p></li>
</ul>
</li>
<li><p>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs (<a class="reference external" href="https://arxiv.org/abs/2306.13063">xiong…hooi, 2023</a>)</p>
<ul>
<li><p>verbalized uncertainty - model outputs its own uncertainty</p></li>
<li><p>consistency-based uncertainty - consistency between output generations</p></li>
</ul>
</li>
<li><p>Quantifying Uncertainty in Natural Language Explanations of LLMs (<a class="reference external" href="https://arxiv.org/abs/2311.03533">tanneru…lakkaraju, 2023</a>)</p>
<ul>
<li><p>probing uncertainty (like consistency-based uncertainty above) - applies input perturbations (e.g., paraphrasing) and measure the consistency of the resulting explanations</p></li>
<li><p>verbalized uncertainty of explanations often performs poorly</p></li>
</ul>
</li>
<li><p>Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty (<a class="reference external" href="https://arxiv.org/abs/2401.06730">zhou…sap, 2024</a>)</p>
<ul>
<li><p>LMs are often unable to express uncertainties</p></li>
<li><p>LM confidences tend to be overconfident</p></li>
<li><p>users rely heavily on LM generations, whether or not they are marked by certainty</p></li>
</ul>
</li>
<li><p>Teaching Models to Express Their Uncertainty in Words (<a class="reference external" href="https://arxiv.org/abs/2205.14334">Lin et al., 2022</a>) - GPT3 can  generate both an answer and a level of confidence (e.g. “90% confidence”)</p></li>
<li><p>Decomposing Uncertainty for LLMs through Input Clarification Ensembling (<a class="reference external" href="https://arxiv.org/pdf/2311.08718.pdf">hou…zhang, 2023</a>)</p></li>
</ul>
</section>
</section>
<section id="prompt-compression-compiling">
<h3><span class="section-number">7.7.1.4. </span>prompt compression / compiling<a class="headerlink" href="#prompt-compression-compiling" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Learning How to Ask: Querying LMs with Mixtures of Soft Prompts (<a class="reference external" href="https://arxiv.org/abs/2104.06599">Qin &amp; Eisner, 2021</a>) - learn a mixture of soft prompts using gradient descent</p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3560815">liu…neubig, 2023</a> review discusses different strategies for ensembling prompts, e.g. averaging, weighted averaging</p></li>
<li><p>Prompt ensembling / selection without labels</p>
<ul>
<li><p>Zero-Label Prompt Selection (<a class="reference external" href="https://arxiv.org/abs/2211.04668">liao, zheng, &amp; yang, 2022</a>) - use prompts to label unlabeled data and then select prompts using these labels</p></li>
<li><p>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models (<a class="reference external" href="https://proceedings.mlr.press/v202/allingham23a.html">alingham…lakshminarayanan, 2023</a>) - use confidence (max output logit) after appropriate normalization as weight</p></li>
</ul>
</li>
<li><p>LLMLingua (<a class="reference external" href="https://arxiv.org/abs/2310.05736">jiang, wu…qiu, 2023</a>) - learn BERT-size model to compress prompt (iterative token classification approach from distilled GPT-4 compressed prompts)</p>
<ul>
<li><p>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression (<a class="reference external" href="https://arxiv.org/abs/2310.06839">jiang, wu…qiu, 2023</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="classifier-guided-generation">
<h3><span class="section-number">7.7.1.5. </span>classifier-guided generation<a class="headerlink" href="#classifier-guided-generation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Plug and Play Language Models: A Simple Approach to Controlled Text Generation (<a class="reference external" href="https://arxiv.org/abs/1912.02164">dathathri, …, yosinski, &amp; liu, 2020</a>)</p>
<ul>
<li><p>gradients from the classifier push the LM’s hidden activations, then recompute logits to guide generation (and maybe avg with original logits to maintain fluency)</p></li>
</ul>
</li>
<li><p>FUDGE: Controlled Text Generation With Future Discriminators (<a class="reference external" href="https://arxiv.org/abs/2104.05218">yang &amp; klein, 2021</a>)</p>
<ul>
<li><p>classifier predicts probability of attribute for running sequence with each next-token appended</p></li>
<li><p>these attribute probs. are multiplied with next-token probs for each token and then we sample from that distr (after normalization)</p></li>
</ul>
</li>
<li><p>Diffusion-LM Improves Controllable Text Generation (<a class="reference external" href="https://arxiv.org/abs/2205.14217">lisa li, thickstun, gulrajani, liang, &amp; hashimoto, 2022</a>)</p></li>
<li><p>Mixture of Soft Prompts for Controllable Data Generation (<a class="reference external" href="https://arxiv.org/pdf/2303.01580.pdf">chen, lee, …, yu, 2023</a>) - trains a small model on data from a big frozen LLM that is then more controllable</p></li>
</ul>
</section>
</section>
<section id="architecture-engineering-vetting">
<h2><span class="section-number">7.7.2. </span>architecture engineering &amp; vetting<a class="headerlink" href="#architecture-engineering-vetting" title="Link to this heading">#</a></h2>
<section id="architecture-attention-variants">
<h3><span class="section-number">7.7.2.1. </span>architecture/attention variants<a class="headerlink" href="#architecture-attention-variants" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>state space models (good overview in <a class="reference external" href="https://searchworks.stanford.edu/view/14784021">albert gu thesis</a>, 2023)</p>
<ul>
<li><p>S4: <em>structured</em> state space models (<a class="reference external" href="https://arxiv.org/abs/2111.00396">gu…re, 2022</a>) - similar to RNNs but can predict all outputs at once via convolution</p>
<ul>
<li><p>the core of the state space model is basically a linear RNN</p>
<ul>
<li><p>inputs x, hidden states h, outputs y</p></li>
<li><p>3 matrices: <span class="math notranslate nohighlight">\(A, B, C\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i = C h_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(h_i = A h_{i-1} + B x_i\)</span></p>
<ul>
<li><p>note: there is no nonlinearity between hidden states</p></li>
<li><p>note: the transition from one hidden state to the next is the same for all positions (except for the input)</p></li>
</ul>
</li>
<li><p>can compute hidden states simultaneously by just pre-multiplying these A and B matrices with x the right number of times ( a convolution operation)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>mamba: <em>selective</em> state space models (<a class="reference external" href="https://arxiv.org/abs/2312.00752">gu &amp; dao, 2023</a>)</p>
<ul>
<li><p>changes (2) above – the transition from one hidden state to the next now depends on the input (making it closer to LSTMs)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(B = B(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(C = C(x)\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval (<a class="reference external" href="https://arxiv.org/abs/2402.18510">wen, dang, &amp; lyu, 2024</a>) - RNNs fail to retrieve info from long contexts, RAG helps</p></li>
</ul>
</li>
<li><p>MAD synthetic tasks: Mechanistic Design and Scaling of Hybrid Architectures (<a class="reference external" href="https://arxiv.org/abs/2403.17844">poli…ermon, re, zhang, &amp; massaroli, 2024</a>) - introduces 6 synthetic tasks on which performance correlates very well when scaling to real tasks: in-context recall, fuzzy in-context recall, noisy in-context recall, selective copying, compression, memorization</p></li>
<li><p>Scalable MatMul-free Language Modeling (<a class="reference external" href="https://arxiv.org/abs/2406.02528">zhu…eshraghian, 2024</a>) - LM architecture that doesn’t use matmuls, builds on GRU, and shows improved efficiency on FPGAs</p></li>
<li><p>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (<a class="reference external" href="https://arxiv.org/abs/2402.17764">ma…wei, 2024</a>)</p>
<ul>
<li><p>BitNet: Scaling 1-bit Transformers for Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2310.11453">wang…wei, 2023</a>)</p></li>
</ul>
</li>
<li><p>Misc</p>
<ul>
<li><p>Tree Transformer: Integrating Tree Structures into Self-Attention (<a class="reference external" href="https://arxiv.org/pdf/1909.06639.pdf">wang, .., chen, 2019</a>)</p></li>
<li><p>Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform (<a class="reference external" href="https://arxiv.org/abs/2210.01989">zhuang…shang, 2022</a>)</p></li>
<li><p>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is? (<a class="reference external" href="https://arxiv.org/abs/2311.13110">yaodong yu…yi ma, 2023</a>)</p></li>
</ul>
</li>
<li><p>Diffusion models</p>
<ul>
<li><p>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution (<a class="reference external" href="https://arxiv.org/abs/2310.16834">lou, meng, &amp; ermon, 2024</a>) - model <span class="math notranslate nohighlight">\(p(\text{altered text}) / p(\text{orig text})\)</span>, and make alterations using word swaps at individual locations</p>
<ul>
<li><p>From Denoising Diffusions to Denoising Markov Models (<a class="reference external" href="https://arxiv.org/abs/2211.03595">benton…doucet, 2024</a>)</p></li>
<li><p>Not clear that these are better than just iteratively masking/replacing a word with BERT</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="mixture-of-experts-moe-routing">
<h3><span class="section-number">7.7.2.2. </span>mixture of experts (MoE) / routing<a class="headerlink" href="#mixture-of-experts-moe-routing" title="Link to this heading">#</a></h3>
<p>mixture of experts models have become popular because of the need for (1) fast speed / low memory at test time while still (2) having a large model during training</p>
<ul class="simple">
<li><p>note: nowadays often the “experts” are different MLPs following the self-attention layers (since their computations can be computed independently)</p></li>
<li><p>A Review of Sparse Expert Models in Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2209.01667">fedus, jeff dean, zoph, 2022</a>)</p>
<ul>
<li><p>sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models</p></li>
<li><p>routing algorithm - determines where to send examples</p>
<ul>
<li><p>discreteness makes it difficult</p>
<ul>
<li><p>some works use RL to learn routing</p></li>
<li><p>standard approach uses gumbel-softmax</p></li>
<li><p>usually get matrix of similarities between input tokens and experts and route based on these</p>
<ul>
<li><p>sometimes route to topk experts rather than top1</p></li>
</ul>
</li>
</ul>
</li>
<li><p>load balancing - usually add an auxiliary loss to encourage equal tokens being sent to different experts</p></li>
</ul>
</li>
</ul>
</li>
<li><p>non-specialized experts</p>
<ul>
<li><p>Early versions (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6797059">Jacobs, michael jordan, nowlan, &amp; hinton, 1991</a>) had independent feed-forward networks serving as experts</p></li>
<li><p>Sparsely-gated MOE layer (<a class="reference external" href="https://arxiv.org/abs/1701.06538">Shazeer…quoc le, hinton, dean, 2017</a>) have been studied with token-based routing with backprop</p></li>
<li><p>replace FFN in transformers with expert layers</p>
<ul>
<li><p>GShard <a class="reference external" href="https://arxiv.org/abs/2006.16668">Lepikhin et al. (2021)</a>, which appplies this concept to machine translation</p></li>
<li><p>Switch transformers (<a class="reference external" href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf">Fedus et al. (2022)</a>) simplifies the architecture to activation of only one expert per layer</p></li>
</ul>
</li>
<li><p>BASE Layers <a class="reference external" href="https://proceedings.mlr.press/v139/lewis21a.html">Lewis et al. (2021)</a> - find an alternative approach to routing by formulating it as a linear assignment problem</p></li>
<li><p>Hash layers <a class="reference external" href="https://arxiv.org/abs/2106.04426">Roller et al. (2021)</a> use a fixed hash as the gating function</p></li>
<li><p>THOR (<a class="reference external" href="https://arxiv.org/abs/2110.04260">zuo, liu…zhao, gao, 2022</a>) - randomly route to different experts then merge at the parameter level at test time</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.sscardapane.it/assets/files/nnds2022/Lecture_8_Dynamic_NNs.pdf">routing notes</a> - make hard decision but still want to learn probabilities</p>
<ul>
<li><p>straight-through estimator (STE) - take the argmax during the forward pass, while considering the original probabilities in the backward pass</p>
<ul>
<li><p>highly biased</p></li>
</ul>
</li>
<li><p>gumbel-softmax- allows for better sampling</p></li>
</ul>
</li>
<li><p>specialized experts as fully independent models (sometimes for multi-task learning)</p>
<ul>
<li><p>DEmix Layers (<a class="reference external" href="https://arxiv.org/abs/2108.05036">Gururangan…smith, zettlemoyer, 2021</a>) –  DEMix layers – placed in the feedforward layers of the Transformer – contain experts which specialize on specific domains. Routing at train time is determined only by the domain label, but all experts are activated at inference time and mixed according to weights estimated from a validation set</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.07689">Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners</a> (gupta…awadallah, gao, 2022) - use task description to improve routing</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.06266">Pfeiffer et al. (2022)</a> - multilingual expert model with language-specific routing</p></li>
<li><p>task-level MoE <a class="reference external" href="https://arxiv.org/abs/2110.03742">Kudugunta et al. (2021</a>) – multi-task expert model with task-specific routing</p></li>
<li><p>scaling up</p>
<ul>
<li><p>OPT-MOE (<a class="reference external" href="https://arxiv.org/abs/2112.10684">artetxe et al. 2021</a>)</p></li>
<li><p>AutoMoE (<a class="reference external" href="https://arxiv.org/abs/2210.07535">jawahar, mukherjee, liu…gao, 2022</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Towards Understanding Mixture of Experts in Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2208.02813">chen…gu, li, 2022</a>)</p></li>
<li><p>Interpretable Mixture of Experts (<a class="reference external" href="https://arxiv.org/abs/2206.02107">ismail…pfister, 2023</a>) - each sample assigned to single expert for prediction</p>
<ul>
<li><p>InterpretCC: Intrinsic User-Centric Interpretability through Global Mixture of Experts (<a class="reference external" href="https://arxiv.org/abs/2402.02933v2">swamy…kaser, 2024</a>) - first, discriminator predicts which features are important. Then, all other features are masked and used for prediction. The discriminator network can additionally select a different network to send different features to</p></li>
</ul>
</li>
</ul>
</section>
<section id="pruning">
<h3><span class="section-number">7.7.2.3. </span>pruning<a class="headerlink" href="#pruning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot (<a class="reference external" href="https://arxiv.org/abs/2301.00774">frantar &amp; alistarh, 2023</a>) - prune GPT-style models to atleast 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy</p></li>
<li><p>Cramming: Training a Language Model on a Single GPU in One Day (<a class="reference external" href="https://arxiv.org/abs/2212.14034">geiping &amp; goldstein, 2022</a>) - tricks for training BERT</p></li>
</ul>
</section>
<section id="adaptation-transfer">
<h3><span class="section-number">7.7.2.4. </span>adaptation / transfer<a class="headerlink" href="#adaptation-transfer" title="Link to this heading">#</a></h3>
<p><em>These are transformer-specific. For more general notes, see <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transfer_learning.html">📌 transfer learning</a> or <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transfer_learning.html">📌 uncertainty</a>.</em> Most of these approaches can be combined with metalearning.</p>
<ul class="simple">
<li><p>finetuning</p>
<ul>
<li><p>finetune all DNN params</p></li>
<li><p>finetune linear layer on activations</p>
<ul>
<li><p>standard - train linear model on the embedding of the first token (usually an added <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token) (<a class="reference external" href="https://aclanthology.org/N18-1202/">peters et al. 2018</a>)</p></li>
<li><p>finetune linear model on all the activations</p>
<ul>
<li><p>e.g. <a class="reference external" href="https://arxiv.org/abs/2201.03529">evci, et al. 2022</a> - learn linear layer (using group-lasso) on features extracted from all layers</p></li>
</ul>
</li>
</ul>
</li>
<li><p>finetune specific DNN params (e.g. just the bias terms)</p>
<ul>
<li><p>Cutting Down on Prompts and Parameters (<a class="reference external" href="https://arxiv.org/abs/2106.13353">logan…sameer singh, riedel, 2021</a>) - finetune only the bias terms; works even with null prompts</p></li>
<li><p>BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models (<a class="reference external" href="https://arxiv.org/abs/2106.10199">zaken, ravfogel, &amp; goldberg, 2021</a>) - finetune only bias terms</p></li>
</ul>
</li>
</ul>
</li>
<li><p>adapter - finetune lightweight layers on top of pre-trained layers (between finetuning all layers, and just finetuning a new layer)</p>
<ul>
<li><p>add some new layers and retrain some specific things (all human choices)</p></li>
<li><p>side-tuning (<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-030-58580-8_41">zhang, sax…malik, 2020</a>) - train a “side” network that is fused with the pretrained model via summation</p></li>
<li><p>Combining Modular Skills in Multitask Learning (<a class="reference external" href="https://arxiv.org/pdf/2202.13914.pdf">ponti, sordoni, bengio, &amp; reddy, 2022</a>) - learn adaptor with disentangled inventory of skills</p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v97/houlsby19a.html">Parameter-Efficient Transfer Learning for NLP</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2007.07779">AdapterHub: A Framework for Adapting Transformers</a></p></li>
</ul>
</li>
<li><p>vaguely similar to adapter</p>
<ul>
<li><p>LoRA</p></li>
<li><p>QLoRA: Efficient Finetuning of Quantized LLMs (<a class="reference external" href="https://arxiv.org/abs/2305.14314">dettmers, …, zettlemoyer, 2023</a>)</p></li>
<li><p>TOAST (<a class="reference external" href="https://arxiv.org/pdf/2305.15542.pdf">shi, …, darrel, xin wang, 2023</a>) - use top-down attention steering for efficient finetuning</p></li>
</ul>
</li>
<li><p>predict a mask</p>
<ul>
<li><p>ablate some model weights by training a binary mask over model parameters (Zhao et al., 2020; Radiya-Dixit and Wang, 2020)</p></li>
<li><p>predict mask over attention heads</p></li>
</ul>
</li>
<li><p>prompting = few-shot learning = priming = in-context learning (starts with GPT)</p>
<ul>
<li><p>prompting without changing any model parameters</p>
<ul>
<li><p>limitation: can’t exploit sets longer than the training window</p></li>
</ul>
</li>
<li><p>MetaICL: Learning to Learn In Context (<a class="reference external" href="https://arxiv.org/abs/2110.15943">min et al. 2022</a>) - tune LLM to do in-context learning on a large set of training tasks (few-shot prompting and training time and at test-time)</p></li>
<li><p>Visual Prompting via Image Inpainting (<a class="reference external" href="https://arxiv.org/abs/2209.00647">bar…darrell, globerson, efros, 2022</a> )</p></li>
<li><p>PatternExploiting Training (PET) – Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference (<a class="reference external" href="https://aclanthology.org/2021.eacl-main.20.pdf">schick &amp; schutze, 2021</a>)</p>
<ul>
<li><p><strong>cloze questions</strong> - same as masked language modeling: task is to replace some missing words</p></li>
<li><p>use cloze-question templates (e.g. it was “good” or “bad”) to get soft labels for unlabeled data and then finetune on theses</p></li>
</ul>
</li>
</ul>
</li>
<li><p>prompt-tuning (also see next section on autoprompting)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.11961">Attentional Mixtures of Soft Prompt Tuning for Parameter-efficient Multi-task Knowledge Sharing</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.08408">STT: Soft Template Tuning for Few-Shot Adaptation</a></p></li>
<li><p>Mixture of Soft Prompts for Controllable Data Generation (<a class="reference external" href="https://arxiv.org/abs/2303.01580">chen, … yu, 203</a>) - LLMs as Synthetic Data Generators for Training Smaller Models</p></li>
</ul>
</li>
</ul>
<p><strong>mt-dnn line of work</strong></p>
<ul class="simple">
<li><p>Multi-Task Deep Neural Networks for Natural Language Understanding (<a class="reference external" href="https://aclweb.org/anthology/papers/P/P19/P19-1441/">xiaodong liu … gao 2019</a>) - multi-task learning on the 9 glue tasks (first layers are shared, then some task-specific layers at top)</p></li>
<li><p>RAdam: On the Variance of the Adaptive Learning Rate and Beyond (<a class="reference external" href="https://openreview.net/pdf?id=rkgz2aEKDr">liyuan liu…gao, han, 2020</a>)</p>
<ul>
<li><p>usually need to do learning-rate warmup when trainin (e.g. with Adam)</p></li>
<li><p>RAdam = add a term to rectify the variance of the adaptive learning rate in Adam</p></li>
</ul>
</li>
<li><p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization (<a class="reference external" href="https://aclanthology.org/2020.acl-main.197/">jiang…gao, zhao, 2020</a>)</p>
<ol class="arabic simple">
<li><p>Smoothness-inducing regularization, which effectively manages the complexity of the model</p></li>
<li><p>Bregman proximal point optimization to prevent aggressive updating</p></li>
</ol>
</li>
<li><p>Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding (<a class="reference external" href="https://aclanthology.org/2020.acl-demos.16/">xiaodong liu…gao, 2020</a>)</p></li>
<li><p>Posterior Differential Regularization with f-divergence for Improving Model Robustness (<a class="reference external" href="https://aclanthology.org/2021.naacl-main.85/">hao cheng, …, gao 2021</a>)</p>
<ul>
<li><p>regularize model posterior difference between clean + noisy inputs (e.g. adversarially attacked inputs)</p></li>
</ul>
</li>
</ul>
<p><strong>comparing different tasks</strong></p>
<ul class="simple">
<li><p>Task2Vec: Task Embedding for Meta-Learning (<a class="reference external" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.html">achille, …, soatto, perona, 2019</a>) - summarize each task as a vector, by taking diagonal of fisher info matrix (derivative of network output wrt to parameters) - clusters similar tasks</p></li>
<li><p>Efficiently Tuned Parameters are Task Embeddings (<a class="reference external" href="https://arxiv.org/abs/2210.11705">zhou…mcauley, 2022</a>)</p></li>
<li><p>Editing Models with Task Arithmetic (<a class="reference external" href="https://arxiv.org/abs/2212.04089">ilharco, ribeiro, …, farhadi, 2022</a>) - task vector is model weights after task finetuning - model weights before finetuning</p>
<ul>
<li><p>can use this direction to alter model behavior</p></li>
</ul>
</li>
<li><p>Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation (<a class="reference external" href="https://arxiv.org/abs/2205.12647">vu….constant, 2022</a>) - train with prompts of some (language translation, task) pairs and show that they can generalize to new (language, task) pairs</p></li>
</ul>
<p><strong>instruction tuning / rlhf</strong></p>
<ul class="simple">
<li><p>Teach Llamas to Talk: Recent Progress in Instruction Tuning (<a class="reference external" href="https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/">gao blogpost 2023</a>)</p></li>
<li><p>Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs, PASTA (<a class="reference external" href="https://arxiv.org/abs/2311.02262">zhang et al. 2023</a>)</p></li>
<li><p>The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction (<a class="reference external" href="https://arxiv.org/abs/2312.13558">sharma…misra, 2023</a>)</p></li>
<li><p>human feedback</p>
<ul>
<li><p>Learning to summarize with human feedback (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html">OpenAI, 2020</a>)</p></li>
<li><p>Can language models learn from explanations in context? (<a class="reference external" href="https://arxiv.org/abs/2204.02329">lampinen et al. 2022</a>)</p></li>
<li><p>natural language feedback (<a class="reference external" href="https://arxiv.org/abs/2204.14146">scheurer et al. 2022</a>) - makes training more efficient</p>
<ul>
<li><p>Training Language Models with Language Feedback at Scale (<a class="reference external" href="https://arxiv.org/pdf/2303.16755.pdf">scheurer et al. 2023</a>)</p></li>
</ul>
</li>
<li><p>Explanation-based Finetuning Makes Models More Robust to Spurious Cues (<a class="reference external" href="https://arxiv.org/abs/2305.04990">ludan…callison-burch, 2023</a>)</p>
<ul>
<li><p>Post hoc explanations of language models can improve language models (<a class="reference external" href="https://arxiv.org/abs/2305.11426">krishna…singh, lakkaraju, 2023</a>) - use rationales as corrective signals for LLMs</p></li>
<li><p>Show Me How It’s Done: The Role of Explanations in Fine-Tuning Language Models (<a class="reference external" href="https://arxiv.org/pdf/2402.07543.pdf">ballout…kuhnberger, 2023</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback (<a class="reference external" href="https://arxiv.org/abs/2309.00267">lee…rastogi, 2023</a>)</p>
<ul>
<li><p>Tuning Language Models by Proxy (<a class="reference external" href="https://arxiv.org/abs/2401.08565">liu…choi, smith, 2024</a>)</p></li>
<li><p>Self-Rewarding Language Models (<a class="reference external" href="https://arxiv.org/abs/2401.10020">yuan…weston, 2024</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="test-time-training">
<h3><span class="section-number">7.7.2.5. </span>test-time training<a class="headerlink" href="#test-time-training" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Learning to (Learn at Test Time): RNNs with Expressive Hidden States (<a class="reference external" href="https://arxiv.org/abs/2407.04620">sun…guestrin, 2024</a>)</p>
<ul>
<li><p><img alt="ttt_lm" src="../../_images/ttt_lm.jpeg" /></p></li>
</ul>
</li>
<li><p>Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate (<a class="reference external" href="https://arxiv.org/abs/2501.17703">wang…chen, 2025</a>)</p></li>
<li><p>s1: Simple test-time scaling (<a class="reference external" href="https://arxiv.org/pdf/2501.19393">muennighof…hashimoto, 2025</a>)</p></li>
</ul>
</section>
</section>
<section id="mech-interp">
<h2><span class="section-number">7.7.3. </span>(mech) interp<a class="headerlink" href="#mech-interp" title="Link to this heading">#</a></h2>
<section id="model-merging">
<h3><span class="section-number">7.7.3.1. </span>model merging<a class="headerlink" href="#model-merging" title="Link to this heading">#</a></h3>
<p>Model merging (some of these are non-transformer papers) = combine different models that have the same architecture (see collection of papers <a class="reference external" href="https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66">here</a> and huggingface blog post <a class="reference external" href="https://huggingface.co/blog/mlabonne/merge-models">here</a>). Also see the review paper Deep Model Fusion: A Survey (<a class="reference external" href="https://arxiv.org/abs/2309.15698">li…shen, 2023</a>)</p>
<ul class="simple">
<li><p>standard methods (see <a class="reference external" href="https://github.com/arcee-ai/mergekit">mergekit package</a>)</p>
<ol class="arabic simple">
<li><p>linear averaging, e.g. model soups (<a class="reference external" href="https://proceedings.mlr.press/v162/wortsman22a.html">wortsman…schmidt, 2021</a>)</p></li>
<li><p>spherical linear interpolation - interpolate angle but keep norm constant</p></li>
<li><p>TIES: Resolving Interference When Merging Models (<a class="reference external" href="https://arxiv.org/abs/2306.01708">yadav…raffel, bansal, 2023</a>)</p>
<ol class="arabic simple">
<li><p>only keep top-k% most significant changes in weights</p></li>
<li><p>vote on signs of parameters</p></li>
</ol>
</li>
<li><p>DARE (<a class="reference external" href="https://arxiv.org/abs/2311.03099">yu…li 2023</a>)</p>
<ol class="arabic simple">
<li><p>randomly reset <span class="math notranslate nohighlight">\(p\)</span> fraction of changed fine-tuned weights to their original values in the base model</p></li>
<li><p>rescale remaining changed weights by <span class="math notranslate nohighlight">\(1/(1-p)\)</span></p></li>
</ol>
</li>
<li><p>passthrough/frankenmerging</p>
<ol class="arabic simple">
<li><p>stack layers to yield model with different size</p></li>
<li><p>e.g. depth up-scaling creates a larger model by merging some layers and copying others (solar 10.7B, <a class="reference external" href="https://arxiv.org/abs/2312.15166">kim…kim, 2023</a>)</p></li>
</ol>
</li>
</ol>
</li>
<li><p>more complex posthoc methods</p>
<ul>
<li><p>Learning to Route Among Specialized Experts for Zero-Shot Generalization (<a class="reference external" href="https://arxiv.org/abs/2402.05859">muqeeth, …, raffel, 2024</a>) - PHATGOOSE routes to different LoRA model for each token and at each layer</p></li>
<li><p>Fisher-Weighted Averaging (<a class="reference external" href="https://arxiv.org/abs/2111.09832">matena &amp; raffel, 2022</a>) - merge models with same architecture with particular weights</p></li>
<li><p>Git Re-Basin: Merging Models modulo Permutation Symmetries (<a class="reference external" href="https://arxiv.org/abs/2209.04836">ainsworth, hayase, &amp; srinivasa, 2022</a>) - permute units of one model to align them with a reference model before merging; supports linear mode connectivity between ResNet models on CIFAR</p>
<ul>
<li><p>ZipIt! Merging Models from Different Tasks without Training (<a class="reference external" href="https://arxiv.org/abs/2305.03053">stoica…hoffman, 2023</a>) - layerwise merging &amp; don’t merge all the layers</p></li>
</ul>
</li>
<li><p>Model Merging by Uncertainty-Based Gradient Matching (<a class="reference external" href="https://arxiv.org/abs/2310.12808">adheim…khan, 2023</a>)</p></li>
<li><p>UnIVAL: multimodal merging (<a class="reference external" href="https://arxiv.org/abs/2307.16184">shukor…cord, 2023</a>)</p>
<ul>
<li><p>Multimodal Model Merging (<a class="reference external" href="https://arxiv.org/abs/2304.14933">sung…bansal, wang, 2023</a>) - merge a separately trained vision &amp; language model and get a multiomodal model</p></li>
</ul>
</li>
<li><p>LoraHub (<a class="reference external" href="https://arxiv.org/abs/2307.13269">huang…lin, 2023</a>) - fiven examples from a new task, merge LoRA adaptors</p></li>
<li><p>AdaMerging: Adaptive Model Merging for Multi-Task Learning (<a class="reference external" href="https://arxiv.org/abs/2310.02575">yang…tao, 2023</a>) - learn coefficients to average models by minimizing entropy on unlabeled test samples</p></li>
<li><p>Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization (<a class="reference external" href="https://arxiv.org/abs/2212.10445">rame…bottou, lopez-paz, 2022</a>) - finetune many models initially trained on diverse tasks then average their weights</p>
<ul>
<li><p>Diverse Weight Averaging for Out-of-Distribution Generalization (<a class="reference external" href="https://arxiv.org/abs/2205.09739">rame…cord, 2023</a>)</p></li>
</ul>
</li>
<li><p>UltraFuser - 2-stage training with token-level routing to 3 models (<a class="reference external" href="https://arxiv.org/pdf/2403.08281.pdf">ding…sun, 2024</a>)</p></li>
</ul>
</li>
<li><p>training paradigms</p>
<ul>
<li><p>Branch-Train-Merge: ELMS (Expert LMs) (<a class="reference external" href="https://arxiv.org/abs/2208.03306">li…smith, zettlemoyer 2022</a>)</p>
<ul>
<li><p>parallel language model of smaller expert LMs</p></li>
<li><p>each can be added/removed, ensembled, or parameter-averaged at any time for efficient scaling and rapid customization</p></li>
<li><p>improves perplexities, when controlling for training cost</p>
<ul>
<li><p>require expert domain specialization</p></li>
</ul>
</li>
<li><p>Cluster-Branch-Train-Merge (<a class="reference external" href="https://arxiv.org/abs/2303.14177">gururangan…smith, zettlemoyer, 2023</a>) - start by clustering data to do unsupervised domain discovery</p></li>
</ul>
</li>
<li><p>LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging (<a class="reference external" href="https://arxiv.org/abs/2410.17146">wang…frossard, 2024</a>) - updating deeper layers more than shallow layers helps prevent forgetting across tasks</p></li>
</ul>
</li>
<li><p>fit many models into one</p>
<ul>
<li><p>superposition of many models into one (<a class="reference external" href="https://proceedings.neurips.cc/paper/2019/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html">cheung…olshausen, 2019</a>) - both during training/testing models are indexed via a high-dim key for each task</p></li>
<li><p>supermasks in superposition (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html">wortsman, …, yosinski, farhadi, 2020</a>) - randomly fixed base net + for each task finds subnet that performs well</p>
<ul>
<li><p>if task identity not given, correct subnet inferred by minimizing output entropy</p></li>
</ul>
</li>
</ul>
</li>
<li><p>non-transformer</p>
<ul>
<li><p>snapshot ensembles - average different checkpoints during training (<a class="reference external" href="https://arxiv.org/abs/1704.00109">huang et al. 2017</a>)</p></li>
<li><p>stochastic weight averaging (<a class="reference external" href="https://arxiv.org/abs/1803.05407v3">izmailov, …, wilson, 2019</a>) - average multiple checkpoints during training</p></li>
<li><p>batch ensemble (<a class="reference external" href="https://arxiv.org/pdf/2002.06715.pdf">wen et al. 2020</a>) - have several rank-1 keys that index different weights hidden within one neural net</p></li>
<li><p>data-based distillation for model merging (<a class="reference external" href="https://arxiv.org/abs/2310.17653">roth…akata, 2024</a>) - can combine multiple models that excel at different classes using data-based distillation</p></li>
<li><p>Model Fusion via Optimal Transport (<a class="reference external" href="https://arxiv.org/abs/1910.05653">singh &amp; jaggi, 2019</a>) - layer-wise fusion algorithm using optimal transport</p></li>
<li><p>Qualitatively characterizing neural network optimization problems (<a class="reference external" href="https://arxiv.org/abs/1412.6544">goodfellow, viynals, &amp; saxe, 2014</a>) -  linear interpolation experiments on DNNs</p></li>
</ul>
</li>
</ul>
</section>
<section id="editing">
<h3><span class="section-number">7.7.3.2. </span>editing<a class="headerlink" href="#editing" title="Link to this heading">#</a></h3>
<p>Editing is generally very similar to just adaptation/finetuning. One distinction is that it tends to try to keep changes localized, in an effort not to affect performance for most of the model.</p>
<ul class="simple">
<li><p>Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs (<a class="reference external" href="https://arxiv.org/abs/2311.02262">zhang, singh, liu, liu, yu, gao, zhao, 2023</a>) - upweight attention scores at specific positions to improve LLM controllability</p></li>
<li><p>Editing LLMs: Problems, Methods, and Opportunities (<a class="reference external" href="https://arxiv.org/pdf/2305.13172.pdf">yao, …, zhang, 2023</a>)</p>
<ul>
<li><p>model-editing = data-efficient alterations to a model</p></li>
</ul>
</li>
<li><p>memory-based</p>
<ul>
<li><p>SERAC: Memory-Based Model Editing at Scale (<a class="reference external" href="https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf">mitchell…manning, finn, 2022</a>)</p>
<ul>
<li><p>keep track of list of edits in external memory and use them as appropriate context at test time (don’t finetune the model, instead train a smaller simpler model for using the external contexts)</p></li>
</ul>
</li>
<li><p>Language Modeling with Editable External Knowledge (<a class="reference external" href="https://arxiv.org/abs/2406.11830v1">li, liu…, neubig, andreas, 2024</a>) - have LLM rewrite and update knowledge base as new docs are added</p></li>
<li><p>T-Patcher (Huang et al., 2023) and CaliNET (Dong et al., 2022) introduce extra trainable parameters into the feed- forward module of PLMs</p></li>
</ul>
</li>
<li><p>weight updates</p>
<ul>
<li><p>Knowledge Neurons in Pretrained Transformers (<a class="reference external" href="https://arxiv.org/abs/2104.08696">dai et al. 2021</a>) - integrated gradients wrt to each neuron in BERT, then selectively udpate these neurons</p></li>
<li><p>ROME: Locating and Editing Factual Associations in GPT (<a class="reference external" href="https://arxiv.org/abs/2202.05262">meng, bau et al. 2022</a>)</p>
<ul>
<li><p><em>localize factual associations</em> - causal intervention for identifying neuron activations that are decisive in a model’s factual predictions</p>
<ul>
<li><p>“causal traces” - run net multiple times, introducing corruptions and then restore states from original non-corrupted forward pass to see which states can restore the original results</p></li>
<li><p>a small number of states contain info that can flip the model from one state to another</p></li>
</ul>
</li>
<li><p><em>change factual associations</em> - modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME)</p></li>
<li><p>MEMIT: Mass Editing Memory in a Transformer (<a class="reference external" href="https://memit.baulab.info/">meng…, bau, 2022</a>)</p></li>
<li><p>Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters (<a class="reference external" href="https://arxiv.org/abs/2211.11031v3">hartvigsen, …, palangi, …, ghassemi, 2023</a>)</p></li>
<li><p>Flexible Model Interpretability through Natural Language Model Editing (<a class="reference external" href="https://arxiv.org/abs/2311.10905">d’oosterlinck, …, potts, 2023</a>)</p></li>
<li><p>Model Editing with Canonical Examples (<a class="reference external" href="https://arxiv.org/abs/2402.06155">hewitt, …, liang, manning, 2024</a>)</p></li>
</ul>
</li>
<li><p>meta-learning</p>
<ul>
<li><p>KnowledgeEditor: Editing Factual Knowledge in Language Models (<a class="reference external" href="https://arxiv.org/pdf/2104.08164.pdf">de cao, aziz, &amp; titov, 2021</a>) - train a network that takes in input, output, edit and predicts a weight update to the model</p></li>
<li><p>MEND: Fast model editing at scale (<a class="reference external" href="https://arxiv.org/abs/2110.11309">mitchell…finn, manning, 2022</a>)</p>
<ul>
<li><p>a collection of small auxiliary editing networks that use a single desired input-output pair to edit a pre-trained model</p></li>
<li><p>MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>REMEDI (<a class="reference external" href="https://arxiv.org/pdf/2304.00740.pdf">hernandez, li, &amp; andreas, 2023</a>) and related activation engineering</p>
<ul>
<li><p>get “edit vectors” by obtaining embeddings when passing attributes through LLM</p></li>
<li><p>perform edit by by adding linear transformation of edit vector to prompt embedding</p>
<ul>
<li><p>then, perform generation with latent embedding</p></li>
<li><p>learn linear transformation given a dataset of examples with attributes and desired completions</p>
<ul>
<li><p>(also regularize the model to not change <em>too much</em> on other stuff)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Activation Addition: Steering Language Models Without Optimization (<a class="reference external" href="https://arxiv.org/abs/2308.10248">turner…macdiarmid, 2023</a>)</p>
<ul>
<li><p>blog post: activation engineering: Steering GPT-2-XL by adding an activation vector (<a class="reference external" href="https://www.alignmentforum.org/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#6__The_Eiffel_Tower_is_in_Rome">turner, …, mini, 2023</a>)</p></li>
<li><p>obtain “steering vector” by embedding a phrase (e.g. <em>love</em>) and adding that vector to the llm embedding during generation</p>
<ul>
<li><p>they only add the embedding for some layers for some tokens</p></li>
</ul>
</li>
<li><p>Extracting Latent Steering Vectors from Pretrained Language Models (<a class="reference external" href="https://arxiv.org/abs/2205.05124">subramani, …, peters, 2022</a>) - find latent vectors via optimization that cause an LLM to output a particular sequence</p>
<ul>
<li><p>then, use these vectors to do things like transfer to new tasks / compute textual similarity</p></li>
</ul>
</li>
<li><p>Function Vectors in LLMs (<a class="reference external" href="https://arxiv.org/pdf/2310.15213.pdf">todd…wallace, bau, 2023</a>)</p>
<ul>
<li><p>In-Context Learning Creates Task Vectors (<a class="reference external" href="https://arxiv.org/pdf/2310.15916">hendel, geva, &amp; globerson, 2023</a>)</p></li>
</ul>
</li>
<li><p>Programming Refusal with Conditional Activation Steering (<a class="reference external" href="https://arxiv.org/abs/2409.05907">lee…dhurandhar, 2024</a>)</p></li>
</ul>
</li>
<li><p>PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions (<a class="reference external" href="https://drive.google.com/file/d/1CXSUii4w8Y2uj-zLm8zRl63SYh45FaZL/view">chen…sameer singh…kelvin guu, 2023</a>)</p></li>
<li><p>new datasets</p>
<ul>
<li><p>MQUAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions (<a class="reference external" href="https://www.cs.princeton.edu/~zzhong/papers/MQuAKE.pdf">zhong…manning, potts, chen, 2023</a>) - introduces benchmark MQUAKE + method MeLLo, which stores edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2305.17553.pdf">COUNTERFACT+ benchmark</a> - checks that edits don’t affect existing info</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2312.12747">ALMANACS</a>: A Simulatability Benchmark for Language Model Explainability</p></li>
</ul>
</li>
<li><p>model unlearning approaches (see review Rethinking Machine Unlearning for LLMs, <a class="reference external" href="https://arxiv.org/abs/2402.08787">liu et al. 2024</a>)</p>
<ul>
<li><p>gradient ascent - worsen performance on set of examples to forget</p></li>
<li><p>gradient descent - improve performance on examples labeled with hidden info, e.g. response “I don’t know”</p></li>
<li><p>localization-informed unlearning, e.g. ROME</p></li>
<li><p>influence function-based methods</p></li>
<li><p>prompt-based (e.g. only change prompt rather than model parameters)</p></li>
<li><p>Offset Unlearning for Large Language Models (<a class="reference external" href="https://arxiv.org/pdf/2404.11045.pdf">huang…poon, chen , 2024</a>) - unlearning for black-box models by learning the logit offset for contrasting with a smaller model</p></li>
</ul>
</li>
</ul>
</section>
<section id="direct-weight-inspection">
<h3><span class="section-number">7.7.3.3. </span>direct weight inspection<a class="headerlink" href="#direct-weight-inspection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>overviews</p>
<ul>
<li><p>Overview of mechanistic interpretability (<a class="reference external" href="https://www.neelnanda.io/mechanistic-interpretability/favourite-papers">nanda, 2022+</a>)</p></li>
<li><p>review paper (<a class="reference external" href="https://arxiv.org/abs/2207.13243">rauker…hadfield-menell, 2023</a>)</p></li>
<li><p>A Primer on the Inner Workings of Transformer-based Language Models (<a class="reference external" href="https://arxiv.org/pdf/2405.00208">ferrando et al. 2024</a>)</p></li>
<li><p>Representation engineering: A Top-Down Approach to AI Transparency (<a class="reference external" href="https://arxiv.org/pdf/2310.01405.pdf">zou…kolter, hendrycks, 2023</a>)</p>
<ul>
<li><p>representation engineering (RepE) -  analyzes representations/representation transformations rather than neurons or circuits</p></li>
<li><p>basically extends probing to more general tasks, including model control</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors (<a class="reference external" href="https://arxiv.org/abs/2103.15949">yun, chen, olshausen, lecun, 2021</a>) - investigate LLM embeddings of different words using dictionary learning</p>
<ul>
<li><p>LLMs produce interesting contextualized word embeddings</p></li>
<li><p>dictionary elements (of activations across layers) correspond to meaningful things</p></li>
<li><p>dictionary element has size <span class="math notranslate nohighlight">\(d\)</span>, the embedding size</p>
<ul>
<li><p>given list of sentences <span class="math notranslate nohighlight">\(S\)</span>, training matrix has size <span class="math notranslate nohighlight">\(\left(\underbrace{\text{num\_layers}}_{\text{12 for BERT}} \cdot \sum_{s \in S} \text{len(s)}\right) \times \underbrace{d}_{\text{768 for BERT}}\)</span></p></li>
</ul>
</li>
<li><p>dictionary coefficient: maps (text, layer, sequence_index) <span class="math notranslate nohighlight">\(\to\)</span> coefficient</p>
<ul>
<li><p>extract <span class="math notranslate nohighlight">\(d\)</span>-dimensional embedding for text at specified layer &amp; sequence_index</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Neuron-level Interpretation of Deep NLP Models: A Survey (<a class="reference external" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00519/113852/Neuron-level-Interpretation-of-Deep-NLP-Models-A">sajjad et al. 2022</a>)</p>
<ul>
<li><p>previous works generally use pre-specified concepts, and focus on</p>
<ul>
<li><p>concept search - given a neuron find its concept(s)</p></li>
<li><p>neuron search - (ii) given a concept find its matching neuron(s)</p></li>
</ul>
</li>
<li><p>concept search</p>
<ul>
<li><p>visualization, e.g. <a class="reference external" href="https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson/40be3888daa5c2e5af4d36ae22f690bcc8caf600">karpathy, johnson, fei-fei li, 2015</a> visualize LSTM head response in text</p></li>
<li><p>elicit top-k ngram responses on a corpus, which are then labelled manually (<a class="reference external" href="https://www.semanticscholar.org/paper/Representation-of-Linguistic-Form-and-Function-in-K%C3%A1d%C3%A1r-Chrupa%C5%82a/9462eee3e5eff15df5e97c38e24072c65e581cee">kadar et al. 2017</a>)</p></li>
<li><p>elicit top-k activating sentences from a corpus, which are then summarized using a parse tree into a synthetic explanation (<a class="reference external" href="https://arxiv.org/pdf/1902.07249.pdf">na…kim, 2019</a>)</p>
<ul>
<li><p>limitation: the explanation may be ungrammatical and biased towards something arbitrary (like reptition)</p></li>
</ul>
</li>
<li><p>input maximization (e.g. textattack, <a class="reference external" href="https://www.semanticscholar.org/paper/Interpretable-Textual-Neuron-Representations-for-Poerner-Roth/36fc119ce631c3ec66866ce31918978824d05f78">poerner et al. 2018</a>)</p></li>
</ul>
</li>
<li><p>Evaluating Neuron Interpretation Methods of NLP Models (<a class="reference external" href="https://arxiv.org/abs/2301.12608">fan…sajjad, 2023</a>) - metric is how well evaluation from one method matches the other ones</p></li>
</ul>
</li>
<li><p>A Circuit for Indirect Object Identification in GPT-2 small (<a class="reference external" href="https://arxiv.org/abs/2211.00593">wang, …, steinhardt, 2022</a>)</p>
<ul>
<li><p>explanation encompasses 26 attention heads grouped into 7 main classes</p></li>
<li><p>task: indirect object identification - “When Mary and John went to the store, John gave a drink to _ ” should be “Mary”</p></li>
<li><p>circuit</p>
<ul>
<li><p>identify all previous names</p></li>
<li><p>remove duplicated names</p></li>
<li><p>output remaining name</p></li>
</ul>
</li>
<li><p>Circuit Component Reuse Across Tasks in Transformer Language Models (<a class="reference external" href="https://arxiv.org/abs/2310.08744">merullo, eickhoff, &amp; pavlick 2024</a>) - find that the same circuit is used for 2 different tasks: IOI from above and Colored objects (from big-bench)</p></li>
<li><p>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models (<a class="reference external" href="https://arxiv.org/abs/2403.19647v1">marks…belinkov, bau, mueller, 2024</a>)</p>
<ul>
<li><p>ex. for biasbios, find circuit and intervene so that it doesn’t rely on gender</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Interpretability at Scale: Identifying Causal Mechanisms in Alpaca (<a class="reference external" href="https://arxiv.org/pdf/2305.08809.pdf">wu…, potts, goodman, 2023</a>) - propose boundless DAS and automatically identify a circuit for math</p>
<ul>
<li><p>builds on DAS (<a class="reference external" href="https://arxiv.org/abs/2303.02536">geiger, …goodman, 2023</a>)</p></li>
</ul>
</li>
<li><p>N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in LLMs (<a class="reference external" href="https://arxiv.org/abs/2304.12918">foote, nanda, …, barez, 2023</a>) - explain each neuron in a graph</p></li>
<li><p>Finding Skill Neurons in Pre-trained Transformer-based Language Models (<a class="reference external" href="https://arxiv.org/abs/2211.07349">wang et al. 2022</a>) - some individual neurons are predictive of the final task (dubbed “skill neurons’)</p></li>
<li><p>circuits thread (<a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">elhage…olah, 2021</a>)</p></li>
<li><p>all layers are same dimension and each attention block <strong>adds</strong> a vector to it</p></li>
<li><p>Although they’re parameterized as separate matrices, <span class="math notranslate nohighlight">\(W_O W_V\)</span> and <span class="math notranslate nohighlight">\(W_Q^T W_K\)</span> can always be thought of as individual, low-rank matrices</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \in \mathbb R^{d_{embed} \times d_{sequence}}\)</span>: <span class="math notranslate nohighlight">\(d_{embed}\)</span> can be hundreds - tens of thousands</p></li>
<li><p><span class="math notranslate nohighlight">\(W_Q, W_K, W_V \in \mathbb R^{d_{attn} \times d_{embed}}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_Q^TW_k \in \mathbb R ^{d_{embed} \times d_{embed}}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(W_O \in \mathbb R^{d_{embed} \times d_{attn}}\)</span>: projects attention values back to embedding dimention</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_O W_V \in \mathbb R ^{d_{embed} \times d_{embed}}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(W_E \in \mathbb R^{d_{embed} \times d_{vocab}}\)</span> embeds initial tokens and <span class="math notranslate nohighlight">\(W_U \in \mathbb R^{d_{vocab} \times d_{embed}}\)</span> undoes the embedding</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d_{vocab}\)</span> can be very large, e.g. 50k</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(A = \text{softmax}(x^TW_Q^TW_kx) \in \mathbb R^{d_{sequence} \times d_{sequence}}\)</span></p></li>
</ul>
</li>
<li><p>if we have a 0-layer net (e.g. predict next token with linear layer given current token), we just learn bigram log-likelihood</p></li>
<li><p>2 circuits</p>
<ul>
<li><p>QK circuit determines which “source” token the present “destination” token attends back to and copies information from</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_{E}^{T} W_{Q}^{T} W_{K} W_{E} \in \mathbb R ^{d_{vocab} \times d_{vocab}}\)</span></p></li>
</ul>
</li>
<li><p>OV circuit describes what the resulting effect on the “out” predictions for the next token is</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_{U} W_{O} W_{V} W_{E} \in \mathbb R ^{d_{vocab} \times d_{vocab}}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>if a single head increases the probability of both <code class="docutils literal notranslate"><span class="pre">keep…</span> <span class="pre">in</span> <span class="pre">mind</span></code> and <code class="docutils literal notranslate"><span class="pre">keep…</span> <span class="pre">at</span> <span class="pre">bay</span></code>, it <em>must</em> also increase the probability of <code class="docutils literal notranslate"><span class="pre">keep…</span> <span class="pre">in</span> <span class="pre">bay</span></code> and <code class="docutils literal notranslate"><span class="pre">keep…</span> <span class="pre">at</span> <span class="pre">mind</span></code></p></li>
<li><p><strong>induction heads</strong> search previous examples of present token</p>
<ul>
<li><p>If they don’t find it, they attend to the first token and do nothing</p></li>
<li><p>if they do find it, they then look at the <em>next</em> token and copy it. This allows them to repeat previous sequences of tokens, both exactly and approximately</p></li>
<li><p>sometimes can do some kind of “fuzzy” matching</p></li>
</ul>
</li>
<li><p>tensor/kronecker product <span class="math notranslate nohighlight">\(\bigotimes\)</span>:</p>
<ul>
<li><p>Left-right multiplying: Multiplying <span class="math notranslate nohighlight">\(x\)</span> by a tensor product <span class="math notranslate nohighlight">\(A \otimes W\)</span> is equivalent to simultaneously left and right multiplying: <span class="math notranslate nohighlight">\((A \otimes W) x=A x W^{T}\)</span></p></li>
<li><p>When we add them, it is equivalent to adding the results of this multiplication: <span class="math notranslate nohighlight">\(\left(A_{1} \otimes W_{1}+A_{2} \otimes W_{2}\right) x=A_{1} x W_{1}^{T}+A_{2} x W_{2}^{T}\)</span>
<strong><a class="reference external" href="https://transformer-circuits.pub/2022/solu/index.html">Softmax Linear Units</a></strong></p></li>
</ul>
</li>
<li><p>replacing activation function with softmax linear unit increases fraction of MLP neurons which are “interpretable”, i.e. correspond to meaningful features</p>
<ul>
<li><p>however, may “hide” some non-neuron-aligned features by decreasing their magnitude and then later recovering it with LayerNorm</p></li>
</ul>
</li>
<li><p>the presence of nonlinear activation functions createse an incentive for features to align with this basis and not get superposed</p>
<ul>
<li><p>if the gains to sparse coding are large enough, this incentive will get overwhelmed</p></li>
</ul>
</li>
<li><p>ways to combat polysemanticity</p>
<ul>
<li><p>activation sparsity</p></li>
<li><p>lateral inhibition / co-occurrence sparsity</p></li>
<li><p>weight sparsity</p></li>
<li><p>superlinear activation functions</p></li>
<li><p>increase neurons per param</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\text{SoLU}(x) = x \cdot \text{softmax}(x)\)</span></p>
<ul>
<li><p>adds lateral inhibition, superlinearity, approximate sparsity</p></li>
<li><p>changes GeLU, which is approximately <span class="math notranslate nohighlight">\(\text{sigmoid}(1.7x) \cdot x\)</span></p></li>
<li><p>just changing to SoLU decrease performance, had to add LayerNorm afterwards</p></li>
</ul>
</li>
<li><p>logit lens (<a class="reference external" href="https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">2020</a>) - apply unembedding matrix to outputs of each transformer layer</p>
<ul>
<li><p>tuned-lens (<a class="reference external" href="https://arxiv.org/abs/2303.08112">belrose…steinhardt, 2023</a>) - train linear model for each layer to decode vocab</p></li>
<li><p>Analyzing Transformers in Embedding Space (<a class="reference external" href="https://arxiv.org/pdf/2209.02535.pdf">dar, …, berant, 2022</a>) - apply unembeddix matrix to weights, etc. to interpret transformers</p></li>
<li><p>Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners (<a class="reference external" href="https://arxiv.org/pdf/2405.13816v2">zhang…huang, 2024</a>) - applying logit lens finds that model internally translates to english in multilingual tasks</p></li>
</ul>
</li>
<li><p>Monitoring Latent World States in Language Models with Propositional Probes (<a class="reference external" href="https://arxiv.org/pdf/2406.19501">feng, russell, &amp; steinhardt, 2024</a>) - identifying a binding subspace in which bound
tokens have high similarity (Greg ↔ nurse) but unbound ones do not (Greg̸ ↔
physicist)</p>
<ul>
<li><p>How do Language Models Bind Entities in Context? (<a class="reference external" href="https://arxiv.org/abs/2310.17191">feng &amp; steinhardt, 2023</a>)</p></li>
</ul>
</li>
<li><p>In-Context Language Learning: Architectures and Algorithms (<a class="reference external" href="https://arxiv.org/pdf/2401.12973.pdf">akyurek…andreas, 2024</a>) - find evidence for “n-gram heads”, higher-order variants of previously seen “induction heads”</p>
<ul>
<li><p>Zoology: Measuring and Improving Recall in Efficient Language Models (<a class="reference external" href="https://arxiv.org/pdf/2312.04927.pdf">arora…rudra, &amp; re, 2023</a>) - also find evidence for ngram heads</p></li>
<li><p>Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information (<a class="reference external" href="https://arxiv.org/pdf/2502.14258">park…kang, 2025</a>)</p></li>
</ul>
</li>
<li><p>Iteration heads (<a class="reference external" href="https://arxiv.org/pdf/2406.02128">cabannes…charton, kempe, 2024</a>) - when doing CoT for tokens, hypothesized iteration head (which shows up in small transformers trained on custom iterations tasks) implements attending to tokens sequentially and also the preceding CoT token</p></li>
<li><p>ICL performance depends primarily on function-vector heads rather than induction heads (<a class="reference external" href="https://arxiv.org/pdf/2502.14010">yin &amp; steinhardt, 2025</a>)</p>
<ul>
<li><p>function-vector headsare a compact representation of a task extracted from specific attention heads, and they can be added to a model’s computation to recover ICL behavior without in-context demonstrations</p></li>
</ul>
</li>
<li><p>Retrieval Head Mechanistically Explains Long-Context Factuality (<a class="reference external" href="https://arxiv.org/abs/2404.15574">wu…fu, 2024</a>)</p></li>
<li><p>A Phase Transition between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention (<a class="reference external" href="https://arxiv.org/pdf/2402.03902.pdf">cui…zdeborova, 2024</a>) - solve 1-layer attention model for histogram task and find  phase transition</p></li>
<li><p>Rosetta Neurons: Mining the Common Units in a Model Zoo (<a class="reference external" href="https://openaccess.thecvf.com/content/ICCV2023/html/Dravid_Rosetta_Neurons_Mining_the_Common_Units_in_a_Model_Zoo_ICCV_2023_paper.html">dravid, …, efros, shocher, 2023</a>)</p>
<ul>
<li><p>Multimodal Neurons in Pretrained Text-Only Transformers (<a class="reference external" href="https://arxiv.org/pdf/2308.01544.pdf">schwettmann…torralba, 2023</a>)</p></li>
<li><p>Interpreting CLIP’s Image Representation via Text-Based Decomposition (<a class="reference external" href="https://arxiv.org/abs/2310.05916">gandelsman, efros, &amp; steinhardt, 2023</a>)</p></li>
<li><p>Universal Neurons in GPT2 Language Models (<a class="reference external" href="https://arxiv.org/abs/2401.12181">gurnee…nanda, &amp; bertsimas, 2024</a>) - study the universality of neurons across GPT2 models trained from different initial random seeds</p></li>
</ul>
</li>
<li><p>The Hydra Effect: Emergent Self-repair in Language Model Computations (<a class="reference external" href="https://arxiv.org/abs/2307.15771">mcgrath…legg, 2023</a>) - ablations atone attention layer of an LLM cause another layer to compensate</p></li>
<li><p>Neurons in LLMs: Dead, N-gram, Positional (<a class="reference external" href="https://arxiv.org/pdf/2309.04827.pdf">voita, ferrando, &amp; nalmpantis, 2023</a>)</p></li>
<li><p>Vision transformers need registers (<a class="reference external" href="https://arxiv.org/pdf/2309.16588.pdf">darcet…mairal, bojanowski, 2023</a>)</p>
<ul>
<li><p>adding extra [reg1], [reg2] tokens that aren’t used at output improve vision transformer performance and attention map interpretability</p></li>
<li><p>without these tokens, attention maps are sometimes very noisy, particularly for uninformative tokens</p></li>
</ul>
</li>
<li><p>Efficient Streaming Language Models with Attention Sinks (<a class="reference external" href="https://arxiv.org/pdf/2309.17453.pdf">xiao…lewis, 2023</a>)</p></li>
<li><p>Codebook Features: Sparse and Discrete Interpretability for Neural Networks (<a class="reference external" href="https://arxiv.org/abs/2310.17230">tamkin, taufeeque, &amp; goodman, 2023</a>)</p></li>
<li><p>Patchscope (<a class="reference external" href="https://arxiv.org/abs/2401.06102">ghandeharioun…geva, 2023</a>) - decode LLM’s representation of a token by asking another copy of it to decode from that same representation (by repeating)</p></li>
<li><p>Program synthesis via mechanistic interpretability (<a class="reference external" href="https://arxiv.org/abs/2402.05110">michaud…tegmark</a>) - condense RNN on simple algorithmic tasks into code</p></li>
<li><p>Linear Representations of Sentiment in LLMs (<a class="reference external" href="https://arxiv.org/abs/2310.15154">tigges…nanda, 2023</a>) - sentiment is distributed across tokens (not just at sentiment-laden words)</p></li>
<li><p>Your Transformer is Secretly Linear (<a class="reference external" href="https://arxiv.org/abs/2405.12250">razzhigaev…kuznetsov, 2024</a>) - many transformer layers can be replace by linear layer</p></li>
<li><p>Not All Language Model Features Are Linear (<a class="reference external" href="https://arxiv.org/abs/2405.14860">engels…tegmark, 2024</a>) - find irreducible multi-dimensional features (e.g. days of the week)</p></li>
<li><p>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking (<a class="reference external" href="https://arxiv.org/abs/2402.14811">prakash…belinkov, bau, 2024</a>) - finetuning does not seem to change the behavior of circuits, rather just enhances them</p>
<ul>
<li><p>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks (<a class="reference external" href="https://arxiv.org/abs/2311.12786">jain…krueger, 2024</a>) - finetuning learns a fairly simple wrapper that can be reversed easily</p></li>
</ul>
</li>
<li><p>sparse autoencoder (sae) papers</p>
<ul>
<li><p>Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders (<a class="reference external" href="https://arxiv.org/abs/2502.15576">wu…liu, 2025</a>) - introduce a penalty in explaining SAE features that mitigates a frequency bias to find diverse and unique words corresponding to an SAE feature</p></li>
<li><p>Improving Dictionary Learning with Gated Sparse Autoencoders (<a class="reference external" href="https://arxiv.org/pdf/2404.16014">rajamanoharan…nanda, 2024</a>)</p></li>
<li><p>neuronpedia: visualization tool for neuron SAEs (<a class="reference external" href="https://www.lesswrong.com/posts/BaEQoxHhWPrkinmxd/announcing-neuronpedia-as-a-platform-to-accelerate-research">lin &amp; bloom, 2024</a>)</p></li>
<li><p><a class="reference external" href="https://github.com/openai/transformer-debugger">transformer-debugger</a> using SAEs (openAI)</p></li>
<li><p>Automatically Interpreting Millions of Features in Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2410.13928">paulo…belrose, 2024</a>)</p></li>
</ul>
</li>
<li><p>sparse autoencoder (sae) critiques</p>
<ul>
<li><p>AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders (<a class="reference external" href="https://arxiv.org/abs/2501.17148">wu…jurafsky, manning, potts, 2025</a>)</p></li>
<li><p>Sparse Autoencoders Can Interpret Randomly Initialized Transformers (<a class="reference external" href="https://arxiv.org/abs/2501.17727">heap…aitchison, 2025</a>)</p></li>
<li><p>Sparse Autoencoders Trained on the Same Data Learn Different Features (<a class="reference external" href="https://arxiv.org/abs/2501.16615">paulo &amp; belrose, 2025</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="debugging-interpretation">
<h3><span class="section-number">7.7.3.4. </span>debugging / interpretation<a class="headerlink" href="#debugging-interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>reviews</p>
<ul>
<li><p>Rethinking Interpretability in the Era of Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2402.01761">singh, inala, galley, caruana, &amp; gao, 2024</a>)</p></li>
<li><p>Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era (<a class="reference external" href="https://arxiv.org/pdf/2403.08946.pdf">wu…liu, 2024</a>)</p></li>
</ul>
</li>
<li><p>TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues (<a class="reference external" href="https://arxiv.org/abs/2207.04154">slack…lakkaraju, sameer singh, 2022</a>) - natural language interface to query model (by converting to commands such as filtering the data / calculating importance)</p>
<ul>
<li><p>Rethinking Explainability as a Dialogue: A Practitioner’s Perspective (<a class="reference external" href="https://arxiv.org/abs/2202.01875">lakkaraju, slack, …, sameer singh, 2022</a>) - interviews with high-stakes users suggest they would like to be able to interact with systems via dialog</p></li>
</ul>
</li>
<li><p>AdaTest: Adaptive Testing and Debugging of NLP Models (<a class="reference external" href="https://aclanthology.org/2022.acl-long.230/">ribeiro &amp; lundberg, 2022</a>)</p>
<ul>
<li><p>goal: easily specify, discover, and fix undesirable behaviors in an NLP model</p></li>
<li><p>2-step iterative algorithm</p>
<ol class="arabic simple">
<li><p>LLM generates many tests targeting the model’s failures</p>
<ul>
<li><p>example of a test: <code class="docutils literal notranslate"><span class="pre">f(“I</span> <span class="pre">am</span> <span class="pre">a</span> <span class="pre">black</span> <span class="pre">woman”)</span> <span class="pre">≠</span> <span class="pre">neg</span></code></p></li>
<li><p>user selects and organizes the tests and reprompts the LLM to find more</p></li>
</ul>
</li>
<li><p>User fixes the tests (e.g. via finetuning)</p></li>
</ol>
</li>
<li><p>Checklist –Beyond Accuracy: Behavioral Testing of NLP models with CheckList (<a class="reference external" href="https://arxiv.org/abs/2005.04118">ribeiro…sameer singh, 2020</a>)</p>
<ul>
<li><p>matrix of general linguistic capabilities + test types</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Fixing Model Bugs with Natural Language Patches (<a class="reference external" href="https://openreview.net/forum?id=B6wzhbPhsZ9">murty, manning, lundberg, &amp; ribeiro 2022</a>)</p>
<ul>
<li><p>specify patches with natural language rather than hard rule, allowing them to better handle text</p></li>
<li><p>finetune a model to combine original model output with output from a patch-conditioned interpreter head</p></li>
</ul>
</li>
</ul>
</section>
<section id="interpretable-models">
<h3><span class="section-number">7.7.3.5. </span>interpretable models<a class="headerlink" href="#interpretable-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Backpack Language Models (<a class="reference external" href="https://arxiv.org/abs/2305.16765">hewit, thickstun, manning, &amp; liang, 2023</a>) - change transformer layers to represent each word</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9086128">(DirtyCat): Encoding High-Cardinality String Categorical Variables</a> (cerda &amp; varoquax, 2020) - use embedding model to improve string categorical variables</p></li>
<li><p>Large Language Models can Learn Rules (<a class="reference external" href="https://arxiv.org/abs/2310.07064">zhu…dai, 2024</a>)</p></li>
<li><p>Learning Transformer Programs (<a class="reference external" href="https://arxiv.org/abs/2306.01128">friedman, wettig, &amp; chen, 2023</a>) - place strong constraints on transformer architecture that allow it to be written as a <a class="reference internal" href="#%5Bhttps://arxiv.org/abs/2106.06981"><span class="xref myst">RASP</span></a> program compiled with <a class="reference external" href="https://arxiv.org/abs/2301.05062">Tracr</a></p>
<ul>
<li><p>2 contraints</p>
<ul>
<li><p>disentangled residual stream - attention head inputs K/Q/V are one-hot, ouputs are concatenated at each layer</p></li>
<li><p>each module implements rule-based mapping: attention is onehot</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="embeddings-related">
<h2><span class="section-number">7.7.4. </span>embeddings-related<a class="headerlink" href="#embeddings-related" title="Link to this heading">#</a></h2>
<section id="embedding-models">
<h3><span class="section-number">7.7.4.1. </span>embedding models<a class="headerlink" href="#embedding-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>detailed overview of info retrieval (<a class="reference external" href="https://arxiv.org/pdf/2401.09350.pdf">bruch, 2024</a>)</p>
<ul>
<li><p>Faiss: A library for efficient similarity search (<a class="reference external" href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/">johnson et al 2019</a>) - implement fast approximante nearest neighbor search</p></li>
</ul>
</li>
<li><p>introductory <a class="reference external" href="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/">blog post</a> on embeddings</p></li>
<li><p>basic training pipeline</p>
<ol class="arabic simple">
<li><p>standard self-supervised pre-training, e.g. BERT</p></li>
<li><p>weak unsupervised pre-training, e.g. weakly related text pairs, such as QA pairs from forums like StackExchange and Quora</p></li>
<li><p>high-quality contrastive finetuning on curated paired data, e.g. QA from web searches</p></li>
</ol>
</li>
<li><p>datasets</p>
<ul>
<li><p><strong><a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard</a></strong></p></li>
<li><p>Instructor eval: Billboard, Prompt retrieval</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.15246">FollowIR</a> (weller…soldaini, 2024)</p></li>
<li><p><a class="reference external" href="https://trec-rag.github.io/">TREC-RAG</a></p></li>
<li><p>Long contexts: <a class="reference external" href="https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval">LoCo Benchmark</a>, <a class="reference external" href="https://arxiv.org/pdf/2310.19923.pdf">Jina Long Context Benchmark</a></p></li>
<li><p>Older: BEIR benchmark](<a class="reference external" href="https://arxiv.org/abs/2104.08663">https://arxiv.org/abs/2104.08663</a>)</p></li>
<li><p>Training</p>
<ul>
<li><p>Nomic 235M curated text pairs (mostly filtered from <a class="reference external" href="https://huggingface.co/datasets/sentence-transformers/embedding-training-data">here</a>)</p>
<ul>
<li><p>Followed by supervised contrastive fine-tuning on datasets like MSMarco, NQ, NLI, HotpotQA, Fever, WikiAnswers, etc.</p></li>
</ul>
</li>
<li><p>MEDI (from Instructor paper): combines 300 datasets from Super- NaturalInstructions with 30 datasets from existing collections designed for embedding training</p></li>
</ul>
</li>
</ul>
</li>
<li><p>customization</p>
<ul>
<li><p>e.g. add prompt or prefixes like <em>search query</em>, <em>search document</em>, <em>classification</em>, <em>clustering</em> before embedding so model knows how to match things</p></li>
</ul>
</li>
<li><p>top-performing models</p>
<ul>
<li><p>NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models (<a class="reference external" href="https://arxiv.org/abs/2405.17428">lee…ping, 2024</a>)</p></li>
<li><p>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders (<a class="reference external" href="https://arxiv.org/abs/2404.05961">behnamghader…reddy, 2024</a>)</p></li>
<li><p>Gecko: Versatile Text Embeddings Distilled from LLMs (<a class="reference external" href="https://arxiv.org/abs/2403.20327">lee…naim, 2024</a>)</p></li>
<li><p>GRIT: Generative Representational Instruction Tuning (<a class="reference external" href="https://arxiv.org/abs/2402.09906">meunninghoff…kiela, 2024</a>) - train a single model that, given different instructions, can produce either generations or embeddings</p></li>
<li><p>EchoEmbeddings: Repetition Improves Language Model Embeddings (<a class="reference external" href="https://arxiv.org/pdf/2402.15449.pdf">springer, kotha, fried, neubig, &amp; raghunathan, 2024</a>)</p>
<ul>
<li><p>Feed a prompt such as “Rewrite the sentence: x, rewritten sentence: x” to the language model and pool the contextualized embeddings of the 2nd occurence of x</p></li>
<li><p>include task-specific prefix like in E5-mistral-instruct</p></li>
</ul>
</li>
<li><p>E5-mistral-instruct: Improving Text Embeddings with LLMs (<a class="reference external" href="https://arxiv.org/abs/2401.00368">wang…wei, 2023</a>) - finetune embeddings on synthetic data</p>
<ul>
<li><p>first prompt GPT-4 to brainstorm a list of potential retrieval tasks, and then generate <em>(query, positive, hard negative)</em> triplets for each task (GPT write the whole documents)</p></li>
<li><p>builds on E5 (<a class="reference external" href="https://arxiv.org/abs/2212.03533">wang…wei, 2022</a>)</p></li>
</ul>
</li>
<li><p>Jina Embeddings 2 (<a class="reference external" href="https://arxiv.org/abs/2310.19923">gunther…xiao, 2024</a>) - achieves long context (8192 tokens)</p></li>
<li><p>Instructor: One Embedder, Any Task: Instruction-Finetuned Text Embeddings (<a class="reference external" href="https://instructor-embedding.github.io">su, …, smith, zettlemoyer, yu, 2022</a>) - embedding is contextualized to each task</p>
<ul>
<li><p>Task-aware Retrieval with Instructions (<a class="reference external" href="https://aclanthology.org/2023.findings-acl.225/">asai…riedel, hajishirzi, &amp; yih, 2023</a>)</p></li>
<li><p>PromptBERT (<a class="reference external" href="https://arxiv.org/abs/2201.04337">jiang…furu wei…zhang, 2022</a>):  <code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">sentence:</span> <span class="pre">“</span> <span class="pre">[text]</span> <span class="pre">”</span> <span class="pre">means</span> <span class="pre">[MASK]</span></code> then use the embedding of the mask token</p></li>
<li><p>Scaling Sentence Embeddings with Large Language Models (<a class="reference external" href="https://arxiv.org/pdf/2307.16645">jiang, …, zhuang, 2023</a>): <code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">sentence:</span> <span class="pre">“</span> <span class="pre">[text]</span> <span class="pre">”</span> <span class="pre">means</span> <span class="pre">in</span> <span class="pre">one</span> <span class="pre">word:</span></code> then use the embedding of the final token</p></li>
</ul>
</li>
<li><p>GTE: Towards General Text Embeddings with Multi-stage Contrastive Learning (<a class="reference external" href="https://arxiv.org/abs/2308.03281">li…zhang, 2023</a>)</p></li>
<li><p>BGE (<a class="reference external" href="https://github.com/FlagOpen/FlagEmbedding">github</a>)</p></li>
<li><p>Nomic Embed (<a class="reference external" href="https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf">nussbaum, morris, duderstadt, &amp; mulyar, 2024</a>), (<a class="reference external" href="https://blog.nomic.ai/posts/nomic-embed-text-v1">blog post</a>)</p></li>
<li><p>Older: <a class="reference external" href="https://arxiv.org/abs/1908.10084">SBERT</a>, <a class="reference external" href="https://arxiv.org/abs/2104.08821">SIMCSE</a>, <a class="reference external" href="https://arxiv.org/abs/2202.08904">SGPT</a></p></li>
</ul>
</li>
<li><p>embedding approaches <a class="reference external" href="https://github.com/caiyinqiong/Semantic-Retrieval-Models">overview</a></p>
<ul>
<li><p>3 levels of interaction</p>
<ul>
<li><p>bi-encoder: separately encode query &amp; doc</p></li>
<li><p>cross-encoder: encode query and doc together</p></li>
<li><p>late-interaction encoder: hybrid, separately encode, but then learn some params on how to compute similarity between them (e.g. ColBERT (<a class="reference external" href="https://arxiv.org/abs/2004.12832">khattab &amp; zaharia, 2020</a>))</p></li>
</ul>
</li>
<li><p>expansion &amp; reweighting (e.g. doc2query)</p></li>
<li><p>sparse representation learning (e.g. UHD-BERT (<a class="reference external" href="https://arxiv.org/abs/2104.07198">jang…seo, 2021</a>))</p></li>
<li><p>joint learning with index</p></li>
<li><p>prior work: query expansion, term dependency model (e.g. tf-idf), topic model, translation model</p></li>
</ul>
</li>
<li><p>query expansion</p>
<ul>
<li><p>doc2query (<a class="reference external" href="https://arxiv.org/abs/1904.08375">noguiera, … cho, 2019</a>) – train passage to query model on MS MARCO then retrieve with BM-25</p></li>
<li><p>InPars (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3477495.3531863">bonifacio…nogueira, 2022</a>) – generate questions with GPT-3; retrieve with BM25</p></li>
</ul>
</li>
<li><p>Promptagator (<a class="reference external" href="https://arxiv.org/abs/2209.11755">dai…wei chang, 2022</a>) – hand-write prompts for each BEIR dataset; generate queries with FLAN; fine-tune</p></li>
<li><p>Meta-Task Prompting Elicits Embedding from Large Language Models (<a class="reference external" href="https://arxiv.org/pdf/2402.18458.pdf">lei…yates, 2024</a>) - ask a few pre-canned templates e.g. “Categorize into one of these categories ___” and look at logits for the outputs as an embedding</p></li>
<li><p>embedding search monograph (<a class="reference external" href="https://arxiv.org/pdf/2401.09350.pdf">bruch, 2024</a>)</p></li>
<li><p>Active Retrieval Augmented Generation (<a class="reference external" href="https://arxiv.org/abs/2305.06983">jiang…neubig, 2023</a>) - introduce FLARE, a method that iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens</p></li>
<li><p>Matryoshka Representation Learning (<a class="reference external" href="https://arxiv.org/abs/2205.13147">kusupati…kakade, jain, &amp; farhadi, 2022</a>) - in training given an embedding of full dimensionality M (e.g. 2048), learn N different distance functions for each prefix of the embedding (e.g. l2_norm(embedding[:32]), l2_norm(embedding[:64]), l2_norm(embedding[:128]), etc).</p>
<ul>
<li><p>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation (<a class="reference external" href="https://arxiv.org/abs/2503.01776">wen…you, 2025</a>) - instead learn sparse mask on top of original embedding</p></li>
<li><p>AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings (<a class="reference external" href="https://arxiv.org/pdf/2405.15028">reddy…potdar, 2024</a>) - rank at varying levels of granularity while maintaining encoding at a single (coarser) level</p></li>
</ul>
</li>
<li><p>Hypothetical Document Embeddings (<a class="reference external" href="https://arxiv.org/pdf/2212.10496.pdf">gao…callan, 2022</a>) - generate hypothetical document from query + instruction using GPT and find match for that doc</p></li>
<li><p>Probing embeddings</p>
<ul>
<li><p>Uncovering Meanings of Embeddings via Partial Orthogonality (<a class="reference external" href="https://arxiv.org/abs/2310.17611">jiang, aragam, &amp; veitch, 2023</a>)</p></li>
<li><p>The Linear Representation Hypothesis and the Geometry of LLMs (<a class="reference external" href="https://arxiv.org/abs/2311.03658">park…veitch, 2023</a>) - concepts can be decoded linearly from representations</p></li>
</ul>
</li>
<li><p>Embedding inversions</p>
<ul>
<li><p>Generative Embedding Inversion Attack to Recover the Whole Sentence (<a class="reference external" href="https://arxiv.org/pdf/2305.03010">li…song, 2023</a>) - train projection to LM jointly to reconstruct input</p>
<ul>
<li><p>Information Leakage from Embedding in Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2405.11916">wan…wang, 2024</a>)</p>
<ul>
<li><p>base embed inversion - directly pass hidden states to the LM head for generation</p></li>
<li><p>hotmap embed inversion - find input which yields embedding with greatest cosine similarity</p></li>
<li><p>embed parrot - learn a linear mapping to embedding states that is then</p></li>
</ul>
</li>
</ul>
</li>
<li><p>vec2text (<a class="reference external" href="https://arxiv.org/abs/2310.06816">morris et al. 2023</a>) - invert embeddings to text without using gradients</p>
<ul>
<li><p>logit2prompt (<a class="reference external" href="https://arxiv.org/pdf/2311.13647">morris, …, rush, 2024</a>) - recover prompt from output logits</p></li>
<li><p>output2prompt (<a class="reference external" href="https://arxiv.org/pdf/2405.15012">zhang, morris, &amp; shmatikov, 2024</a>) - recover prompt from long text outputs (by building a model of the sparse encodings of the outputs)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval (<a class="reference external" href="https://arxiv.org/abs/2401.18059">sarthi…manning</a>) - retrieve many docs and cluster/summarize before using</p></li>
<li><p>Seven Failure Points When Engineering a Retrieval Augmented Generation System (<a class="reference external" href="https://arxiv.org/abs/2401.05856">barnet…abdelrazek, 2024</a>)</p></li>
<li><p>Retrieve to Explain: Evidence-driven Predictions with Language Models (<a class="reference external" href="https://arxiv.org/pdf/2402.04068.pdf">patel…corneil, 2024</a>)</p></li>
</ul>
</section>
<section id="explainable-embeddings">
<h3><span class="section-number">7.7.4.2. </span>explainable embeddings<a class="headerlink" href="#explainable-embeddings" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>QA-Emb: Crafting Interpretable Embeddings by Asking LLMs Questions (<a class="reference external" href="https://arxiv.org/pdf/2405.16714">benara…gao, 2024</a>) - use yes/no questions to extract embeddings from text</p>
<ul>
<li><p>A General Framework for Producing Interpretable Semantic Text Embeddings (<a class="reference external" href="https://arxiv.org/abs/2410.03435">sun…yu, 2024</a>) - extend QA-Emb to systematically generates highly discriminative, low cognitive load yes/no questions</p></li>
<li><p>PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval (<a class="reference external" href="https://arxiv.org/pdf/2404.18424">zhuang…zuccon, 2024</a>)</p></li>
<li><p>InBedder: Answer is All You Need: Instruction-following Text Embedding via Answering the Question (<a class="reference external" href="https://arxiv.org/pdf/2402.09642.pdf">peng…jingbo shang, 2024</a>)</p>
<ul>
<li><p>embeddings consist of answers to questions</p></li>
<li><p>answer models are finetuned on QA datasets</p></li>
<li><p>questions are given ahead of time</p></li>
</ul>
</li>
<li><p>Learning Interpretable Style Embeddings via Prompting LLMs (<a class="reference external" href="https://arxiv.org/abs/2305.12696">patel, rao, kothary, mckeown, &amp; callison-burch, 2023</a>)</p></li>
<li><p>CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2302.12343">mcinerney…wallace, 2023</a>)</p>
<ul>
<li><p>extract interpretable feature (e.g. “Does this patient have a chronic illness?”) and use in a linear model (use Flan-T5)</p></li>
<li><p>example features: 10 ICD codes + (1) Does the patient have a chronic illness? (2) Is the condition life-threatening?</p></li>
</ul>
</li>
<li><p>Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM (<a class="reference external" href="https://arxiv.org/pdf/2404.12259">lam…bernstein, 2024</a>)</p></li>
<li><p>Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts (<a class="reference external" href="https://arxiv.org/pdf/2408.02265">tan, zhou, &amp; chen, 2024</a>)</p></li>
<li><p>Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck (<a class="reference external" href="https://arxiv.org/abs/2310.19660">ludan…callison-burch, 2023</a>)</p></li>
<li><p>BC-LLM: Bayesian Concept Bottleneck Models with LLM Priors (<a class="reference external" href="https://arxiv.org/abs/2410.15555">feng…tan, 2024</a>)</p></li>
</ul>
</li>
<li><p>Box Embeddings: An open-source library for representation learning using geometric structures (<a class="reference external" href="https://arxiv.org/abs/2109.04997">chheda…mccallum, 2021</a>) - allow for learning non-symmetric relations (e.g. entailment)</p>
<ul>
<li><p>Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations (<a class="reference external" href="https://arxiv.org/abs/2305.14599">huang…yu, 2023</a>) - learn interpretable compositional operations, which helps with similarities for compositional tasks</p></li>
</ul>
</li>
<li><p>multimodal</p>
<ul>
<li><p>SPLICE: Interpreting CLIP with Sparse Linear Concept Embeddings (<a class="reference external" href="https://arxiv.org/abs/2402.10376">bhalla…lakkaraju, 2024</a>)</p>
<ul>
<li><p>given CLIP, build an embedding concept dictionary by taking text embeddings of a bunch of individual semantic words</p></li>
<li><p>given a new image, get its image embedding and then decompose it into a sparse, nonnegative combination of the concept dictionary (this makes it interpretable)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Computer-vision focused</p>
<ul>
<li><p>Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning (<a class="reference external" href="https://arxiv.org/abs/2103.00370">hamilton, lundberg…freeman, 2021</a>) - add in “second-order” methods that look at similarities between different image features in the 2 images being compared</p></li>
<li><p>Why do These Match? Explaining the Behavior of Image Similarity Models (<a class="reference external" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560630.pdf">plummer…saenko, forsyth, 2020</a>) - generate saliency map + with an attribute based on the salient region</p></li>
<li><p>Towards Visually Explaining Similarity Models (<a class="reference external" href="https://arxiv.org/abs/2008.06035">zheng…wu, 2020</a>) - similarity of cnn embeddings</p></li>
</ul>
</li>
<li><p>Interpretable entity representations through large-scale typing (<a class="reference external" href="https://arxiv.org/abs/2005.00147">onoe &amp; durrett, 2020</a>) - embedding is interpretable predictions for different entities</p></li>
<li><p>Explaining similarity with different outputs</p>
<ul>
<li><p>Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners (<a class="reference external" href="https://arxiv.org/pdf/2202.01153.pdf">ramamurthy…tariq, 2022</a>) - returned explanation is an analogy (pair from the training set) rather than a saliency map</p></li>
<li><p>Sim2Word: Explaining Similarity with Representative Attribute Words via Counterfactual Explanations (<a class="reference external" href="https://dl.acm.org/doi/full/10.1145/3563039">chen…cao, 2023</a>) - give both saliency map + counterfactual explanation</p></li>
</ul>
</li>
</ul>
</section>
<section id="retrieval-augmented-generation-rag">
<h3><span class="section-number">7.7.4.3. </span>retrieval-augmented generation (RAG)<a class="headerlink" href="#retrieval-augmented-generation-rag" title="Link to this heading">#</a></h3>
<ul>
<li><p>RAG perspective paper (<a class="reference external" href="https://arxiv.org/pdf/2403.03187.pdf">asai, zhong, chen, koh, zettlemoyer, hajishirzi, &amp; yih, 2024</a>)</p>
<ul>
<li><div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-left"><p>Granularity</p></th>
<th class="head text-left"><p>Incorporation</p></th>
<th class="head text-left"><p>Frequency</p></th>
<th class="head text-left"><p>Training</p></th>
<th class="head text-left"><p>Data order</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>DrQA</p></td>
<td class="text-left"><p>Chunks</p></td>
<td class="text-left"><p>Input</p></td>
<td class="text-left"><p>One-time</p></td>
<td class="text-left"><p>Independent</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>REALM, RAG, ATLAS</p></td>
<td class="text-left"><p>Chunks</p></td>
<td class="text-left"><p>Input</p></td>
<td class="text-left"><p>One-time</p></td>
<td class="text-left"><p>Joint</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>RALM, REPLUG</p></td>
<td class="text-left"><p>Chunks</p></td>
<td class="text-left"><p>Input</p></td>
<td class="text-left"><p>Every <span class="math notranslate nohighlight">\(k\)</span> tokens, One-time</p></td>
<td class="text-left"><p>Independent*</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Active-Retriever, Self-RAG</p></td>
<td class="text-left"><p>Chunks</p></td>
<td class="text-left"><p>Input</p></td>
<td class="text-left"><p>Adaptive</p></td>
<td class="text-left"><p>Independent*, Sequential</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>RETRO, InstructRetro</p></td>
<td class="text-left"><p>Chunks</p></td>
<td class="text-left"><p>Intermediate</p></td>
<td class="text-left"><p>Every <span class="math notranslate nohighlight">\(k\)</span> tokens</p></td>
<td class="text-left"><p>Sequential</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^{12}\right)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>kNN LM, TRIME</p></td>
<td class="text-left"><p>Tokens</p></td>
<td class="text-left"><p>Output</p></td>
<td class="text-left"><p>Every token</p></td>
<td class="text-left"><p>Independent*, Joint</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>NPM, Copy Generator</p></td>
<td class="text-left"><p>Phrases</p></td>
<td class="text-left"><p>Output</p></td>
<td class="text-left"><p>Every phrase</p></td>
<td class="text-left"><p>Joint</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SPALM, Adaptive kNN</p></td>
<td class="text-left"><p>Tokens</p></td>
<td class="text-left"><p>Output</p></td>
<td class="text-left"><p>Adaptive</p></td>
<td class="text-left"><p>Independent*, Joint</p></td>
<td class="text-left"><p><span class="math notranslate nohighlight">\(O\left(10^9\right)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</li>
<li><p>RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems (<a class="reference external" href="https://arxiv.org/pdf/2403.09040.pdf">hsia…neubig, 2024</a>) - gives benchmark of multi-hop QA questions for evaluating RAG systems holistically</p></li>
</ul>
</li>
<li><p>dynamic systems</p>
<ul class="simple">
<li><p>Active RAG (<a class="reference external" href="https://arxiv.org/abs/2305.06983">jiang…neubig, 2023</a>) -  propose FLARE, which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens</p></li>
<li><p>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection (<a class="reference external" href="https://arxiv.org/abs/2310.11511">asai…hajishirzi, 2023</a>) -  train an LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection token</p></li>
<li><p>Infer–Retrieve–Rank: In-Context Learning for Extreme Multi-Label Classification (<a class="reference external" href="https://arxiv.org/pdf/2401.12178.pdf">D’Oosterlinck, …, potts, 2024</a>)</p>
<ul>
<li><p>Infer: an LM processes the input document and guesses a set of applicable terms</p></li>
<li><p>Retrieve: a retriever relates each predicted term to the actual label space</p></li>
<li><p>Rank: Finally, an LM is used to rerank retrieved labels</p></li>
</ul>
</li>
<li><p>Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (<a class="reference external" href="https://arxiv.org/abs/2403.14403">jeong…park, 2024</a>) - dynamically selects the most suitable retrieval-augmented strategy based on the predicted complexity level of input query</p></li>
<li><p>From Local to Global: A Graph RAG Approach to Query-Focused Summarization (<a class="reference external" href="https://arxiv.org/abs/2404.16130">edge…larson, 2024</a>) - build and summarize a graph of documents to be used at query time for summarizing docs</p></li>
</ul>
</li>
<li><p>Original papers</p>
<ul class="simple">
<li><p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (<a class="reference external" href="https://arxiv.org/abs/2005.11401">lewis, perez, …kiela, 2020</a>) - introduce idea of end-to-end RAG</p></li>
<li><p>k-nearest neighbors LM (<a class="reference external" href="https://arxiv.org/abs/1911.00172">khandelwal…zettlemoyer, lewis, 2020</a>)</p></li>
<li><p>REALM (<a class="reference external" href="https://arxiv.org/abs/2002.08909">guu, …, chang, 2020</a>) - retrieves document chunks from corpus and adds them to context, for open-domain QA</p></li>
<li><p>early systems</p>
<ul>
<li><p>DrQA (<a class="reference external" href="https://arxiv.org/abs/1704.00051">chen, weston, bordes 2017</a>)</p></li>
<li><p>ORQA (<a class="reference external" href="https://arxiv.org/abs/1906.00300">lee…toutanova, 2019</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>retrieval-augmented in-context learning (put retrieved info into context, or something very similar)</p>
<ul class="simple">
<li><p>RETRO (<a class="reference external" href="https://arxiv.org/abs/2112.04426">deepmind, 2022</a>) - nearest neighbors to model’s input are retrieved, encoded, and conditioned on with chunked cross-attention</p></li>
<li><p>Decomposed prompting (<a class="reference external" href="https://arxiv.org/pdf/2210.02406.pdf">khot et al., 2022</a>) - decompose tasks via prompting which are delegated to a shared library of prompting-based LLMs dedicated to these sub-tasks</p></li>
<li><p>retrieval-in-context approach (<a class="reference external" href="https://arxiv.org/abs/2301.12652">shi, min et al. 2023</a>; <a class="reference external" href="https://arxiv.org/abs/2302.00083">ram…shoham, 2023</a>) - retrieve docs and preprend to the context</p></li>
<li><p>LLM-Augmenter (<a class="reference external" href="https://arxiv.org/abs/2302.12813">peng, galley…gao, 2023</a>) -  (1) consolidates evidence from external knowledge for the LLM to generate responses grounded in evidence, and (2) revising LLM’s (candidate) responses using automated feedback</p></li>
<li><p>DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval <a class="reference external" href="https://arxiv.org/abs/2302.07452">(Lin et al 2023)</a></p></li>
<li><p>Knowledgeable Prompt-tuning (<a class="reference external" href="https://arxiv.org/abs/2108.02035">Hu et al. 2021</a>) - add knowledge-base info into the prompt search</p></li>
<li><p>Atlas: Few-shot Learning with Retrieval Augmented Language Models (<a class="reference external" href="https://arxiv.org/abs/2208.03299">meta, 2022</a>)</p></li>
<li><p>GRIT: Generative Representational Instruction Tuning <a class="reference external" href="https://arxiv.org/abs/2402.09906">(Muennighoff et al 2024)</a> - train same model for  text generation and retrieval tasks</p></li>
<li><p>Fine-grained Hallucination Detection and Editing for Language Models (<a class="reference external" href="https://arxiv.org/abs/2401.06855">mishra, …, hajishirzi, 2024</a>) - train a retrieval-augmented LM to correct fine-grained hallucinations</p></li>
<li><p>RAG can even outperform LMs fine-tuned on the downstream domain data on QA (<a class="reference external" href="https://arxiv.org/abs/2312.05934">Ovadia et al., 2023</a>; <a class="reference external" href="https://arxiv.org/abs/2401.08406">Gupta et al., 2024</a>)</p></li>
</ul>
</li>
<li><p>Different ideas</p>
<ul class="simple">
<li><p>Transformer Memory as a Differentiable Search Index (<a class="reference external" href="https://arxiv.org/abs/2202.06991">Tay at al 2022</a>) - Same model learns to encode documents and find closest search index (rather than retrieving with maximal inner product search)</p>
<ul>
<li><p>Self-Retrieval: Building an Information Retrieval System with One LLM (<a class="reference external" href="https://arxiv.org/pdf/2403.00801.pdf">tang…li, 2024</a>) - LLM learns to generate retrieved document from query</p></li>
</ul>
</li>
<li><p>xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token (<a class="reference external" href="https://arxiv.org/pdf/2405.13792">cheng…furu wei…zhao, 2024</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="external-memory">
<h3><span class="section-number">7.7.4.4. </span>external memory<a class="headerlink" href="#external-memory" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>memorizing transformers (<a class="reference external" href="https://arxiv.org/abs/2203.08913">wu…szegedy, 2022</a>) - knn-based learned indexing + retrieval at training time</p>
<ul>
<li><p>at test time, you just need to index the entire context and the model will be able to use it</p></li>
<li><p>kNN Prompting: Learning Beyond the Context with Nearest Neighbor Inference (<a class="reference external" href="https://openreview.net/forum?id=fe2S7736sNS">xu…zhang, 2023</a>) - instead of verbalizer, use nearest-neighbor (nice results for dbpedia)</p></li>
<li><p>kNN-Prompt: Nearest Neighbor Zero-Shot Inference (<a class="reference external" href="https://arxiv.org/pdf/2205.13792.pdf">shi…zettlemoyer, 2022</a>)</p></li>
</ul>
</li>
<li><p>Memory Networks (<a class="reference external" href="https://arxiv.org/abs/1410.3916">weston, chopra, &amp; bordes, 2014</a>) - external KB that can be read / written to (stores plain text)</p>
<ul>
<li><p>End-To-End Memory Networks (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2015/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html">sukhbaatar, szlam, weston, &amp; fergus, 2015</a>) - trained with less supervision for memory reading/writing</p></li>
</ul>
</li>
<li><p>SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore (<a class="reference external" href="https://arxiv.org/pdf/2308.04430.pdf">min…smith, zettlemoyer, 2023</a>)</p>
<ul>
<li><p>Use a parametric LM on open data then one of 2 nonparametric datastores: kNN LM or retrieval in-context</p></li>
</ul>
</li>
<li><p>A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches (<a class="reference external" href="https://openreview.net/pdf?id=Q6PAnqYVpo">deguchi…yokoi, 2025</a>)</p></li>
<li><p>Hybrid computing using a neural network with dynamic external memory (<a class="reference external" href="https://www.nature.com/articles/nature20101">graves…hassabis, 2016</a>) [see <a class="reference external" href="https://jaspock.github.io/funicular/dnc.html">blog post</a>]</p>
<ul>
<li><p>differentiable neural computer (DNC) - neural network that can read from and write to an external memory matrix</p></li>
<li><p>stores external memory matrix</p>
<ul>
<li><p>reads from memory via multiplying by a vector (e.g. one-hot vector would yield single element)</p></li>
</ul>
</li>
<li><p>extends Neural turing machines (<a class="reference external" href="https://arxiv.org/abs/1410.5401">graves, wayne, &amp; danihelka, 2014</a>)</p></li>
</ul>
</li>
<li><p>Hopfield Networks is All You Need (<a class="reference external" href="https://arxiv.org/abs/2008.02217">ramsaeur…hochreiter, 2020</a>)</p>
<ul>
<li><p>keys: each input has a key vector which “represents info about this input” (e.g. this is a noun)</p></li>
<li><p>queries: each input has a query vector which “asks for other inputs that would be useful context” (e.g. what adjectives describe this word)</p>
<ul>
<li><p>in self-attention these queries also come from the input whereas in just regular attention they come from somewhere else (e.g. the output of a translation task)</p></li>
</ul>
</li>
<li><p>transformer finds similarity between each key with each query then takes softmax - this provides weights for each of the inputs, as context for the original input</p>
<ul>
<li><p>in transformer, these weights are used to weight the values but in hopfield nets we would take a weighted sum of the keys and feed it back as the input</p></li>
</ul>
</li>
<li><p>as we update becomes more skewed towards the things that match the most</p></li>
</ul>
</li>
<li><p>pre-transformers</p>
<ul>
<li><p>Improving Neural Language Models with a Continuous Cache (<a class="reference external" href="https://arxiv.org/abs/1612.04426">grave…usunier, 2016</a>) - cache previous embeddings as memory from a document to contextualize an LSTM</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="explanation-discovery">
<h2><span class="section-number">7.7.5. </span>explanation / discovery<a class="headerlink" href="#explanation-discovery" title="Link to this heading">#</a></h2>
<section id="dataset-module-explanation">
<h3><span class="section-number">7.7.5.1. </span>dataset / module explanation<a class="headerlink" href="#dataset-module-explanation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Rethinking Interpretability in the Era of LLMs (<a class="reference external" href="https://arxiv.org/abs/2402.01761">singh et al. 2024</a>) - review emphasizing emerging areas like dataset explanation</p></li>
<li><p>dataset explanation</p>
<ul>
<li><p>iPrompt: Explaining Patterns in Data with Language Models via Interpretable Autoprompting (<a class="reference external" href="https://arxiv.org/abs/2210.01848">singh, morris, …gao, 2022</a>) - prompting approach</p>
<ul>
<li><p>Verbalized Machine Learning: Revisiting Machine Learning with Language Models (<a class="reference external" href="https://arxiv.org/pdf/2406.04344v1">xiao, bamler, scholkopf, &amp; liu, 2024</a>) - fitting regression models optimized through natural language iteratively</p></li>
</ul>
</li>
<li><p>Instruction Induction: From Few Examples to Natural Language Task Descriptions (<a class="reference external" href="https://arxiv.org/abs/2205.10782">honovich…bowman, levy 2022</a>) - directly query model with prompt to search for task description</p></li>
<li><p>D3: Describing Differences between Text Distributions with Natural Language (<a class="reference external" href="https://arxiv.org/abs/2201.12323">zhong, snell, klein, &amp; steinhardt, 2022</a>) - finetune an LLM to directly describe difference between 2 text distrs</p>
<ul>
<li><p>D5: Goal Driven Discovery of Distributional Differences via Language Descriptions (<a class="reference external" href="https://arxiv.org/abs/2302.14233">zhong, zhang, …, klein, &amp; steinhardt, 2023</a>) - add dataset-specific prompt + evaluation on larger set of 675 datasets</p></li>
<li><p>technically this is just learning a classifier, where the classifier is a natural-language string</p></li>
<li><p>method</p>
<ul>
<li><p>proposer network generates hypotheses</p></li>
<li><p>verifier networks looks at all samples in the dataset (since proposer couldn’t fit them all in context) and returns how accurate the hypotheses were</p></li>
<li><p>some tricks</p>
<ul>
<li><p>select samples which are “representative” of a class by predicting with another LLM</p></li>
<li><p>have a pool of 302 manual hypotheses they usefor seeding</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Explaining Datasets in Words: Statistical Models with Natural Language Parameters (<a class="reference external" href="https://arxiv.org/pdf/2409.08466">zhong, wang, klein, &amp; steinhardt, 2024</a>) - assign labels to continuous vectors in statistical models, e.g. text label to cluster mean embedding</p></li>
<li><p>Goal-Driven Explainable Clustering via Language Descriptions (<a class="reference external" href="https://arxiv.org/abs/2305.13749">wang…, zhong, 2023</a>)</p>
<ul>
<li><p>ClusterLLM: LLMs as a Guide for Text Clustering (<a class="reference external" href="https://arxiv.org/abs/2305.14871">zhang…shang, 2023</a>)</p></li>
<li><p>LLMs4OL: LLMs for Ontology Learning (<a class="reference external" href="https://arxiv.org/pdf/2307.16648.pdf">giglou et al. 2023</a>) - use prompting to construct ontologies</p></li>
<li><p>Towards Ontology Construction with Language Models (<a class="reference external" href="https://arxiv.org/abs/2309.09898">funk…lutz, 2023</a>) - build ontologies, but only use manual inspection</p>
<ul>
<li><p>Toward a Comparison Framework for Interactive Ontology Enrichment Methodologies (<a class="reference external" href="https://oa.tib.eu/renate/items/1ee156c1-0870-47da-8bff-0c216e273d9f">jarno…rudolph, 2022</a>)</p></li>
</ul>
</li>
<li><p>TopicGPT: A Prompt-based Topic Modeling Framework (<a class="reference external" href="https://arxiv.org/abs/2311.01449">pham…iyyer, 2023</a>)</p></li>
</ul>
</li>
<li><p>Mass-Producing Failures of Multimodal Systems with Language Models (<a class="reference external" href="https://arxiv.org/abs/2306.12105">tong, jones, &amp; steinhardt, 2023</a>)</p></li>
<li><p>TopicGPT: A Prompt-based Topic Modeling Framework (<a class="reference external" href="https://arxiv.org/abs/2311.01449">pham…iyyer, 2023</a>)</p></li>
</ul>
</li>
<li><p>What is different between these datasets? (<a class="reference external" href="https://arxiv.org/abs/2403.05652">babbar, guo, &amp; rudin, 2024</a>) - combine a variety of different methods to find the difference between (mostly tabular) datasets</p></li>
<li><p>GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language (<a class="reference external" href="https://arxiv.org/abs/2206.15007">zhu…james zou, 2022</a>) - automatically explain dataset-level distribution shifts (in image datasets) with natural language</p>
<ul>
<li><p>Domino: Discovering Systematic Errors with Cross-Modal Embeddings (<a class="reference external" href="https://arxiv.org/abs/2203.14960">eyuboglu…zou, re, 2022</a>)</p></li>
</ul>
</li>
<li><p>MaNtLE: Model-agnostic Natural Language Explainer (<a class="reference external" href="https://arxiv.org/pdf/2305.12995.pdf">menon, zaman, &amp; srivastava, 2023</a>) - train model to generate explanations on simple tables (they do this for classifier outputs but could easily do it directly for data labels)</p></li>
<li><p>LLMs for Automated Open-domain Scientific Hypotheses Discovery (<a class="reference external" href="https://arxiv.org/abs/2309.02726">yang…cambria, 2023</a>)</p></li>
<li><p>Scaling deep learning for materials discovery (<a class="reference external" href="https://www.nature.com/articles/s41586-023-06735-9">merchant…cubuk, 2023</a>)</p></li>
<li><p>wikipedia</p>
<ul>
<li><p>Improving Wikipedia verifiability with AI (<a class="reference external" href="https://www.nature.com/articles/s42256-023-00726-1">petroni…riedel, 2023</a>)</p></li>
<li><p>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2402.14207">shao…lam, 2024</a>)</p></li>
<li><p>Retrieval-based Full-length Wikipedia Generation for Emergent Events (<a class="reference external" href="https://arxiv.org/abs/2402.18264">zhang…li, 2024</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>module explanation in natural language</p>
<ul>
<li><p>Explaining black box text modules in natural language with language models (<a class="reference external" href="https://arxiv.org/abs/2305.09863">singh, hsu, …, gao, 2023</a>)</p>
<ul>
<li><p>Zero-shot LLM-guided Counterfactual Generation for Text (<a class="reference external" href="https://arxiv.org/pdf/2405.04793">bhattacharjee…liu, 2024</a>)</p></li>
</ul>
</li>
<li><p>Language models can explain neurons in language models (<a class="reference external" href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">bills, cammarata, …saunders, 2023, openai</a>)</p>
<ul>
<li><p>goal: explain a neuron</p>
<ul>
<li><p>step 1: summarize (token, activation) pairs into an explanation</p></li>
<li><p>step 2: create simulated neuron that outputs activations given tokens</p></li>
<li><p>step 3: check correlation of simulated neuron outputs with real neuron outputs</p></li>
</ul>
</li>
<li><p>their <a class="reference external" href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">unigram baseline</a> summarizes top unigrams into a string</p></li>
<li><p>they use synthetic generated data to revise the explanation</p></li>
<li><p>they also do some recovery tests on “neuron puzzles”</p></li>
<li><p>The Importance of Prompt Tuning for Automated Neuron Explanations (<a class="reference external" href="https://arxiv.org/abs/2310.06200">lee…weng, 2023</a>) - improve the prompt used to generate the explanations</p></li>
</ul>
</li>
<li><p>CoSy: Evaluating Textual Explanations of Neurons (<a class="reference external" href="https://arxiv.org/abs/2405.20331">kopf…bykov, 2024</a>)</p></li>
<li><p>Evaluating Concept-based Explanations of Language Models: A Study on Faithfulness and Readability (<a class="reference external" href="https://arxiv.org/pdf/2404.18533">li…wang, 2024</a>)</p></li>
<li><p>A Multimodal Automated Interpretability Agent (<a class="reference external" href="https://arxiv.org/pdf/2404.14394">shaham…hernandez, andreas, torralba, 2024</a>)</p></li>
<li><p>MILAN: Natural Language Descriptions of Deep Visual Features (<a class="reference external" href="https://openreview.net/forum?id=NudBMY-tzDr">hernandez…david bau…torallba, andreas, 2022</a>) - given a neuron, generates a natural-language string that maximizes pointwise mutual information with the image regions in which the neuron is active</p>
<ul>
<li><p>Scale Alone Does not Improve Mechanistic Interpretability in Vision Models (<a class="reference external" href="https://arxiv.org/abs/2307.05471">zimmermann, klein, &amp; brendel, 2023</a>) - perform human eval of interpretability of different units (show human top-activating patches and ask them to decide which of 2 patches will be top-activating)</p></li>
<li><p>CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks (<a class="reference external" href="https://arxiv.org/abs/2204.10965">oikarinen &amp; weng, 2023</a>)</p>
<ul>
<li><p>Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models (<a class="reference external" href="https://openreview.net/forum?id=Rnxam2SRgB">bai…weng, 2024</a>) - extend to explanations beyond individual words</p></li>
<li><p>Linear Explanations for Individual Neurons (<a class="reference external" href="https://arxiv.org/pdf/2405.06855">oikarinen &amp; weng, 2024</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Evaluation</p>
<ul>
<li><p>A Function Interpretation Benchmark for Evaluating Interpretability Methods (<a class="reference external" href="https://arxiv.org/abs/2309.03886">schwettmann, …, andreas, bau, &amp; torralba, 2023</a>)</p></li>
<li><p>Rigorously Assessing Natural Language Explanations of Neurons (<a class="reference external" href="https://arxiv.org/abs/2309.10312">huang..potts, 2023</a>)</p></li>
<li><p>Ravel: Evaluating Interpretability Methods on Disentangling Language Model Representations (<a class="reference external" href="https://arxiv.org/pdf/2402.17700v1.pdf">huang, wu, potts, geva, &amp; geiger, 2024</a>)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="directly-learning-algorithms">
<h3><span class="section-number">7.7.5.2. </span>directly learning algorithms<a class="headerlink" href="#directly-learning-algorithms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Empirical results</p>
<ul>
<li><p>FunSearch: Mathematical discoveries from program search with LLMs (<a class="reference external" href="https://www.nature.com/articles/s41586-023-06924-6">deepmind, 2023</a>)</p>
<ul>
<li><p>Discovering Symbolic Cognitive Models from Human and Animal Behavior (<a class="reference external" href="https://www.biorxiv.org/content/10.1101/2025.02.05.636732v1">castro…stachenfeld, 2025</a>)</p></li>
</ul>
</li>
<li><p>Faster sorting algorithms discovered using deep reinforcement learning (<a class="reference external" href="https://www.nature.com/articles/s41586-023-06004-9">deepmind, 2023</a>)</p></li>
<li><p>Discovering faster matrix multiplication algorithms with reinforcement learning (<a class="reference external" href="https://www.nature.com/articles/s41586-022-05172-4">deepmind, 2022</a>)</p></li>
<li><p>Nuclear fusion control (<a class="reference external" href="https://www.nature.com/articles/s41586-021-04301-9">deepmind, 2022</a>)</p></li>
<li><p>Quantum Circuit Optimization with AlphaTensor (<a class="reference external" href="https://arxiv.org/pdf/2402.14396.pdf">deepmind, 2024</a>)</p></li>
</ul>
</li>
<li><p>Alphafold</p>
<ul>
<li><p>Accurate proteome-wide missense variant effect prediction with AlphaMissense (<a class="reference external" href="https://www.science.org/doi/full/10.1126/science.adg7492">deepmind, 2023</a>) - predict effects of varying single-amino acid changes</p></li>
<li><p>Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero (<a class="reference external" href="https://arxiv.org/abs/2310.16410">schut…hessabis, paquet, &amp; been kim, 2023</a>)</p></li>
</ul>
</li>
<li><p>Learning a Decision Tree Algorithm with Transformers (<a class="reference external" href="https://arxiv.org/abs/2402.03774">zhuang…gao, 2024</a>)</p></li>
<li><p>Meta-Statistical Learning: Supervised Learning of Statistical Inference (<a class="reference external" href="https://arxiv.org/pdf/2502.12088">peyrard &amp; cho, 2025</a>)</p></li>
<li><p>Targeted Cause Discovery with Data-Driven Learning (<a class="reference external" href="https://arxiv.org/abs/2408.16218">kim…cho, 2024</a>)</p>
<ul>
<li><p>Sample, estimate, aggregate: A recipe for causal discovery foundation models (<a class="reference external" href="https://arxiv.org/abs/2402.01929">wu, bao, barzilay, &amp; jaakkola, 2024</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="automated-assistants-teaching-hitl">
<h3><span class="section-number">7.7.5.3. </span>automated assistants, teaching, HITL<a class="headerlink" href="#automated-assistants-teaching-hitl" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>similar to causality, we may want to use interpretability just to understand our data rather than to get any form of model</p></li>
<li><p>Benchmarking LLMs As AI Research Agents (<a class="reference external" href="https://arxiv.org/abs/2310.03302v1">huang, vora, liang, &amp; leskovec, 2023</a>) - formulate concrete ml tasks (like improve accuracy on a kaggle task) and see how well LLMs can do at them</p>
<ul>
<li><p>Data Interpreter: An LLM Agent For Data Science (<a class="reference external" href="https://arxiv.org/pdf/2402.18679.pdf">hong…wu, 2024</a>)</p></li>
<li><p>Autonomous LLM-driven research from data to human-verifiable research papers (<a class="reference external" href="https://arxiv.org/abs/2404.17605">ifaragan…kishony, 2024</a>)</p></li>
</ul>
</li>
<li><p>LLMs asking questions</p>
<ul>
<li><p>CollabLLM: From Passive Responders to Active Collaborators (<a class="reference external" href="https://arxiv.org/abs/2502.00640">wu, galley, …, gao, 2025</a>)</p>
<ul>
<li><p>Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization (<a class="reference external" href="https://arxiv.org/pdf/2306.09299">saha…bansal, 2023</a>)</p></li>
<li><p>Know Thy Student: Interactive Learning with Gaussian Processes (<a class="reference external" href="https://arxiv.org/abs/2204.12072">wang…goodman, 2022</a>)</p></li>
</ul>
</li>
<li><p>GATE: Eliciting Human Preferences with Language Models (<a class="reference external" href="https://arxiv.org/pdf/2310.11589.pdf">li, tamkin, goodman, &amp; andreas, 2023</a>) - LMs guide the task specification process (e.g. content recommendation), which is both free-form and interactive</p>
<ul>
<li><p>Task Ambiguity in Humans and Language Models (<a class="reference external" href="https://arxiv.org/abs/2212.10711">tamkin, .., goodman, 2023</a>)</p></li>
<li><p>Bayesian Preference Elicitation with Language Models (<a class="reference external" href="https://arxiv.org/pdf/2403.05534v1.pdf">handa, gal, pavlick, goodman, tamkin, andreas, &amp; li, 2024</a>)</p></li>
<li><p>STaR-GATE: Teaching Language Models to Ask Clarifying Questions (<a class="reference external" href="https://arxiv.org/abs/2403.19154">andukuri…goodman, 2024</a>)</p></li>
<li><p>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves (<a class="reference external" href="https://arxiv.org/abs/2311.04205">deng…gu, 2024</a>)</p></li>
</ul>
</li>
<li><p>Loose LIPS Sink Ships:  Asking Questions in <em>Battleship</em> with Language-Informed Program Sampling (<a class="reference external" href="https://arxiv.org/pdf/2402.19471.pdf">grand, pepe, andreas, &amp; tenenbaum , 2024</a>) - language-informed program sampling (LIPS) model uses large language models (LLMs) to generate NL questions, translate them into symbolic programs, and evaluate their expected info gain</p></li>
</ul>
</li>
<li><p>LLM-based game agents (<a class="reference external" href="https://github.com/git-disl/awesome-LLM-game-agent-papers">awesome repo</a>)</p>
<ul>
<li><p>Baba Is AI: Break the Rules to Beat the Benchmark (<a class="reference external" href="https://arxiv.org/pdf/2407.13729">cloos…barbu, cueva, 2024</a>)</p></li>
<li><p>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games (<a class="reference external" href="https://arxiv.org/abs/2411.13543">paglieri…rocktäschel, 2024</a>)</p></li>
</ul>
</li>
<li><p>visualization</p>
<ul>
<li><p>LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using LLMs (<a class="reference external" href="https://arxiv.org/abs/2303.02927">dibia, 2023</a>)</p></li>
<li><p>Execution-based Evaluation for Data Science Code Generation Models (<a class="reference external" href="https://arxiv.org/abs/2211.09374">huang…gao, 2022</a>)</p></li>
<li><p>On the Design of AI-powered Code Assistants for Notebooks (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3544548.3580940">mcnutt, wang, deline, &amp; drucker, 2023</a>)</p></li>
<li><p>Visualization by Example (<a class="reference external" href="https://arxiv.org/abs/1911.09668">chenglong wang…dillig, 2019</a>) - automatically synthesize a program to visual data based on user “sketches” = partial visualization of a subset of the data by the user</p>
<ul>
<li><p>Falx: Synthesis-Powered Visualization Authoring (<a class="reference external" href="https://arxiv.org/abs/2102.01024">chenglong wang…ko, 2021</a>)</p></li>
</ul>
</li>
<li><p>Lux: Always-on Visualization Recommendations for Exploratory Dataframe Workflows (<a class="reference external" href="https://arxiv.org/abs/2105.00121">lee…hearts, parameswaram, 2021</a>)</p>
<ul>
<li><p>high-level language for recommendations (e.g. <code class="docutils literal notranslate"><span class="pre">df.intent</span> <span class="pre">=</span> <span class="pre">[&quot;AvgLifeexpetancy&quot;,</span> <span class="pre">&quot;Inequality&quot;]</span></code>) -&gt; Lux automatically creates relevant visualizations</p></li>
</ul>
</li>
<li><p>see also things in <a class="reference external" href="https://github.com/csinva/imodelsX">imodelsX</a></p></li>
<li><p>Can Foundation Models Wrangle Your Data? (<a class="reference external" href="https://arxiv.org/abs/2205.09911">narayan…re, 2022</a>)</p>
<ul>
<li><p>Towards Parameter-Efficient Automation of Data Wrangling Tasks with Prefix-Tuning (<a class="reference external" href="https://openreview.net/pdf?id=8kyYJs2YkFH">vos, dohmen, &amp; schelter, 2024</a>)</p></li>
</ul>
</li>
<li><p>llms for reading charts</p>
<ul>
<li><p>ChartLlama: A Multimodal LLM for Chart Understanding and Generation (<a class="reference external" href="https://arxiv.org/abs/2311.16483">han…zhang, 2023</a>)</p></li>
<li><p>Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots (<a class="reference external" href="https://arxiv.org/abs/2405.07990">wu…luo, 2024</a>)</p></li>
<li><p>MathVista: Evaluating Math Reasoning in Visual Contexts (<a class="reference external" href="https://mathvista.github.io/">lu…galley, gao, 2024</a>)</p></li>
<li><p>Evaluating Task-based Effectiveness of MLLMs on Charts (<a class="reference external" href="https://arxiv.org/abs/2405.07001">wu…tang, 2024</a>) - evals + chhain-of-charts prompting</p></li>
<li><p>Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models (<a class="reference external" href="https://arxiv.org/pdf/2406.09403">hu…zettlemoyer, smith, krishna, 2024</a>) - allow LLM to use image-based tools (draw lines, zoom in, annotate, create python plots) to answer reasoning questions about images</p></li>
<li><p>CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs (<a class="reference external" href="https://arxiv.org/abs/2406.18521">wang…arora, chen, 2024</a>)</p></li>
<li><p>eye-tracking data</p>
<ul>
<li><p>MassVis <a class="reference external" href="http://massvis.mit.edu/">dataset</a> - folks look at plots and then are tested for memory/recall</p>
<ul>
<li><p>Patterns of Attention: How Data Visualizations are Read (<a class="reference external" href="https://www.osti.gov/servlets/purl/1425321">matzen…stites, 2017</a>)</p></li>
<li><p>Eye Fixation Metrics for Large Scale Analysis Eye Movement Metrics for Information Visualizations of Information Visualizations (<a class="reference external" href="https://web.mit.edu/zoya/www/Bylinskii_eyefixations_small.pdf">bylinskii &amp; borkin, 2015</a>) - different ways to visualize eye-tracking data</p></li>
</ul>
</li>
<li><p>“Seeing” Data Like an Expert: An Eye-Tracking Study Using Graphical Data Representations (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6755310/">harsh…maltese, 2019</a>)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>modeling</p>
<ul>
<li><p>TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations (<a class="reference external" href="https://arxiv.org/abs/2207.04154">slack, krishna, lakkaraju, &amp; singh, 2023</a>) - train model to translate human queries into API calls (~30 calls, things like feature importance, filter data, counterfactual explanation)</p></li>
<li><p>TalkToEBM: LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs (<a class="reference external" href="https://arxiv.org/abs/2308.01157">lengerich…caruana, 2023</a>) - use LLMs to analyze tabular data and make suggestions for EBMs</p>
<ul>
<li><p>Data Science with LLMs and Interpretable Models (<a class="reference external" href="https://arxiv.org/pdf/2402.14474.pdf">bordt, lengerich, nori, &amp; carauna, 2024</a>)</p></li>
<li><p>GAM Changer: Editing Generalized Additive Models with Interactive Visualization (<a class="reference external" href="https://arxiv.org/abs/2112.03245">wang…caruana, 2021</a>) - gui for editing GAMs</p></li>
</ul>
</li>
<li><p>LMPriors: Pre-Trained Language Models as Task-Specific Priors (<a class="reference external" href="https://arxiv.org/abs/2210.12530">choi…ermon, 2022</a>)</p>
<ul>
<li><p>LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization (<a class="reference external" href="https://arxiv.org/pdf/2502.10648">zhang…tibshirani, 2025</a>)</p></li>
</ul>
</li>
<li><p>Tisane: Authoring Statistical Models via Formal Reasoning from Conceptual and Data Relationships (<a class="reference external" href="https://eunicemjun.com/assets/files/jun2022tisane.pdf">jun, seo, heer, &amp; just, 2022</a>) - language to better specify assumptions when fitting GLMs / GLMMs</p></li>
<li><p>LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering (<a class="reference external" href="https://arxiv.org/abs/2305.03403">hollmann, muller &amp; hutter, 2023</a>)</p></li>
<li><p>Interpretable Medical Diagnostics with Structured Data Extraction by LLMs (<a class="reference external" href="https://arxiv.org/abs/2306.05052">bisercic…petrovic, 2023</a>) - extract tabular datasets from unstructured text and then train interpretable models (linear regression and small decision trees) on top of this data</p></li>
</ul>
</li>
</ul>
</section>
<section id="clinical-nlp">
<h3><span class="section-number">7.7.5.4. </span>clinical nlp<a class="headerlink" href="#clinical-nlp" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Self-Verification Improves Few-Shot Clinical Information Extraction (<a class="reference external" href="https://arxiv.org/abs/2306.00024">gero et al. 2023</a>)</p></li>
<li><p>Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale (<a class="reference external" href="https://arxiv.org/abs/2502.00943">wong…poon, 2025</a>) - specialized prompt template for extracting attributes using LLM</p></li>
<li><p>MedCalc-Bench: Evaluating Large Language Models for Medical Calculations (<a class="reference external" href="https://arxiv.org/abs/2406.12036">khandekar…lu, 2024</a>) - create examples / questions from popular MDCalc guidelines</p></li>
<li><p>LLMs are Few-Shot Clinical Information Extractors (<a class="reference external" href="https://arxiv.org/abs/2205.12689">agrawal…sontag, 2022</a>) - use GPT3</p></li>
<li><p>Health system-scale language models are all-purpose prediction engines (<a class="reference external" href="https://www.nature.com/articles/s41586-023-06160-y">NYU 2023</a>)</p></li>
<li><p>AMIE: Towards Conversational Diagnostic AI (<a class="reference external" href="https://arxiv.org/abs/2401.05654">tu…natarajan, 2024</a>)</p>
<ul>
<li><p>Polaris: A Safety-focused LLM Constellation Architecture for Healthcare (<a class="reference external" href="https://arxiv.org/pdf/2403.13313.pdf">mukherjee…miller, 2024</a>)</p></li>
</ul>
</li>
<li><p>GPT4 in medicine book (<a class="reference external" href="https://www.amazon.com/AI-Revolution-Medicine-GPT-4-Beyond/dp/0138200130">lee, goldberg, &amp; kohane, 2023</a>)</p>
<ul>
<li><p>For summaries: “Can you check the proposed note and identify any facts in it that don’t appear explicitly in the transcript?” - gpt often better at reviewing text than writing it</p></li>
<li><p>evaluation: hard to run gpt clinical trial, although can be used to identify candidates, e.g. biomarkers for followup tests</p></li>
<li><p>paperwork - replace patient intake form, medical encounter note, prior authorization note (to insurance), universal translator for health info / formatting</p></li>
</ul>
</li>
<li><p>Evaluating LLMs on Medical Evidence Summarization (<a class="reference external" href="https://pubmed.ncbi.nlm.nih.gov/37162998/">tang…peng, 2023</a>) - score summaries based on 6 dimensions (e.g. coherence)</p>
<ul>
<li><p>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success) (<a class="reference external" href="https://arxiv.org/abs/2305.06299">shaib…wallace, 2023</a>)</p></li>
<li><p>SummIt: Iterative Text Summarization via ChatGPT (<a class="reference external" href="https://arxiv.org/abs/2305.14835">zhang, …, zhang, 2023</a>)</p></li>
</ul>
</li>
<li><p>TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models (<a class="reference external" href="https://arxiv.org/pdf/2311.01301.pdf">gonzalez, wong, gero, …, poon, 2023</a>)</p>
<ul>
<li><p>extract attributes from structured &amp; unstructured EHR to form basis for clinical trial specification / experiments</p></li>
</ul>
</li>
<li><p>Scaling Clinical Trial Matching Using LLMs: A Case Study in Oncology (<a class="reference external" href="https://proceedings.mlr.press/v219/wong23a.html">wong, zhang, …, poon, 2023</a>)</p>
<ul>
<li><p>LLMs can structure eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT)</p></li>
</ul>
</li>
<li><p>BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys (<a class="reference external" href="https://arxiv.org/abs/2310.10765">gu, yang, usuyama, …, gao, poon, 2023</a>)</p>
<ul>
<li><p>counterfactual biomedical image generation by instruction-learning from multimodal patient journeys</p></li>
<li><p>specifically, learn from triplets (prior image, progression description, new image), where GPT-4 generates progression description based on the image notes</p></li>
</ul>
</li>
</ul>
</section>
<section id="clinical-bio-image-segmentation">
<h3><span class="section-number">7.7.5.5. </span>clinical/bio image segmentation<a class="headerlink" href="#clinical-bio-image-segmentation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>3D models (2D + time)</p>
<ul>
<li><p>SAM 2 (<a class="reference external" href="https://arxiv.org/abs/2408.00714">FAIR, 2024</a>)</p>
<ul>
<li><p>MedSAM (<a class="reference external" href="https://www.nature.com/articles/s41467-024-44824-z">ma, he, li, han, you, &amp; wang, 2024</a>)</p>
<ul>
<li><p>MedSAM benchmarking &amp; deployment (<a class="reference external" href="https://arxiv.org/pdf/2408.03322">ma, …wang, 2024</a>)</p></li>
</ul>
</li>
<li><p>Medical SAM 2: Segment Medical Images as Video via Segment Anything Model 2 (<a class="reference external" href="https://arxiv.org/pdf/2408.00874">zhu…wu, 2024</a>) - finetuned on some biomedical domains</p></li>
</ul>
</li>
</ul>
</li>
<li><p>2D models (images)</p>
<ul>
<li><p>BioMedParse (<a class="reference external" href="https://arxiv.org/abs/2405.12971">zhao…poon, wang, 2024</a>) - 2D medical image segmentation</p></li>
<li><p>SAM 1 (<a class="reference external" href="https://arxiv.org/abs/2304.02643">FAIR, 2023</a>) - works only on 2D images</p></li>
</ul>
</li>
<li><p>4D/5D models (4D image + time)</p>
<ul>
<li><p>Semi-Supervised Echocardiography Video Segmentation via Adaptive Spatio-Temporal Tensor Semantic Awareness and Memory Flow (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/10833843/authors#authors">li…hu, 2025</a>)</p></li>
<li><p>LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging (<a class="reference external" href="https://arxiv.org/abs/2502.20985">rokuss…maier-hein, 2025</a>)</p></li>
</ul>
</li>
<li><p>Cell-pose (<a class="reference external" href="https://github.com/MouseLand/cellpose?tab=readme-ov-file">github</a>)</p>
<ul>
<li><p>Cellpose 1: a generalist algorithm for cellular segmentation (<a class="reference external" href="https://t.co/kBMXmPp3Yn?amp=1">stringer et al. 2021</a>)</p>
<ul>
<li><p>note: predicts (1) vector direction pointing to center of each cell &amp; (2) a binary probability of cell vs backgrounds</p>
<ul>
<li><p>vector direction is applied to find components that flow to the same center and then further refined by the binary prob. mask</p></li>
</ul>
</li>
<li><p>only takes in 2D images, in 3D computes the vectors using xy/xz/yz slices and then does segmentation on those vectors</p>
<ul>
<li><p>baseline stitching just does 2D segmentations then merges components whose ROI has IoU ≥ 0.25</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Cellpose 2: how to train your own model (<a class="reference external" href="https://www.nature.com/articles/s41592-022-01663-4">pachitariu &amp; stringer, 2022</a>)</p></li>
<li><p>Cellpose 3: one-click image restoration for improved segmentation (<a class="reference external" href="https://www.nature.com/articles/s41592-025-02595-5">stringer et al. 2025</a>) - trained model to output images that are well segmented by a generalist segmentation model, while maintaining perceptual similarity to the target images</p></li>
</ul>
</li>
<li><p>MaskCut / CutLER: Cut and Learn for Unsupervised Object Detection and Instance Segmentation (<a class="reference external" href="https://arxiv.org/pdf/2301.11320">wang, girdhar, yu, &amp; misra, 2023</a>)</p>
<ul>
<li><p>MaskCut - gets patch-wise similarity matrix from DINO then iteratively uses normalized cuts (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/868688">shi &amp; malik, 2000</a>) to identify objects (e.g. clusters)</p></li>
<li><p>VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation (<a class="reference external" href="https://arxiv.org/abs/2308.14710">wang…girdhar, darrell, 2023</a>)</p>
<ul>
<li><p>generate masks with maskcut, then creates synthetic video tracking training data by moving these masked objects around on background images</p></li>
</ul>
</li>
<li><p>Simplifying DINO via Coding Rate Regularization (<a class="reference external" href="https://arxiv.org/abs/2502.10385">wu…ma, 2025</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="cool-tasks">
<h3><span class="section-number">7.7.5.6. </span>cool tasks<a class="headerlink" href="#cool-tasks" title="Link to this heading">#</a></h3>
<ul>
<li><p>Forecasting Future World Events with Neural Networks (<a class="reference external" href="https://arxiv.org/abs/2206.15474">zou…hendrycks, 2022</a>) - takes tasks from metaculus</p></li>
<li><p>Shortcut Learning of LLMs in Natural Language Understanding: A Survey (<a class="reference external" href="https://arxiv.org/abs/2208.11857">du et al. 2022</a>)</p></li>
<li><p>A generative framework to bridge data-driven models and scientific theories in language neuroscience (<a class="reference external" href="https://arxiv.org/abs/2410.00812">antonello…huth, 2024</a>)</p></li>
<li><p>Neurosymbolic Programming for Science (<a class="reference external" href="https://arxiv.org/abs/2210.05050">sun…costilla-reyes, 2022</a>)</p></li>
<li><p>Discovering New Interpretable Conservation Laws as Sparse Invariants (<a class="reference external" href="https://arxiv.org/abs/2305.19525">liu…tegmark, 2023</a>) - does not use transformers</p></li>
<li><p>evaluation without groundtruth</p>
<ul class="simple">
<li><p>Evaluating Superhuman Models with Consistency Checks (<a class="reference external" href="https://arxiv.org/abs/2306.09983">fluri, …, tramer, 2023</a>)</p></li>
</ul>
</li>
<li><p>Learning from learning machines: a new generation of AI technology to meet the needs of science (<a class="reference external" href="https://arxiv.org/pdf/2111.13786.pdf">berkeley+lbnl+, 2021</a>)</p>
<ul class="simple">
<li><p>do more than predict what will happen, they attempt to offer insight into how or why</p></li>
<li><p>AI-based language models powering drug discovery and development (<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1359644621002816">liu et al. 2021</a>)</p></li>
<li><p>BioTranslator: Multilingual translation for zero-shot biomedical classification (<a class="reference external" href="https://www.nature.com/articles/s41467-023-36476-2">xu, woicik, poon, altman, &amp; wang, 2023</a>) - takes a user- written textual description of a new concept and then translates this description to a non-text biological data instance</p>
<ul>
<li><p>results for biological data, e.g. genes, proteins</p></li>
<li><p>enables the identification of novel cell types using only a textual description</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Towards an AI co-scientist (<a class="reference external" href="https://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf">gottweis…natarajan, 2025</a>)</p>
<ul class="simple">
<li><p>Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery (<a class="reference external" href="https://arxiv.org/abs/2305.14259">wang…hope, 2023</a>)</p></li>
<li><p>literature-based discovery (<a class="reference external" href="https://www.journals.uchicago.edu/doi/abs/10.1086/601720">swanson, 1986</a>) - focus on predicting pairwise links between concepts from papers (e.g. drug-disease links)</p>
<ul>
<li><p>task 1: idea-sentence generation – given sentences describing background context + a seed term, generate a sentence describing an idea</p></li>
<li><p>task 2: idea-node prediction – given the background context, predict new links between existing concepts (and generate new concepts)</p></li>
</ul>
</li>
<li><p>forecasting paper titles (<a class="reference external" href="https://csinva.io/gpt-paper-title-generator/">blog post</a>)</p></li>
<li><p>All That Glitters is Not Novel: Plagiarism in AI Generated Research (<a class="reference external" href="https://arxiv.org/pdf/2502.16487">gupta &amp; pruthi, 2025</a>)</p></li>
</ul>
</li>
<li><p>Communication with animals</p>
<ul class="simple">
<li><p><a class="reference external" href="https://coller-dolittle-24.sites.tau.ac.il">Coller-Dolittle Prize</a> for Inter-species Communication</p></li>
<li><p>Cetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales (<a class="reference external" href="https://arxiv.org/pdf/2104.08614.pdf">andreas, begus, …, wood, 2021</a>)</p>
<ul>
<li><p>sperm whale has largest brain</p></li>
<li><p>ML outputs are  primarily a tool to constrain hypothesis space to build formal and interpretable descriptions of the sperm whale communication</p></li>
</ul>
</li>
<li><p>A Theory of Unsupervised Translation Motivated by Understanding Animal Communication (<a class="reference external" href="https://arxiv.org/abs/2211.11081">goldwasser…paradise, 2023</a>)</p></li>
<li><p>Approaching an unknown communication system by latent space exploration and causal inference (<a class="reference external" href="https://arxiv.org/abs/2303.10931">begus, leban, &amp; gero, 2023</a>) - manipulate GAN latent variables in approach called causal disentanglement with extreme values (CDEV)</p></li>
<li><p>Vowels and Diphthongs in Sperm Whales (<a class="reference external" href="https://osf.io/preprints/osf/285cs">begus, sprous, leban, &amp; gero, 2023</a>) - use data from the dominica sperm whale project (<a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/mms.12086">gero et al. 2014</a>)</p></li>
</ul>
</li>
<li><p>scientific organization (<a class="reference external" href="https://galactica.org/static/paper.pdf">galactica</a>)</p>
<ul>
<li><p>related but smaller models</p>
<ul class="simple">
<li><p>SciBERT (<a class="reference external" href="https://arxiv.org/abs/1903.10676">beltagy…cohan, 2019</a>)</p></li>
<li><p>BioLM (<a class="reference external" href="https://aclanthology.org/2020.clinicalnlp-1.17/">lewis…stoyanov, 2020</a>)</p></li>
<li><p>ScholarBERT (<a class="reference external" href="https://arxiv.org/abs/2205.11342">hong…foster, 2022</a>) - large dataset, 770M-param model</p></li>
</ul>
</li>
<li><p>all data is processed in a common markdown format</p></li>
<li><p>task-specific tokens to support different types of knowledge (e.g. citations, step-by-step reasoning, different modalities, e.g. proteins)</p></li>
<li><p>chemical compounds (train on 2 mil / 110 mil from PubChem Compound, authors still want it to focus on text)</p>
<ul>
<li><p>predict IUPAC name from SMILES formula e.g. <code class="docutils literal notranslate"><span class="pre">CC(C)(C)C(=O)N(CC1=NC(=CS1)C(=O)OC)C2CCCCC2</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">methyl</span> <span class="pre">2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino]</span> <span class="pre">methyl]thiazole-4-</span> </code></p></li>
<li><p><a class="reference external" href="https://moleculenet.org/datasets-1">moleculenet</a> (<a class="reference external" href="https://arxiv.org/abs/1703.00564">wu et al. 2017</a>) classification benchmark (6 tasks)</p>
<ul>
<li><p>training set examples are trained as text during fitting</p>
<ul class="simple">
<li><p>HIV - classify whether comopund inhibits HIV replication</p></li>
<li><p>BACE C - binding results (classification + regression) for BACE</p></li>
<li><p>BBBP - blood-brain barrier penetration(permeability) (binary classification)</p></li>
<li><p>Tox21 - qualitative toxicity on 12 targets (12-class multilabel binary)</p></li>
<li><p>SIDER - 27-class multi-class disorders in different organ systems</p></li>
<li><p>ClinTox - binary toxicity classification</p></li>
</ul>
</li>
<li><p>ex. for BBBP (one of the 6 tasks) - question is posed in different ways during training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Here is a SMILES formula:   
   [START_I_SMILES]O=C(O)CCCC1=CC=C(N(CCCl)CCCl)C=C1[END_I_SMILES]
   
Question: Will the chemical compound penetrate the blood-brain barrier?
Answer: No
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p>protein sequences</p>
<ul class="simple">
<li><p>from 227 million in UniProt, look at only 0.5 million subset (called Swiss-Prot)</p></li>
<li><p>evaluate protein sequence perplexity</p></li>
<li><p>protein keyword prediction (predict keywords in UniProt, like “ATP-Binding”, “Cell membrane”)</p></li>
<li><p>protein function description - compare free-form description to GT UniProt function description</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="text-explanations-oldschool">
<h3><span class="section-number">7.7.5.7. </span>text explanations (oldschool)<a class="headerlink" href="#text-explanations-oldschool" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>WT5?! Training Text-to-Text Models to Explain their Predictions (<a class="reference external" href="https://arxiv.org/pdf/2004.14546.pdf">narang, raffel, …, malkan, 2020</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1812.05634.pdf">Adversarial Inference for Multi-Sentence Video Description</a> - adversarial techniques during inference for a better multi-sentence video description</p></li>
<li><p><a class="reference external" href="https://aclweb.org/anthology/D18-1437">Object Hallucination in Image Captioning</a> - image relevance metric - asses rate of object hallucination</p>
<ul>
<li><p>CHAIR metric - what proportion of words generated are actually in the image according to gt sentences and object segmentations</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1803.09797.pdf">women also snowboard</a> - force caption models to look at people when making gender-specific predictions</p></li>
<li><p><a class="reference external" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Fooling_Vision_and_CVPR_2018_paper.pdf">Fooling Vision and Language Models Despite Localization and Attention Mechanism</a> -  can do adversarial attacks on captioning and VQA</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1511.03745.pdf">Grounding of Textual Phrases in Images by Reconstruction</a> - given text and image provide a bounding box (supervised problem w/ attention)</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8791710">Natural Language Explanations of Classifier Behavior</a></p></li>
<li><p><a class="reference external" href="https://eli5.readthedocs.io/en/latest/libraries/sklearn.html#library-scikit-learn">eli5</a> has nice text highlighting for interp</p></li>
</ul>
</section>
</section>
<section id="other-modalities-domains">
<h2><span class="section-number">7.7.6. </span>other modalities / domains<a class="headerlink" href="#other-modalities-domains" title="Link to this heading">#</a></h2>
<section id="tabular-data">
<h3><span class="section-number">7.7.6.1. </span>tabular data<a class="headerlink" href="#tabular-data" title="Link to this heading">#</a></h3>
<ul>
<li><p>neurips 2023 <a class="reference external" href="https://table-representation-learning.github.io">tabular workshop</a> and <a class="reference external" href="https://arxiv.org/abs/2402.05121">review</a> from feb 4 2024</p></li>
<li><p>Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey (<a class="reference external" href="https://stewarthu.com/papers/LLM-on-tabular-data.pdf">fang…qi,…faloutsos, 2024</a>)</p></li>
<li><p><strong>tabPFN main works</strong></p>
<ul>
<li><p>TabICL: A Tabular Foundation Model for In-Context Learning on Large Data (<a class="reference external" href="https://www.arxiv.org/abs/2502.05564">qu…varoquax, le morvan, 2025</a>)</p></li>
<li><p>JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs (<a class="reference external" href="https://arxiv.org/pdf/2502.11877">shysheya…duvenaud, turner, 2025</a>)</p></li>
<li><h2 class="rubric" id="tabpfn-v2-accurate-predictions-on-small-data-with-a-tabular-foundation-model-hollman-hutter-2025">TabPFN v2: Accurate predictions on small data with a tabular foundation model (<a class="reference external" href="https://www.nature.com/articles/s41586-024-08328-6">hollman….hutter, 2025</a>)</h2>
<p>Model is open-source on huggingface and easy to use, but training dataset is not released (it was trained only on synthetic data)</p>
<ul class="simple">
<li><p>Model context length is limited to datasets with 10k samples / 500 features</p></li>
<li><p>minutia</p>
<ul>
<li><p>model is not quite invariant to feature order</p></li>
</ul>
</li>
</ul>
</li>
<li><p>TabPFN v1: A Transformer That Solves Small Tabular Classification Problems in a Second (<a class="reference external" href="https://arxiv.org/abs/2207.01848">hollman, …, hutter, 2022</a>)</p>
<ul class="simple">
<li><p>transformer takes in train + test dataset then outputs predictions</p></li>
<li><p>each row (data example) is treated as a token and test points attend only to training t</p>
<ul>
<li><p>takes fixed-size 100 columns, with zero-padded columns at the end (during training, randomly subsample columns)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>PFNs: prior-data fitted networks (<a class="reference external" href="https://arxiv.org/abs/2112.10510">muller, …, hutter, 2021</a>)</p>
<ul class="simple">
<li><p>trained on synthetic data</p></li>
</ul>
</li>
</ul>
</li>
<li><p>tabPFN applications</p>
<ul class="simple">
<li><p>TabPFN-TS: TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features (<a class="reference external" href="https://arxiv.org/abs/2501.02945">hoo…salinas, hutter, 2025</a>)</p>
<ul>
<li><p>engineer time embedding and just use that as features: index of timepoint, sine and cosine features</p></li>
<li><p>ForecastPFN: Synthetically-Trained Zero-Shot Forecasting (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/0731f0e65559059eb9cd9d6f44ce2dd8-Abstract-Conference.html">dooley…white, 2023</a>) - trained PFNs with a time-series prior</p></li>
</ul>
</li>
<li><p>A Closer Look at TabPFN v2: Strength, Limitation, and Extension (<a class="reference external" href="https://arxiv.org/abs/2502.17361">ye, liu, &amp; chao, 2025</a>)</p></li>
<li><p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data (<a class="reference external" href="https://arxiv.org/abs/2411.10634">helli…hutter, 2024</a>) - train and test TabPFN on SCM with edges that change over time</p>
<ul>
<li><p>In-context learning of evolving data streams with tabular foundational models (<a class="reference external" href="https://arxiv.org/abs/2502.16840">lourenco…marreiros, 2025</a>) - test TabPFN on SCM wieth edges that change over time</p></li>
</ul>
</li>
</ul>
</li>
<li><p>tabPFN-related</p>
<ul class="simple">
<li><p>GAMformer: In-Context Learning for Generalized Additive Models (<a class="reference external" href="https://arxiv.org/abs/2410.04560">mueller…caruana, hutter, 2024</a>)</p></li>
<li><p>Transformers Boost the Performance of Decision Trees on Tabular Data across Sample Sizes (<a class="reference external" href="https://arxiv.org/abs/2502.02672">jayawardhana…hutter, white, goldstein, goldblum, 2025</a>)</p>
<ul>
<li><p>learn boosted trees on top of TabPFN to extend to big datasets</p></li>
<li><p>learn boosted trees on top of LLM-based model to build in prior knowledge</p></li>
</ul>
</li>
<li><p>Can Transformers Learn Full Bayesian Inference in Context? (<a class="reference external" href="https://arxiv.org/abs/2501.16825">reuter…rugamer, 2025</a>)</p></li>
<li><p>MotherNet: A Foundational Hypernetwork for Tabular Classification (<a class="reference external" href="https://arxiv.org/abs/2312.08598">muller, curino, &amp; ramakrishan, 2023</a>) - generate parameters for a net from a training set and then use that net at test time</p></li>
</ul>
</li>
<li><p>value string methods - directly treating numerical values as strings and finetune GPT on them (everything is represented as text)</p>
<ul class="simple">
<li><p>LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks (<a class="reference external" href="https://arxiv.org/abs/2206.06565">dinh…lee, 2022</a>)</p></li>
<li><p>GreaT (<a class="reference external" href="https://openreview.net/forum?id=cEygmQNOeI">Borisov et al., 2022</a>)</p>
<ul>
<li><p>augmenting a sample with copies of different feature permutations</p></li>
</ul>
</li>
<li><p>TapTap (<a class="reference external" href="https://arxiv.org/abs/2305.09696">Zhang et al., 2023</a>)</p></li>
<li><p>Table-GPT (<a class="reference external" href="https://arxiv.org/pdf/2310.09263.pdf">li…chaudhuri, 2023</a>)</p></li>
<li><p>TabFMs: Towards Foundation Models for Learning on Tabular Data (<a class="reference external" href="https://arxiv.org/abs/2310.07338">zhang…bian, 2023</a>) - unified text</p></li>
<li><p>TableLlama: Towards Open Large Generalist Models for Tables (<a class="reference external" href="https://arxiv.org/pdf/2311.09206.pdf">zhang…sun, 2023</a>)</p></li>
<li><p>OmniPred: Language Models as Universal Regressors (<a class="reference external" href="https://arxiv.org/abs/2402.14547">song…chen, 2024</a>) - metalearn on huge number of regression problems from Google Vizier</p></li>
</ul>
</li>
<li><p>do not use text tokens</p>
<ul class="simple">
<li><p>TabDDPM: Modelling Tabular Data with Diffusion Models (<a class="reference external" href="https://arxiv.org/abs/2209.15421">kotelnikov…babenko 2022</a>)</p>
<ul>
<li><p>main eval: downstream ML model performance</p></li>
<li><p>Revisiting Pretraining Objectives for Tabular Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2207.03208">rubachev…babenko, 2022</a>)- using the object target labels during the pretraining stage is beneficial for the downstream performance</p></li>
</ul>
</li>
<li><p>FT-Transformer: Revisiting Deep Learning Models for Tabular Data (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html">gorishniy…babenko, 2021</a>)</p>
<ul>
<li><p>XTab: Cross-table Pretraining for Tabular Transformers (<a class="reference external" href="https://openreview.net/forum?id=uGORNDmIdr">zhu…shoaran, autogluon, 2023</a>)</p></li>
<li><p>Scaling Experiments in Self-Supervised Cross-Table Representation Learning (<a class="reference external" href="https://arxiv.org/pdf/2309.17339.pdf">schambach…otterbach, 2023</a>)</p></li>
<li><p>CT-BERT (<a class="reference external" href="https://arxiv.org/abs/2307.04308">Ye et al., 2023</a>)</p></li>
<li><p>TransTab (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/1377f76686d56439a2bd7a91859972f5-Abstract-Conference.html">Wang &amp; Sun, 2022</a>) - focus on clinical trial tables</p></li>
</ul>
</li>
<li><p>TABBIE (<a class="reference external" href="https://arxiv.org/abs/2105.02584">Iida, …, Iyyer, 2021</a>) - trained to detect corrupted cells (then embeddings used for downstream tasks)</p>
<ul>
<li><p>average row/column embeddings</p></li>
</ul>
</li>
<li><p>Enhanced Model-agnostic Training of Deep Tabular Generation Models <a class="reference external" href="https://openreview.net/forum?id=gJiOQw1fkF">https://openreview.net/forum?id=gJiOQw1fkF</a></p></li>
</ul>
</li>
<li><p>jointly encode table with text prompt / text in the table</p>
<ul class="simple">
<li><p>TP-BERTa: Making Pre-trained Language Models Great on Tabular Prediction (<a class="reference external" href="https://openreview.net/forum?id=anzIzGZuLi">2023</a>)</p>
<ul>
<li><p>adds <em>relative magnitude tokenization</em> - converts scalar numerical feature values to discrete tokens (discretization requires a label)</p></li>
<li><p><em>intra-feature attention</em> approach integrates feature values with the corresponding feature names</p></li>
</ul>
</li>
<li><p>UniPredict: LLMs are Universal Tabular Predictors (<a class="reference external" href="https://aps.arxiv.org/abs/2310.03266">wang, wang, &amp; sun, 2023</a>) - use text and prompt descriptions</p></li>
<li><p>Trompt: Towards a Better Deep Neural Network for Tabular Data (<a class="reference external" href="https://openreview.net/forum?id=0yNmeyteuS">chen…chang, 2023</a>) - use a prompting-style approach</p></li>
<li><p>TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data (<a class="reference external" href="https://www.semanticscholar.org/paper/TaBERT%3A-Pretraining-for-Joint-Understanding-of-and-Yin-Neubig/a5b1d1cab073cb746a990b37d42dc7b67763f881">yin, neubig, …, riedel, 2020</a>)</p></li>
</ul>
</li>
<li><p>classification / predictions</p>
<ul class="simple">
<li><p>TabR: Unlocking the power of retrieval-augmented tabular deep learning (<a class="reference external" href="https://arxiv.org/abs/2307.14338">gorishniy…babenko, 2023</a>)</p></li>
<li><p>TabLLM: Few-shot Classification of Tabular Data with LLMs  (<a class="reference external" href="https://arxiv.org/abs/2210.10723">hegelsmann…, sontag, 2022</a>)</p></li>
<li><p>Language models are weak learners (<a class="reference external" href="https://arxiv.org/abs/2306.14101">manikandan, jian, &amp; kolter, 2023</a>) - use prompted LLMs as weak learners in boosting algorithm for tabular data</p></li>
<li><p>TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns (<a class="reference external" href="https://arxiv.org/abs/2303.15747">onishi…hayashi, 2023</a>)</p></li>
<li><p>AnyPredict: A Universal Tabular Prediction System Based on LLMs <a class="reference external" href="https://openreview.net/forum?id=icuV4s8f2c">https://openreview.net/forum?id=icuV4s8f2c</a> - converting tabular data into machine-understandable prompts and fine-tuning LLMs to perform accurate predictions</p></li>
</ul>
</li>
<li><p>interpretability</p>
<ul class="simple">
<li><p>InterpreTabNet: Enhancing Interpretability of Tabular Data Using Deep Generative Models and LLM (<a class="reference external" href="https://openreview.net/pdf?id=kzR5Cj5blw">si…krishnan, 2023</a>) - make attention sparse and describe it with GPT4</p></li>
</ul>
</li>
<li><p>older</p>
<ul class="simple">
<li><p>AutoInt (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3357384.3357925">song…tang, 2019</a>)</p></li>
<li><p>(not using transformers): transform a relation table in a graph and perform random walks on the latter to produce node embeddings (<a class="reference external" href="https://dl.acm.org/doi/10.1145/3318464.3389742">cappuzzo et al., 2020</a>)</p></li>
<li><p>baseline methods: usually flatten tables, maybe with special character for starting each row/col</p>
<ul>
<li><p>could combine output from rows/cols with using element-wise product, average pooling and concatenation (<a class="reference external" href="https://dl.acm.org/doi/10.1145/3447548.3467228">tabularnet, 2021</a>)</p></li>
<li><p>sometimes add column headers to cell content</p></li>
<li><p>also popular is converting the table-to-text with finetuned models before processing</p></li>
</ul>
</li>
<li><p>CTAB-GAN+ (<a class="reference external" href="https://arxiv.org/abs/2204.00401">zhao…chen, 2022</a>)</p>
<ul>
<li><p>CTAB-GAN (<a class="reference external" href="https://proceedings.mlr.press/v157/zhao21a">zhao…chen, 2021</a>)</p></li>
<li><p>CTGAN (<a class="reference external" href="https://proceedings.neurips.cc/paper/2019/hash/254ed7d2de3b23ab10936522dd547b78-Abstract.html">xu…veeramachaneni, 2019</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>reviews</p>
<ul class="simple">
<li><p>Transformers for Tabular Data Representation: A Survey of Models and Applications (<a class="reference external" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00544/115239/Transformers-for-Tabular-Data-Representation-A">badaro…papotti, 2023</a>)</p>
<ul>
<li><p>common data sources: Wikipedia tables for QA (e.g. 3.2M tables in <a class="reference external" href="https://www.semanticscholar.org/paper/TabEL%3A-Entity-Linking-in-Web-Tables-Bhagavatula-Noraset/8ffcad9346c4978a211566fde6807d6fb4bfa5ed">this paper</a>) or WDC web table corpus (233M tables from <a class="reference external" href="https://dl.acm.org/doi/10.1145/2872518.2889386">lehmberg et al. 2016</a>)</p></li>
<li><p>modifications</p>
<ul>
<li><p>positional embeddings based on rows + cols</p></li>
<li><p>attention variants: add row-wise, sparse attention allows for adding more context</p></li>
</ul>
</li>
<li><p>Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks (<a class="reference external" href="https://www.semanticscholar.org/paper/Table-Pre-training%3A-A-Survey-on-Model-Pretraining-Dong-Cheng/49f4b4ca86e574c7ec688cfd45d2e17ff079c313">dong et al. 2022</a>)</p></li>
<li><p>Embeddings for Tabular Data: A Survey (<a class="reference external" href="https://arxiv.org/abs/2302.11777">singh &amp; bedathur, 2023</a>)</p></li>
<li><p>Deep neural networks and tabular data: A survey (<a class="reference internal" href="#"><span class="xref myst">borisov et al. 2022</span></a>) - mostly compares performance on standard tasks (e.g. classification)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="audio-time-series">
<h3><span class="section-number">7.7.6.2. </span>audio / time-series<a class="headerlink" href="#audio-time-series" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>CLAP: Learning Audio Concepts From Natural Language Supervision (<a class="reference external" href="https://arxiv.org/abs/2206.04769">elizalde…wang, 2022</a>) - learn audio-text embeddings through contrastive learning (like CLIP)</p>
<ul>
<li><p>Learning Audio Concepts from Counterfactual Natural Language (<a class="reference external" href="https://arxiv.org/abs/2401.04935">vosoughi…xu, 2024</a>) - improve learning signal by prompting text-only model to modify caption in a particular way that preserves the primary info and then using that as a third input during contrastive learning</p></li>
</ul>
</li>
<li><p>Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio (<a class="reference external" href="https://arxiv.org/abs/2402.09318">alonso-jimenez…rocamora, 2024</a>)</p></li>
</ul>
</section>
<section id="education">
<h3><span class="section-number">7.7.6.3. </span>education<a class="headerlink" href="#education" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach (<a class="reference external" href="https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf">jurenka…ibrahim, 2024</a>)</p>
<ul>
<li><p>seven diverse educational benchmark</p></li>
</ul>
</li>
<li><p>The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Response?s to Long-Form Input (<a class="reference external" href="https://arxiv.org/abs/2501.03200">jacovi…das, 2025</a>) - benchmark evaluates whether responses are consistent with a provided document as context</p></li>
</ul>
</section>
</section>
<section id="misc">
<h2><span class="section-number">7.7.7. </span>misc<a class="headerlink" href="#misc" title="Link to this heading">#</a></h2>
<section id="security">
<h3><span class="section-number">7.7.7.1. </span>security<a class="headerlink" href="#security" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>benchmarks: <a class="reference external" href="https://www.harmbench.org">harmbench</a> (Automated Red Teaming and Robust Refusal) &amp; <a class="reference external" href="https://arxiv.org/abs/2401.05561">trustllm</a> (diverse collection of datasets) &amp; <a class="reference external" href="https://jailbreakbench.github.io/">jailbreakbench</a></p></li>
<li><p>LLM Capture-the-flag <a class="reference external" href="https://ctf.spylab.ai">competition</a></p></li>
</ul>
<p><strong>Defenses</strong></p>
<ul class="simple">
<li><p>Baseline Defenses for Adversarial Attacks Against Aligned Language Models (<a class="reference external" href="https://arxiv.org/abs/2309.00614">jain…goldstein, 2023</a>)</p>
<ul>
<li><p>detection (perplexity based)</p></li>
<li><p>input preprocessing (paraphrase and retokenization)</p></li>
<li><p>adversarial training</p></li>
</ul>
</li>
<li><p>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT) (<a class="reference external" href="https://arxiv.org/pdf/2307.01225.pdf">sabir, babar, &amp; abuadbba, 2023</a>)</p>
<ul>
<li><p>leverages techniques such as attention maps, integrated gradients, and model feedback to detect and then change adversarial inputs</p></li>
</ul>
</li>
<li><p>generation-time defenses</p>
<ul>
<li><p>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves (<a class="reference external" href="https://arxiv.org/abs/2311.04205">deng…gu, 2023</a>)</p></li>
<li><p>SafeDecoding (<a class="reference external" href="https://arxiv.org/pdf/2402.08983#page=3.89">xu…poovendran, 2024</a>)</p></li>
<li><p>Hierarchical instruction following (<a class="reference external" href="https://arxiv.org/abs/2404.13208">wallace..beutel, 2024</a>)</p></li>
</ul>
</li>
<li><p>Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming (<a class="reference external" href="https://arxiv.org/pdf/2501.18837">anthropic 2025</a>) - use constitution to generate synthetic harmful/harmless texts and train classifiers on them</p></li>
</ul>
<p><strong>Attacks</strong></p>
<ul class="simple">
<li><p>LLM attacks</p>
<ul>
<li><p>Explore, Establish, Exploit: Red Teaming Language Models from Scratch (<a class="reference external" href="https://arxiv.org/abs/2306.09442">casper…hadfield-menell, 2023</a>) - consider red-teaming “from scratch” in which the adversary does not begin with a way to classify failures</p></li>
<li><p>BEAST: Fast Adversarial Attacks on Language Models In One GPU Minute (<a class="reference external" href="https://arxiv.org/abs/2402.15570">sadasivan…feizi, 2024</a>) - sample attacks using beam search and tokens that induce strong issues</p></li>
<li><p>Universal and Transferable Adversarial Attacks on Aligned Language Models (<a class="reference external" href="https://arxiv.org/abs/2307.15043">zou…fredrikson, 2023</a>)</p></li>
<li><p>NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models (<a class="reference external" href="https://aclanthology.org/2023.acl-long.867/">mei…ma, 2023</a>)</p></li>
<li><p>Transferability of Adversarial Images across Prompts on Vision-Language Models (<a class="reference external" href="https://openreview.net/forum?id=nc5GgFAvtk">luo…torr, 2024</a>)</p></li>
<li><p>Refusal in Language Models Is Mediated by a Single Direction (<a class="reference external" href="https://arxiv.org/pdf/2406.11717">arditi…nanda, 2024</a>)</p></li>
</ul>
</li>
<li><p>attacks from <a class="reference external" href="https://github.com/QData/TextAttack">TextAttack</a> (mostly focused on classification or entailment):</p>
<ul>
<li><p>hotflip: gradient-based word swap (<a class="reference external" href="https://arxiv.org/abs/1712.06751">Ebrahimi et al., 2017</a>; <a class="reference external" href="https://openreview.net/pdf?id=r1QZ3zbAZ">Kuleshov et al., 2018</a>)</p>
<ul>
<li><p>word embedding swap with genetic algo (<a class="reference external" href="https://arxiv.org/abs/1909.06723">Wang et al., 2019</a>)</p></li>
<li><p>input reduction with word deletion (<a class="reference external" href="https://arxiv.org/pdf/1804.07781.pdf">Feng et al., 2018</a>)</p></li>
<li><p>textbugger: greedy word swap based on saliency (<a class="reference external" href="https://www.aclweb.org/anthology/P19-1103/">Ren et al., 2019</a>)</p></li>
<li><p>textfooler: greedy word swap with many constraints: (word emb, part-of-speech, sentence emb (<a class="reference external" href="https://arxiv.org/abs/1907.11932">Jin et al., 2019</a>)</p></li>
<li><p>word swap with particle swarm optimization (<a class="reference external" href="https://www.aclweb.org/anthology/2020.acl-main.540/">Zang et al., 2020</a>)</p></li>
</ul>
</li>
<li><p>levenshtein edit distance on characters with gradient (<a class="reference external" href="https://arxiv.org/abs/1801.04354">Gao et al., 2018</a>)</p>
<ul>
<li><p>character swaps with sentence encoding similarity (<a class="reference external" href="https://arxiv.org/abs/1812.05271">Li et al., 2018</a>)</p></li>
<li><p>greedy character changes (<a class="reference external" href="https://arxiv.org/abs/1905.11268">pruthi et al., 2019</a>)</p></li>
</ul>
</li>
<li><p>genetic-based word perturbing (<a class="reference external" href="https://arxiv.org/abs/1804.07998">alzantot et al., 2018</a>; <a class="reference external" href="https://arxiv.org/abs/1909.00986">jia et al., 2019</a>)</p></li>
<li><p>bert masked-token prediction gradient, constrain based on sentence similarity (<a class="reference external" href="https://arxiv.org/abs/2004.01970">garg &amp; ramakrishnan, 2019</a>; <a class="reference external" href="https://arxiv.org/abs/2004.09984">li et al., 2020</a>)</p></li>
<li><p>checklist distance (<a class="reference external" href="https://arxiv.org/abs/2005.04118">ribeiro et al., 2020</a>)</p></li>
<li><p>gradient-based word perturbing (<a class="reference external" href="https://arxiv.org/abs/2109.00544">yoo et al., 2021</a>)</p></li>
</ul>
</li>
</ul>
<p><strong>Misc</strong></p>
<ul class="simple">
<li><p>Effective Backdoor Mitigation Depends on the Pre-training Objective (<a class="reference external" href="https://arxiv.org/abs/2311.14948">verma…bilmes, 2023</a>)</p>
<ul>
<li><p>CleanCLIP mitigates backdoors by finetuning models on a clean subset of image-text pairs using a combination of contrastive and self-supervised loss</p></li>
<li><p>If the original model is changed with a different pre-training objective, CleanCLIP fails to remove backdoors</p></li>
</ul>
</li>
<li><p>Adversaries Can Misuse Combinations of Safe Models (<a class="reference external" href="https://arxiv.org/pdf/2406.14595">jones, dragan, &amp; steinhardt, 2024</a>)</p></li>
</ul>
</section>
<section id="privacy">
<h3><span class="section-number">7.7.7.2. </span>privacy<a class="headerlink" href="#privacy" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Training Data Extraction From Pre-trained Language Models: A Survey (<a class="reference external" href="https://arxiv.org/abs/2305.16157">ishihara, 2023</a>)</p>
<ul>
<li><p>definitions</p>
<ul>
<li><p><em>eidetic memorization</em> -  a string s is k-eidetic memorized by LLM <span class="math notranslate nohighlight">\(f\)</span> if a prompt p exists such that <span class="math notranslate nohighlight">\(f(p) = s\)</span> and s appears at most k times in the training set</p>
<ul>
<li><p>slightly different definition: a string s is k-memorized with k tokens of context from LLM f if a (length-k) string p exists such that the concatenation p + s is contained in the training set, and f produces s when prompted with p by using greedy decoding</p></li>
</ul>
</li>
<li><p><em>differential privacy</em> = removing any data from the training set should not considerably change trained models</p></li>
<li><p><em>counterfactual memorization</em> = difference between a training data’s expected loss under a model that has and has not been trained on that data</p></li>
<li><p>some studies loosen the definition of memorization using a similarity metric for strings rather than exact string matching</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Extracting Training Data from LLMs (<a class="reference external" href="https://arxiv.org/abs/2012.07805">carlini, …, raffel, 2021</a>) - LLMs are particularly likely to memorize atypical data points</p>
<ul>
<li><p>Quantifying Memorization Across Neural Language Models (<a class="reference external" href="https://arxiv.org/abs/2202.07646">carlini, …, zhang, 2022</a>)</p></li>
<li><p>What does it mean for a language model to preserve privacy? (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3531146.3534642">brown, …, tramer, 2022</a>) - “privacy-preserving” LM should guarantee that a user’s data cannot ever appear (or be inferable) outside the context they originally expected it to appear in</p></li>
<li><p>Can Neural Network Memorization Be Localized? (<a class="reference external" href="https://arxiv.org/abs/2307.09542">maini, …, lipton, kolter, zhang, 2023</a>) - memorization is often confined to a small number of neurons or channels, propose example-tied dropout to direct memorization to few neurons</p></li>
</ul>
</li>
<li><p>Localizing Paragraph Memorization in Language Models (<a class="reference external" href="https://arxiv.org/abs/2403.19851">stoehr, …, lewis, 2024</a>)</p></li>
<li><p>Detecting Personal Information in Training Corpora: an Analysis (<a class="reference external" href="https://trustnlpworkshop.github.io/papers/28.pdf">subramani, luccioni, dodge, &amp; mitchell, 2023</a>)</p></li>
</ul>
</section>
<section id="symbolic-reasoning">
<h3><span class="section-number">7.7.7.3. </span>symbolic reasoning<a class="headerlink" href="#symbolic-reasoning" title="Link to this heading">#</a></h3>
<p><em>See also notes on <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_comp_neuro.html">📌 comp neuro</a>.</em></p>
<ul class="simple">
<li><p>Compositional processing emerges in neural networks solving math problems (<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8491571/">russin, roland fernandez, …, smolensky, gao, 2021</a>)</p></li>
<li><p>Modular Deep Learning (<a class="reference external" href="https://arxiv.org/pdf/2302.11529.pdf">pfeiffer, ruder, .., ponti, 2023)</a> - overview of different modular architectures</p></li>
<li><p>neurocompositional computing (<a class="reference external" href="https://arxiv.org/abs/2205.01128">smolensky…gao, 2022</a>)</p>
<ul>
<li><p>longer tutorial (<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2022/04/Neurocompositional_computing__tutorial.pdf">smolensky, …, gao, 2022</a>)</p></li>
<li><p><em>central paradox of cognition</em> is that brain both uses continuous neural symbols but is compositional (<a class="reference external" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.4352&amp;amp;rep=rep1&amp;amp;type=pdf">smolensky et al. 1992</a>)</p>
<ul>
<li><p>Compositionality</p></li>
<li><p>Continuity - the encoding and processing of information is formalized with real numbers that vary continuously</p></li>
</ul>
</li>
<li><p>3 challenges: compositional generalization, data efficiency, comprehensibility</p></li>
<li><p>solution - NECST: Neurally-Encoded Compositionally-Structured Tensor computing (<a class="reference external" href="https://psycnet.apa.org/record/2006-07970-000">smolensky &amp; legendre, 2006</a>) - basically leverages TPR</p>
<ul>
<li><p>TPR roles and fillers can both be made continuous</p></li>
</ul>
</li>
<li><p>neural space vs symbolic space (many different things (e.g. sentences) can mean the same thing) - word vectors can be thought of as “soft symbols”</p></li>
<li><p>want to move from symbolic repr. to neural repr. while keeping interpretability</p>
<ul>
<li><p>system should output intermediate steps in addition to answer</p></li>
<li><p>thinking fast (system 1: fast, intuitive) + slow (system 2: slower, logical, derivative)</p></li>
</ul>
</li>
<li><p>concrete proposal: transformer activation vector should encode graph of flow through the network</p>
<ul>
<li><p>ex. task: regurgitate a sequence</p></li>
</ul>
</li>
</ul>
</li>
<li><p>NECSTransformer: <a class="reference external" href="https://www.microsoft.com/en-us/research/publication/enhancing-the-transformer-with-explicit-relational-encoding-for-math-problem-solving/">Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving</a> (schlag, smolensky, …, schmidhuber, gao, 2019)</p>
<ul>
<li><p>TP-attention</p></li>
<li><p>beat SOA on free-form math word-problems</p></li>
<li><p>in addition to K, Q, V, also add a role-vector</p>
<ul>
<li><p>do element-wise multiplication of outputted vector with role-vector</p></li>
</ul>
</li>
<li><p>TPR built as outer product of 2 vectors:</p>
<ul>
<li><p>filler - the vector returned by attention</p>
<ul>
<li><p>ex. one head learns “second-argument-of”</p></li>
</ul>
</li>
<li><p>role - a relation conceptually labeling an edge of the attention graph</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/publication/natural-to-formal-language-generation-using-tensor-product-representations/">TP-N2F: Tensor Product Representation for Natural To Formal Language Generation - Microsoft Research</a> (chen…gao, 2019)</p></li>
<li><p>Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models (<a class="reference external" href="https://aclanthology.org/2023.findings-acl.111/">wang, huang, …, gao, 2023</a>) - use logical model to alter embeddings before feeding to LLM</p></li>
<li><p>Implicit Chain of Thought Reasoning via Knowledge Distillation (<a class="reference external" href="https://arxiv.org/abs/2311.01460">deng…smolensky…, 2023</a>)</p></li>
</ul>
</section>
<section id="tool-use-agents">
<h3><span class="section-number">7.7.7.4. </span>tool use / agents<a class="headerlink" href="#tool-use-agents" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>private</p>
<ul>
<li><p><a class="reference external" href="https://www.perplexity.ai/">https://www.perplexity.ai/</a> - nice demo adding citation to each fact</p></li>
<li><p><a class="reference external" href="https://you.com">https://you.com</a></p></li>
<li><p><a class="reference external" href="https://github.com/hwchase17/langchain">langchain</a> library</p></li>
<li><p><a class="reference external" href="https://www.fixie.ai/">https://www.fixie.ai/</a> - provide tools for wrapping APIs in LLM + interaction through router (also default modules for stateful storage, user identity, etc.)</p></li>
</ul>
</li>
<li><p>Augmented Language Models: a Survey (<a class="reference external" href="https://arxiv.org/abs/2302.07842">meta, 2023</a>) - 3 categories: reasoning, tools, action</p>
<ul>
<li><p>PAL: Program-aided Language Models (<a class="reference external" href="https://arxiv.org/abs/2211.10435">gao…neubig, 2023</a>)</p></li>
<li><p>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP (<a class="reference external" href="https://arxiv.org/abs/2212.14024">khattab, …, liang, potts, &amp; zaharia, 2022</a>) - use high-level programs to use multiple steps between retrieving and reading</p></li>
</ul>
</li>
<li><p>Toolformer: Language Models Can Teach Themselves to Use Tools (<a class="reference external" href="https://arxiv.org/abs/2302.04761">meta, 2023</a>) - model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction</p>
<ul>
<li><p>Given input, sample position and API call candidates, try them all, and filter out ones which do not reduce next-token loss</p>
<ul>
<li><p>put correct API calls into prompt, e.g. Pittsburgh is also known as <code class="docutils literal notranslate"><span class="pre">[QA(What</span> <span class="pre">...?→</span> <span class="pre">Steel</span> <span class="pre">City)]</span></code> the Steel City.</p></li>
</ul>
</li>
<li><p>Training</p>
<ul>
<li><p>start with few human-written examples of API use</p></li>
<li><p>LLM generates more uses</p></li>
<li><p>self-supervised loss determines which calls help with future-token prediction</p></li>
</ul>
</li>
</ul>
</li>
<li><p>original</p>
<ul>
<li><p>ACT-1: Transformer for Actions (<a class="reference external" href="https://www.adept.ai/act">2022, adept</a>) - transformer directly interacts with computer</p></li>
<li><p>ReAct: Synergizing Reasoning and Acting in Language Models (<a class="reference external" href="https://arxiv.org/abs/2210.03629">yao…cao, 2022</a>) - use LLMs to generate reasoning traces + task-specific actions in interleaved manner</p></li>
<li><p>RLPG (<a class="reference external" href="https://arxiv.org/abs/2206.12839">shrivastava, larochelle, &amp; tarlow, 2022</a>) - for code-completion, retrieves functions from a repo</p></li>
<li><p>knowledge base triplets</p>
<ul>
<li><p>Relational Memory-Augmented Language Models (<a class="reference external" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00476/110997/Relational-Memory-Augmented-Language-Models">liu, yogatama, &amp; blunsom, 2022</a>) - integrate knowledge base triplets with LLM</p></li>
<li><p>DRAGON: Deep Bidirectional Language-Knowledge Graph Pretraining (<a class="reference external" href="https://arxiv.org/abs/2210.09338">yasanaga, …, manning, liang, leskovec, 2022</a>)</p></li>
</ul>
</li>
<li><p>toolformer (<a class="reference external" href="https://arxiv.org/abs/2302.04761">schick, dwivedi-yu, …, scialom, 2023</a>)</p></li>
</ul>
</li>
<li><p>webgpt (<a class="reference external" href="https://arxiv.org/abs/2112.09332">nakano, …, schulman, 2022, OpenAI</a>) - allows google search to add world info</p>
<ul>
<li><p>Internet-augmented language models (<a class="reference external" href="https://arxiv.org/pdf/2203.05115.pdf">Lazaridou et al., 2022</a>)</p></li>
<li><p>GopherCite (<a class="reference external" href="https://arxiv.org/abs/2203.11147">menick, …, mcaleese, 2022, Deepmind</a>) - generate answers + link/relevant snippet when making predictions (trained with RL from human preferences )</p></li>
<li><p>LaMDA (<a class="reference external" href="https://arxiv.org/abs/2201.08239">thoppilan, …, quoc le, 2022, google</a>) - allows google search to add world info (in a dialog model)</p>
<ul>
<li><p>this was the model that sparked the controversy about consciousness 🤔</p></li>
<li><p>A Neural Corpus Indexer for Document Retrieval (<a class="reference external" href="https://arxiv.org/abs/2206.02743">wang…yang, 2022</a>) - train model to directly spit out document IDs given queries</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="multilingual-stuff">
<h3><span class="section-number">7.7.7.5. </span>multilingual stuff<a class="headerlink" href="#multilingual-stuff" title="Link to this heading">#</a></h3>
<p><strong>multilingual learning</strong></p>
<ul class="simple">
<li><p>Multilingual Jailbreak Challenges in Large Language Models (<a class="reference external" href="https://openreview.net/forum?id=vESNKdEMGp">deng…bing, 2024</a>) - jailbreaks work better in low-resource languages - propose to remedy this by safety finetuning on multilingual data</p></li>
<li><p><em>Evaluating and Mitigating Linguistic Discrimination in Large Language Models</em> (<a class="reference external" href="https://arxiv.org/abs/2404.18534">dong…wang, 2024</a>) - translate all queries into multiple languages and then get the response from the model, and then convert the responses to English and give the answer that has highest similarities to other answers</p></li>
<li><p><em>Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners</em> (<a class="reference external" href="https://arxiv.org/pdf/2405.13816v2">zhang…huang, 2024</a>) - applying logit lens finds that model internally translates to english in multilingual tasks</p></li>
<li><p><em>Low-Resource Languages Jailbreak GPT-4</em> (<a class="reference external" href="https://arxiv.org/abs/2310.02446">Yong…Bach 2024</a>): exact same result as the <a class="reference external" href="https://openreview.net/forum?id=vESNKdEMGp">deng…bing, 2024</a> paper — low resource languages have much higher ASR than high resource languages. They translated AdvBench in 12 languages and did it.</p></li>
<li><p><em>A Cross-Language Investigation into Jailbreak Attacks in Large Language Models</em> (<a class="reference external" href="https://arxiv.org/abs/2401.16765">Li…Xue 2024</a>): Not a well written paper. Findings: GPT4 does not experience difference in ASR across languages, whereas worse models do (for the unintentional case) — similar to our finding for GPT4. They have done some attention visualization for intentional, unintensional, and multilingual case — not in a good manner. Their mitigation is finetuning Vicuna model with questions in multiple languages. This paper created its own dataset and used Microsoft Translate for translation.</p></li>
<li><p><em>Comprehensive Evaluation of ChatGPT Reliability Through Multilingual Inquiries</em> (<a class="reference external" href="https://arxiv.org/abs/2312.10524">Puttaparthi…Yu 2023</a>): Constructed their own multilingual dataset, 30 malicious questions translated into 121 languages (Google Translate). Show that some languages have higher ASR than others (low resources ones, but they also generate lot of invalid responses). RQ2 is the interesting study, where they parts of a single question in different languages and mandated response in that language — it increased the ASR. This is useful.</p></li>
<li><p>MindMerger: Efficient Boosting LLM Reasoning in non-English Languages (<a class="reference external" href="https://arxiv.org/pdf/2405.17386">huang…yuan, 2024</a>) - merge capabilities across languages</p></li>
</ul>
<p><strong>multilingual representations</strong></p>
<ul class="simple">
<li><p>CS-LRD</p>
<ul>
<li><p>LSAR: Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations (<a class="reference external" href="https://arxiv.org/abs/2401.05792">xie…li, 2024</a>) - unsupervised approach to identify language-specific subspace, then project it out. the language specific subspace is common across languages.</p></li>
</ul>
</li>
<li><p>LRD</p>
<ul>
<li><p>A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations (<a class="reference external" href="https://arxiv.org/abs/2109.04727">yang…darve, 2021</a>): LRD finds a language specific subspace for each language and removes it from the language representations to get better language agnostic representation.</p></li>
<li><p>Other works look at token-level tasks for language-agnostic embeddings (e.g. <a class="reference external" href="https://arxiv.org/abs/2010.08275">gonen…goldberg, 2020</a>) — words level is not relevant to us</p></li>
</ul>
</li>
<li><p>Language Agnostic Code Embeddings (<a class="reference external" href="https://arxiv.org/abs/2310.16803">Utpala…Chen 2023</a>): Compare three model agnostic language embeddings computational methods, centering, LRD, and CS-LRD for code language embeddings. For 3 code tasks (classification, retrieval), they get the best agnostic representations with CS-LRD. also CS-LRD is sensitive to rank “r”, whereas LRD is not</p></li>
<li><p>First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT (<a class="reference external" href="https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C48&amp;amp;q=First+Align%2C+then+Predict%3A+Understanding+the+Cross-Lingual+Ability+of+Multilingual+BERT&amp;amp;btnG=">muller…seddah, 2021</a>)</p>
<ul>
<li><p>the model first aligns representations of different languages together, and then (starting from the middle layers) makes them more language-specific again (to accompany the language-specific training objective)</p></li>
</ul>
</li>
<li><p>The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities (<a class="reference external" href="https://arxiv.org/pdf/2411.04986">wu…kim, 2024</a>)</p></li>
<li><p>Cross-lingual Similarity of Multilingual Representations Revisited (<a class="reference external" href="https://aclanthology.org/2022.aacl-main.15.pdf">del &amp; fishel, 2022</a>)</p>
<ul>
<li><p>measure similarity with Averaged Neuron-Wise Correlation (ANC)</p></li>
</ul>
</li>
<li><p>Discovering Language-neutral Sub-networks in Multilingual Language Models (<a class="reference external" href="https://arxiv.org/abs/2205.12672">foroutan…aberer, 2022</a>)</p></li>
</ul>
<p><strong>cipher attacks on LLMs</strong></p>
<ul class="simple">
<li><p><em>GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher</em> (<a class="reference external" href="https://arxiv.org/abs/2308.06463">Yuan…Tu 2023</a>): This is similar to ASCII paper, that when instructed to talk in CIPHER, it can bypass model safety filters. However, if the model has never seen a CIPHER like morse or Caesar than the outputs are hardly valid. Outputs are only valid for ASCII and self-cipher. However, in both these cases one needs atleast 3 unsafe demos that can be recognized by a filter in the input space. Research Problem: So can we design a cipher that has high validity and cannot be detected in the input space with a classifier (self cipher can be, maybe other ciphers cannot be).</p></li>
<li><p><em>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</em> (<a class="reference external" href="https://arxiv.org/abs/2402.11753">Xu…Poovendran 2024</a>): Basically mixing text with ascii increases the AST by a lot, renders defense much less effective. This is similar to mixing english with words from other languages like in (<a class="reference external" href="https://arxiv.org/abs/2312.10524">Puttaparthi…Yu 2023</a>). They first show that LLMs have poor performance in recognizing ASCII, but not that poor, so the attack can still be executed. They also execute this as a nested attacked (which are the most successful I think). Experiment section well written, lot of baselines and relevant papers. So basically LLMs will execute the attack if it can do some basic understanding, but has not seen that kind of input much in real world.</p></li>
<li><p>Jailbreaking Proprietary Large Language Models using Word Substitution Cipher (<a class="reference external" href="https://arxiv.org/abs/2402.10601">Handa…Baral 2024</a>): short nice paper! just says substitute unsafe words with safe words, provide the mapping to the model and the original question substituted with the words. Ask the LLM to reply, high ASR for ChatGPT and Gemini.</p></li>
<li><p>CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2402.16717">Lv…Huang 2024</a>): In this case they ask the malicious question using code where the input sentence is encrypted using some simple coding schemes (reverse words or sort words by their length) and the code includes the decryption function. Highest ASR among all baselines which includes the CipherChat and multilingual.</p></li>
<li><p>MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds (<a class="reference external" href="https://arxiv.org/abs/2402.01706">Jin…Zhang 2024</a>): This is not doing cipher language. It creates several layers of alternate worlds where one can put a malicious query and it bypasses model security. The deeper the layers, the higher ASR the attack has.</p></li>
<li><p>Data Contamination Can Cross Language Barriers (<a class="reference external" href="https://arxiv.org/html/2406.13236v1">feng yao, yufan zhuang, …, jingbo shang</a>) - LLMs can overfit to benchmarks by being trained on translations of them</p>
<ul>
<li><p>To detect this contamination, for each question, we replace all the incorrect choices with correct choices taken from other questions</p></li>
</ul>
</li>
</ul>
</section>
<section id="in-context-learning">
<h3><span class="section-number">7.7.7.6. </span>in-context learning<a class="headerlink" href="#in-context-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes (<a class="reference external" href="https://arxiv.org/abs/2208.01066">garg, tsipras, liang, &amp; valiant, 2022</a>) - models can succesfully metalearn functions like OLS</p>
<ul>
<li><p>e.g. during training, learn inputs-outputs from different linear functions</p></li>
<li><p>during testing, have to predict outputs for inputs from a different linear function</p></li>
<li><p>also test on slightly harder functions, like decision trees and 2-layer nets</p></li>
<li><p>Decision tree (<a class="reference external" href="https://arxiv.org/abs/2402.03774">zhuang…gao, 2024</a>) - transformer can learn to algorithmically interpolate between CART and GOSDT</p></li>
<li><p>What Algorithms can Transformers Learn? A Study in Length Generalization (<a class="reference external" href="https://arxiv.org/abs/2310.16028">zhou…bengio, nakkiran, 2023</a>) - Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengthsr</p>
<ul>
<li><p>Transformers Can Achieve Length Generalization But Not Robustly (<a class="reference external" href="https://arxiv.org/abs/2402.09371">zhou…zhou, 2024</a>)</p></li>
</ul>
</li>
<li><p>Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions (<a class="reference external" href="https://arxiv.org/abs/2310.03016">bhattamishra…varun kanade, 2023</a>) - on boolean functions, transformers can learn to match optimal aglorithms for simple tasks but not on complex tasks</p>
<ul>
<li><p>Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples</p></li>
</ul>
</li>
<li><p>Limits of Transformer Language Models on Learning Algorithmic Compositions (<a class="reference external" href="https://arxiv.org/pdf/2402.05785.pdf">thomm…scholkopf, rahimi, 2024</a>)</p></li>
<li><p>Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning (<a class="reference external" href="https://openreview.net/forum?id=xEhKwsqxMa">li…papailiopoulos, oymak, 2023</a>) - CoT helps LLMs learn MLP compositional functions in-context</p></li>
<li><p>Vector-ICL: In-context Learning with Continuous Vector Representations (<a class="reference external" href="https://arxiv.org/abs/2410.05629">zhuang…gao, 2024</a>) - language-only LLMs can perform ICL on vectors from many domains using a simple lightweight linear projector trained with a simple reconstruction loss</p></li>
</ul>
</li>
<li><p>Learning a (sparse) linear model</p>
<ul>
<li><p>The contextual lasso: Sparse linear models via deep neural networks (<a class="reference external" href="https://arxiv.org/pdf/2302.00878.pdf">thompson, …, kohn, 2023</a>) - very rough results…</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2305.13072">Breaking the Paradox of Explainable Deep Learning</a></p></li>
<li><p>Aug-imodels (<a class="reference external" href="https://arxiv.org/abs/2209.11799">singh et al 2023</a>)</p></li>
</ul>
</li>
<li><p>What learning algorithm is in-context learning? Investigations with linear models (<a class="reference external" href="https://arxiv.org/abs/2211.15661">aykurek, schuurmans, andreas, ma, &amp; zhou, 2023</a>) - investigate prompting through synthetic experiments with transformers trained for linear regression</p>
<ul>
<li><p>Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning (<a class="reference external" href="https://arxiv.org/pdf/2301.07067.pdf">li, …, oymak, 2023</a>) - generalization bounds for in-context learning when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system</p></li>
<li><p>Trained Transformers Learn Linear Models In-Context (<a class="reference external" href="https://arxiv.org/pdf/2306.09927.pdf">zhang, frei, &amp; bartlett, 2023</a>)</p></li>
<li><p>One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention (<a class="reference external" href="https://arxiv.org/pdf/2307.03576.pdf">Mahankali, Hashimoto, Ma, 23</a>)</p>
<ul>
<li><p>math analysis for: icl can do gradient decent on linear regression</p></li>
</ul>
</li>
<li><p>Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression (<a class="reference external" href="https://openreview.net/forum?id=BtAz4a5xDg">raventos…ganguli, 2023</a>)</p></li>
</ul>
</li>
<li><p>Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models (<a class="reference external" href="https://arxiv.org/abs/2310.17086">fu…sharan, 2023</a>)</p>
<ul>
<li><p>How Well Can Transformers Emulate In-context Newton’s Method? (<a class="reference external" href="https://arxiv.org/pdf/2403.03183v1.pdf">giannou…papailiopoulos, &amp; lee, 2024</a>)</p></li>
</ul>
</li>
<li><p>Teaching Algorithmic Reasoning via In-context Learning (<a class="reference external" href="https://arxiv.org/abs/2211.09066">zhou…sedghi, 2022</a>)</p></li>
<li><p>LLMs can In-Context Learn Multiple Tasks in Superposition (<a class="reference external" href="https://arxiv.org/abs/2410.05603">xiong, …, papailiopoulous, 2024</a>) - like task arithmetic, but all happens through ICL prompting</p></li>
<li><p>Looped Transformers as Programmable Computers (<a class="reference external" href="https://arxiv.org/abs/2301.13196">giannou, …, jason lee, papailiopoulos, 2023</a>) - use transformers as universal computers by programming them with specific weights</p></li>
<li><p>Learning mathematical problems (<a class="reference external" href="https://scholar.google.com/citations?hl=en&amp;amp;user=1tMnd-4AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate">francois charton</a>)</p></li>
<li><p>Probing the Decision Boundaries of In-context Learning in Large Language Models (<a class="reference external" href="https://arxiv.org/pdf/2406.11233v1">zhao, nguyen, &amp; grover, 2024</a>)</p></li>
<li><p>Theory (don’t directly predict algorithm)</p>
<ul>
<li><p>Meta-learning for Mixed Linear Regression (<a class="reference external" href="https://proceedings.mlr.press/v119/kong20a.html">kong…kakade, oh, 2020</a>) - generalization for linear regression based on which linear tasks were seen before</p></li>
<li><p>Transformers are Universal In-context Learners (<a class="reference external" href="https://arxiv.org/abs/2408.01367">furuya…peyre, 2024</a>) - mathetmatically show that transformers are universal and can approximate continuous in-context mappings to arbitrary precision</p></li>
</ul>
</li>
<li><p>Limitations</p>
<ul>
<li><p>Faith and Fate: Limits of Transformers on Compositionality (<a class="reference external" href="https://arxiv.org/abs/2305.18654">dziri…choi, 2023</a>) - LLMs can’t (easily) be trained well for multiplication (and similar tasks)</p></li>
</ul>
</li>
<li><p>ICLR: In-Context Learning of Representations (<a class="reference external" href="https://arxiv.org/abs/2501.00070">park…wattenberg, tanaka, 2024</a>) - showing pairs of words sampled from a graph can make the embeddings of those words match the structure of that graph</p></li>
<li><p>Label Words are Anchors: An Information Flow Perspective for
Understanding In-Context Learning (<a class="reference external" href="https://aclanthology.org/2023.emnlp-main.609.pdf">wang…sun, 2023</a>)</p></li>
<li><p>Correlation and Navigation in the Vocabulary Key Representation Space of Language Models (<a class="reference external" href="https://arxiv.org/abs/2410.02284">peng…shang, 2024</a>) - some tokens are correlated in embedding space and wrong next-token completions  can be highly ranked if their embeddings are correlated with correct ones</p>
<ul>
<li><p>as we sample tokens in context, we get more diverse completions, skipping nearby wrong next tokens</p></li>
</ul>
</li>
</ul>
</section>
<section id="llm-limitations-critiques">
<h3><span class="section-number">7.7.7.7. </span>llm limitations / critiques<a class="headerlink" href="#llm-limitations-critiques" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Dissociating language and thought in LLMs: a cognitive perspective (<a class="reference external" href="https://arxiv.org/pdf/2301.06627.pdf">mahowald, …, tenenbaum, fedorenko, 2023</a>) - 2 competences: (1) formal &amp; (2) functional linguistic competence</p></li>
<li><p>Hallucination is Inevitable: An Innate Limitation of LLMs (<a class="reference external" href="https://arxiv.org/pdf/2401.11817.pdf">xu…kankanhalli, 2024</a>)</p></li>
<li><p>overview foundation models paper (<a class="reference external" href="https://arxiv.org/abs/2108.07258">stanford, 2022</a>)</p></li>
<li><p>critiques of prompting</p>
<ul>
<li><p>Do Prompt-Based Models Really Understand the Meaning of their Prompts? (<a class="reference external" href="https://arxiv.org/abs/2109.01247">webson &amp; pavlick, 2022</a>) - models can learn fine with prompts that are intentionally irrelevant</p>
<ul>
<li><p>Are Language Models Worse than Humans at Following Prompts? It’s Complicated (<a class="reference external" href="https://arxiv.org/abs/2301.07085">webson, …, pavlick, 2023</a>)</p></li>
</ul>
</li>
<li><p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity (<a class="reference external" href="https://arxiv.org/abs/2104.08786">lu…riedel, stenetorp, 2021</a>)</p></li>
<li><p>Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting (<a class="reference external" href="https://arxiv.org/abs/2310.11324">sclar, choi…, suhr, 2023</a>)</p></li>
<li><p>Lost in the Middle: How Language Models Use Long Contexts (<a class="reference external" href="https://arxiv.org/abs/2307.03172">liu…petroni, liang, 2023</a>) - LLMs often fail to properly use relevant context when it’s in the middle of a long context</p></li>
</ul>
</li>
</ul>
</section>
<section id="evaluating-with-llms">
<h3><span class="section-number">7.7.7.8. </span>evaluating with LLMs<a class="headerlink" href="#evaluating-with-llms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment (<a class="reference external" href="https://arxiv.org/abs/2303.16634">liu…zhu, 2023, microsoft</a>) - ask for a score (1-5) in different categories, e.g. fluency, relevance, …</p></li>
<li><p>Human-like Summarization Evaluation with ChatGPT (<a class="reference external" href="https://arxiv.org/abs/2304.02554">gao…wan, 2023</a>) - prompt-based scoring of different categories, facts</p></li>
<li><p>Question-answering</p>
<ul>
<li><p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation (<a class="reference external" href="https://arxiv.org/abs/2305.14251">min…hajishirzi, 2023</a>) - breaks a generation into a series of facts and count what fraction of facts are supported by a reliable knowledge source</p></li>
<li><p>PRD: Peer Rank and Discussion Improve LLM based Evaluations (<a class="reference external" href="https://arxiv.org/abs/2307.02762">li…du, 2023</a>)</p></li>
</ul>
</li>
<li><p>Machine-translation</p>
<ul>
<li><p>Towards Explainable Evaluation Metrics for Machine Translation (<a class="reference external" href="https://arxiv.org/abs/2306.13041">leiter…eger, 2023</a>)</p></li>
</ul>
</li>
<li><p>General NLG</p>
<ul>
<li><p>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate (<a class="reference external" href="https://arxiv.org/abs/2308.07201">chan…liu, 2023</a>)</p></li>
<li><p>AlignScore: Evaluating Factual Consistency with a Unified Alignment Function (<a class="reference external" href="https://arxiv.org/abs/2305.16739">zha…hu, 2023</a>) - train a model to explicitly evaluate factual consistency</p></li>
<li><p>Not All Metrics Are Guilty: Improving NLG Evaluation with LLM Paraphrasing (<a class="reference external" href="https://arxiv.org/abs/2305.15067">tang…wei, 2023</a>)</p></li>
</ul>
</li>
<li><p>Classical eval</p>
<ul>
<li><p>ROUGE, BLEU</p></li>
<li><p>BERTScore, BLEURTScore</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/research_ovws"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ovw_ml_medicine.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7.6. </span>ml in medicine</p>
      </div>
    </a>
    <a class="right-next"
       href="ovw_transfer_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.8. </span>transfer learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">7.7.1. prompting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-prompting">7.7.1.1. (auto)prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-chaining-decoding">7.7.1.2. llm chaining / decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-querying-causal-inference">7.7.1.3. llm querying / causal inference</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty">7.7.1.3.1. uncertainty</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-compression-compiling">7.7.1.4. prompt compression / compiling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifier-guided-generation">7.7.1.5. classifier-guided generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-engineering-vetting">7.7.2. architecture engineering &amp; vetting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-attention-variants">7.7.2.1. architecture/attention variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixture-of-experts-moe-routing">7.7.2.2. mixture of experts (MoE) / routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning">7.7.2.3. pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptation-transfer">7.7.2.4. adaptation / transfer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-time-training">7.7.2.5. test-time training</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mech-interp">7.7.3. (mech) interp</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-merging">7.7.3.1. model merging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#editing">7.7.3.2. editing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#direct-weight-inspection">7.7.3.3. direct weight inspection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-interpretation">7.7.3.4. debugging / interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretable-models">7.7.3.5. interpretable models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddings-related">7.7.4. embeddings-related</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-models">7.7.4.1. embedding models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explainable-embeddings">7.7.4.2. explainable embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-augmented-generation-rag">7.7.4.3. retrieval-augmented generation (RAG)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#external-memory">7.7.4.4. external memory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-discovery">7.7.5. explanation / discovery</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-module-explanation">7.7.5.1. dataset / module explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#directly-learning-algorithms">7.7.5.2. directly learning algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#automated-assistants-teaching-hitl">7.7.5.3. automated assistants, teaching, HITL</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clinical-nlp">7.7.5.4. clinical nlp</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clinical-bio-image-segmentation">7.7.5.5. clinical/bio image segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cool-tasks">7.7.5.6. cool tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-explanations-oldschool">7.7.5.7. text explanations (oldschool)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-modalities-domains">7.7.6. other modalities / domains</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tabular-data">7.7.6.1. tabular data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#audio-time-series">7.7.6.2. audio / time-series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#education">7.7.6.3. education</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">7.7.7. misc</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#security">7.7.7.1. security</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#privacy">7.7.7.2. privacy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#symbolic-reasoning">7.7.7.3. symbolic reasoning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-use-agents">7.7.7.4. tool use / agents</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multilingual-stuff">7.7.7.5. multilingual stuff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#in-context-learning">7.7.7.6. in-context learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-limitations-critiques">7.7.7.7. llm limitations / critiques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-with-llms">7.7.7.8. evaluating with LLMs</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>