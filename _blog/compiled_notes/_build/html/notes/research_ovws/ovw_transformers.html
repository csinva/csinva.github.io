
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1.9. transformers</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.10. causal inference" href="ovw_causal_inference.html" />
    <link rel="prev" title="1.8. ml in medicine" href="ovw_ml_medicine.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview üëã
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="research_ovws.html">
   1. research_ovws
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_transfer_learning.html">
     1.1. transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_disentanglement.html">
     1.2. disentanglement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_omics.html">
     1.3. omics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_complexity.html">
     1.4. complexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_interesting_science.html">
     1.5. interesting science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_dl_theory.html">
     1.6. dl theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_scat.html">
     1.7. scattering transform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_ml_medicine.html">
     1.8. ml in medicine
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1.9. transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_causal_inference.html">
     1.10. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_uncertainty.html">
     1.11. uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_interp.html">
     1.12. interpretability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_generalization.html">
     1.13. generalization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cs/cs.html">
   2. cs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/retrieval.html">
     2.1. info retrieval
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/data_structures.html">
     2.2. data structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/languages.html">
     2.3. languages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/software.html">
     2.4. software engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/quantum.html">
     2.5. quantum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/algo.html">
     2.6. algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/graphs.html">
     2.7. graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/os.html">
     2.8. os
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/arch.html">
     2.9. architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/reproducibility.html">
     2.10. reproducibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/comp_theory.html">
     2.11. cs theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../math/math.html">
   3. math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/differential_equations.html">
     3.1. differential equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/proofs.html">
     3.2. proofs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/analysis.html">
     3.3. real analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/linear_algebra.html">
     3.4. linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/signals.html">
     3.5. signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/optimization.html">
     3.6. optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/calculus.html">
     3.7. calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/chaos.html">
     3.8. chaos
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/math_basics.html">
     3.9. math basics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../stat/stat.html">
   4. stat
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/graphical_models.html">
     4.1. graphical models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/data_analysis.html">
     4.2. data analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/testing.html">
     4.3. testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/causal_inference.html">
     4.4. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/info_theory.html">
     4.5. info theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/linear_models.html">
     4.6. linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/time_series.html">
     4.7. time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/game_theory.html">
     4.8. game theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml/ml.html">
   5. ml
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/kernels.html">
     5.1. kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/nlp.html">
     5.2. nlp
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/comp_vision.html">
     5.3. computer vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/structure_ml.html">
     5.4. structure learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/classification.html">
     5.5. classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/unsupervised.html">
     5.6. unsupervised
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/deep_learning.html">
     5.7. deep learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/feature_selection.html">
     5.8. feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/learning_theory.html">
     5.9. learning theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/evaluation.html">
     5.10. evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ai/ai.html">
   6. ai
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/search.html">
     6.1. search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/decisions_rl.html">
     6.2. decisions, rl
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/fairness_sts.html">
     6.3. fairness, sts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/cogsci.html">
     6.4. cogsci
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/ai_futures.html">
     6.5. ai futures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/logic.html">
     6.6. logic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/philosophy.html">
     6.7. philosophy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/psychology.html">
     6.8. psychology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/knowledge_rep.html">
     6.9. representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neuro/neuro.html">
   7. neuro
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/disease.html">
     7.1. disease
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/brain_basics.html">
     7.2. brain basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/vissci.html">
     7.3. vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/comp_neuro.html">
     7.4. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/sensory_input.html">
     7.5. sensory input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/memory.html">
     7.6. memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/motor.html">
     7.7. motor system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/development.html">
     7.8. development
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/csinva/csinva.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notes/research_ovws/ovw_transformers.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#papers">
   1.9.1. papers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#high-performing">
     1.9.1.1. high-performing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#external-knowledge-tool-use-grounding">
     1.9.1.2. external knowledge / tool use / grounding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaptation-transfer">
     1.9.1.3. adaptation / transfer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pruning">
     1.9.1.4. pruning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prompting">
   1.9.2. prompting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoprompting">
     1.9.2.1. autoprompting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-chaining-decoding">
     1.9.2.2. llm chaining / decoding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#misc">
   1.9.3. misc
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-weight-inspection">
     1.9.3.1. direct weight inspection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-variants">
     1.9.3.2. attention variants
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#editing">
     1.9.3.3. editing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#debugging-interpretation">
     1.9.3.4. debugging / interpretation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symbolic-reasoning">
     1.9.3.5. symbolic reasoning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-experts-ensembles-mixture-of-experts-moe">
     1.9.3.6. sparse experts / ensembles / mixture of experts (MoE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-querying-causal-inference">
     1.9.3.7. llm querying / causal inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-explanation">
     1.9.3.8. dataset explanation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cool-tasks">
     1.9.3.9. cool tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-limitations-perspectives">
     1.9.3.10. llm limitations / perspectives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics">
   1.9.4. basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-overview-of-transformers-formal-algorithms-for-transformers">
     1.9.4.1. mathematical overview of transformers (Formal Algorithms for Transformers)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visual-explanation-notes-on-article-by-jay-allamar">
     1.9.4.2. visual explanation (notes on article by jay allamar)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#huggingface-tutorial">
     1.9.4.3. huggingface tutorial
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-transformer-nlp-models">
       1.9.4.3.1. pre-transformer nlp models
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>transformers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#papers">
   1.9.1. papers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#high-performing">
     1.9.1.1. high-performing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#external-knowledge-tool-use-grounding">
     1.9.1.2. external knowledge / tool use / grounding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#adaptation-transfer">
     1.9.1.3. adaptation / transfer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pruning">
     1.9.1.4. pruning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prompting">
   1.9.2. prompting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autoprompting">
     1.9.2.1. autoprompting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-chaining-decoding">
     1.9.2.2. llm chaining / decoding
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#misc">
   1.9.3. misc
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-weight-inspection">
     1.9.3.1. direct weight inspection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attention-variants">
     1.9.3.2. attention variants
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#editing">
     1.9.3.3. editing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#debugging-interpretation">
     1.9.3.4. debugging / interpretation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symbolic-reasoning">
     1.9.3.5. symbolic reasoning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-experts-ensembles-mixture-of-experts-moe">
     1.9.3.6. sparse experts / ensembles / mixture of experts (MoE)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-querying-causal-inference">
     1.9.3.7. llm querying / causal inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-explanation">
     1.9.3.8. dataset explanation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cool-tasks">
     1.9.3.9. cool tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#llm-limitations-perspectives">
     1.9.3.10. llm limitations / perspectives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics">
   1.9.4. basics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-overview-of-transformers-formal-algorithms-for-transformers">
     1.9.4.1. mathematical overview of transformers (Formal Algorithms for Transformers)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visual-explanation-notes-on-article-by-jay-allamar">
     1.9.4.2. visual explanation (notes on article by jay allamar)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#huggingface-tutorial">
     1.9.4.3. huggingface tutorial
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pre-transformer-nlp-models">
       1.9.4.3.1. pre-transformer nlp models
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="transformers">
<h1><span class="section-number">1.9. </span>transformers<a class="headerlink" href="#transformers" title="Permalink to this headline">#</a></h1>
<section id="papers">
<h2><span class="section-number">1.9.1. </span>papers<a class="headerlink" href="#papers" title="Permalink to this headline">#</a></h2>
<section id="high-performing">
<h3><span class="section-number">1.9.1.1. </span>high-performing<a class="headerlink" href="#high-performing" title="Permalink to this headline">#</a></h3>
<p><strong>nlp</strong> (see also <a class="reference external" href="https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56">this link</a>)</p>
<ul class="simple">
<li><p>early papers</p>
<ul>
<li><p>attention is all you need (<a class="reference external" href="https://arxiv.org/abs/1706.03762">vaswani et al. 2017</a>) - initial transformer</p>
<ul>
<li><p>encoder-decoder transformer for seq-to-seq (most new models don‚Äôt have  special encoder-decoder structure for translation)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1511.01432">Semi-supervised Sequence Learning</a> (dai &amp; quoc le, 2015)</p>
<ul>
<li><p>context vector is weighted sum of context vector at each word</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1801.06146">ULMFiT</a> (howard &amp; ruder, 2018)</p></li>
</ul>
</li>
<li><p>BERT (<a class="reference external" href="https://arxiv.org/abs/1810.04805">devlin et al. 2018</a>) - semi-supervised learning (predict masked word - this is bidirectional) + supervised finetuning</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1907.11692">roberta</a> (liu et al. 2019)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.13461">BART</a> (lewis et al. 2019) - generalizes BERT with sequence-to-squence training: train by (1) corrupting text then (2) reconstruct the original text</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1802.05365">ELMo</a> (peters‚Ä¶zettlemoyer, 2018) - no word embeddings - train embeddings w/ bidirectional lstm (on language modeling)</p></li>
<li><p>XLNet (<a class="reference external" href="https://arxiv.org/abs/1906.08237">yang‚Ä¶quoc le, 2020</a>)</p></li>
</ul>
</li>
<li><p>GPT-3 (<a class="reference external" href="https://arxiv.org/abs/2005.14165?2">brown et al. 2020</a>) - identitical to GPT-2 except larger and replaces dense attention with sparse attention</p>
<ul>
<li><p>sizes: largest ha 175B params, 96 layers, 96 heads in each layer, head with dim 128, vocab size ~50k</p></li>
<li><p>InstructGPT (<a class="reference external" href="https://arxiv.org/abs/2203.02155">ouyang‚Ä¶lowe, 2022</a>)</p></li>
<li><p>GPT-2 (<a class="reference external" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">radford et al. 2018</a>)</p></li>
<li><p>GPT (<a class="reference external" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">radford et al. 2018</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2112.11446">Gopher</a> (deepmind, 2021) - basically gpt-3 with slight mods (replace layernorm by RMSnorm, different positional embeddings)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a> (beltagy, peters, &amp; cohan, 2020) - processes very long contexts</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways</a> (google, 2022) - 540 Billion params</p>
<ul>
<li><p>pathways hardware center allows for fast/efficient training</p></li>
<li><p>discontinuous improvements - at some point large model improves</p></li>
<li><p>prompt engineering: ‚ÄúExplain yourself‚Äù - lets it explain jokes</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2203.15556">Chinchilla: Training Compute-Optimal Large Language Models</a> (deepmind, 2022)</p>
<ul>
<li><p>‚Äúchinchilla scaling laws‚Äù - for compute-optimal training, the model size and the number of training tokens should be scaled equally</p></li>
</ul>
</li>
</ul>
</li>
<li><p>T0 (<a class="reference external" href="https://arxiv.org/pdf/2110.08207.pdf">sanh‚Ä¶rush, 2022</a>) - multitask training enables better zero-shot generalization</p>
<ul>
<li><p>T5 (<a class="reference external" href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">raffel‚Ä¶liu, 2020</a>) ‚Äì text-to-text transfer transformer</p></li>
<li><p>UL2: Unifying Language Learning Paradigms (<a class="reference external" href="https://arxiv.org/abs/2205.05131">tay‚Ä¶metzler, 2022</a>) - open-source 20B model, beats GPT-3 at zero-shot</p></li>
</ul>
</li>
<li><p>Galactica: A Large Language Model for Science (<a class="reference external" href="https://galactica.org/static/paper.pdf">taylor‚Ä¶, stojnic, 2022, meta ai</a>) - trained on mostly papers + some knowledge bases (e.g. DNA)</p></li>
<li><p>more effective training</p>
<ul>
<li><p>FLAN-PaLM: Scaling Instruction-Finetuned Language Models (<a class="reference external" href="https://arxiv.org/abs/2210.11416">chung, ‚Ä¶, quoc le, jason wei, 2022</a>) - finetune with datasets phrased as instructions</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2203.02155">instructGPT</a> / <a class="reference external" href="https://arxiv.org/abs/2109.01652">FLAN</a> - finetune on instructions to follows instructions</p></li>
</ul>
</li>
<li><p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (<a class="reference external" href="https://arxiv.org/abs/2003.10555">clark‚Ä¶quoc le, chris manning, 2020</a>)</p>
<ul>
<li><p>more efficient: rather than standard masked training, use generator-discriminator setup for ‚Äútoken detection‚Äù</p>
<ul>
<li><p>generator replaces many masked tokens with plausible samples - train with MLM</p></li>
<li><p>discriminator tries to guess which tokens were the masked ones - this is the main model that gets used</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>other</strong></p>
<ul>
<li><p>text-vision models</p>
<ul class="simple">
<li><p>CLIP (<a class="reference external" href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf">radford et al. 2021</a>) - jointly train text/images</p>
<ul>
<li><p>batch-based loss: encodings from same image/text pair should be close while encodings across different examples in the batch should be different</p></li>
<li><p>note: empirically works better with very large batch size</p></li>
</ul>
</li>
<li><p>DALL-E 2 (<a class="reference external" href="https://openai.com/dall-e-2/">OpenAI, 2022</a>)</p>
<ul>
<li><p>clip is foundation as generative model</p></li>
<li><p>generates text + image embeddings</p></li>
<li><p>‚Äúprior network‚Äù maps text embedding to image embedding</p></li>
<li><p>adds diffusion model</p></li>
<li><p>Stable diffusion (<a class="reference external" href="https://stability.ai/blog/stable-diffusion-public-release">stability.ai, 2022</a>) - open-source recreation, now highly optimized for speed</p></li>
</ul>
</li>
<li><p>BEiT-3 (<a class="reference external" href="https://arxiv.org/abs/2208.10442">2022</a>) - treat vision as language and large-scale multimodal training</p>
<ul>
<li><p>outperforms <a class="reference external" href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a> (2022), which uses more domain knowledge to connect vision &amp; language</p></li>
</ul>
</li>
</ul>
</li>
<li><p>vision</p>
<ul class="simple">
<li><p>VIT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (<a class="reference external" href="https://arxiv.org/abs/2010.11929">dosoviskiy, ‚Ä¶, houlsby, 2020</a>)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1904.09925">attention augmentation to resnet</a> for vision (bello‚Ä¶quoc le, 2020)</p></li>
<li><p>here, people call image patches ‚Äútokens‚Äù</p></li>
</ul>
</li>
<li><p>DINO <a class="reference external" href="https://arxiv.org/abs/2104.14294">Emerging Properties in Self-Supervised Vision Transformers</a> (caron‚Ä¶joulin, 2021)</p></li>
<li><p>Masked Autoencoders Are Scalable Vision Learners (<a class="reference external" href="https://arxiv.org/abs/2111.06377">he‚Ä¶dollar, girshick, 2021</a>) - BERT-style training</p>
<ul>
<li><p>speed up by not applying encoder to mask tokens + adding mask to a lot of the data (like 75%)</p></li>
<li><p>really good results without much data</p></li>
</ul>
</li>
</ul>
</li>
<li><p>rl</p>
<ul class="simple">
<li><p>AdA: Human-Timescale Adaptation in an Open-Ended Task Space (<a class="reference external" href="https://arxiv.org/abs/2301.07608">deepmind, 2023</a>)</p></li>
<li><p>GATO: <a class="reference external" href="https://arxiv.org/abs/2205.06175">A Generalist Agent</a> (deepmind, 2022) - single agent plays many different video games</p>
<ul>
<li><p>different modalities are converted to tokens differently (e.g. image patches are fed through resnet)</p></li>
</ul>
</li>
<li><p>In-context Reinforcement Learning with Algorithm Distillation (<a class="reference external" href="https://arxiv.org/abs/2210.14215">laskin, wang, ‚Ä¶, sahni, satinder singh, mnih, 2022, deepmind</a>) - learn to improve an RL algorithm</p>
<ul>
<li><p>put history of (observation, action, reward) sequences into context and then use them to predict new action given new observation</p></li>
</ul>
</li>
<li><p>Decision Transformer: Reinforcement Learning via Sequence Modeling (<a class="reference external" href="https://arxiv.org/pdf/2106.01345.pdf">chen, lu, ‚Ä¶abbeel, srinivas, mordatch, 2021</a>) - transformer that predicts what the next highest reward step is instead of the next word</p></li>
</ul>
</li>
<li><p>biomedical</p>
<ul class="simple">
<li><p>BioGPT: <a class="reference external" href="https://academic.oup.com/bib/article-abstract/23/6/bbac409/6713511">Generative pre-trained transformer for biomedical text generation and mining</a> (luo‚Ä¶poon, liu, 2022)</p>
<ul>
<li><p>PubMedGPT (2.7B): (<a class="reference external" href="https://crfm.stanford.edu/2022/12/15/pubmedgpt.html">bolton, hall, ‚Ä¶, manning, liang, 2022</a>)</p></li>
<li><p>BioBERT: <a class="reference external" href="https://arxiv.org/abs/1901.08746">A pre-trained biomedical language representation model for biomedical text mining</a> (2019)</p></li>
<li><p>PubMedBERT: <a class="reference external" href="https://arxiv.org/abs/2007.15779">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</a> (gu‚Ä¶gao, poon, 2021)</p></li>
<li><p>Large Language Models Encode Clinical Knowledge (<a class="reference external" href="https://arxiv.org/abs/2212.13138">singhal, ‚Ä¶, natarajan, 2022, google/deepmind</a>) - introduce MultiMedQA dataset + derive Med-PaLM, a prompt-tuned version of PaLM</p></li>
</ul>
</li>
</ul>
</li>
<li><p>question-answering (now just done with generic LLMs)</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.00700">UnifiedQA: Crossing Format Boundaries With a Single QA System</a> (khashabi‚Ä¶hajishirzi, 2020)</p></li>
</ul>
</li>
<li><p>tabular</p>
<ul class="simple">
<li><p>TabLLM: Few-shot Classification of Tabular Data with Large Language Models  (<a class="reference external" href="https://arxiv.org/abs/2210.10723">hegelsmann‚Ä¶, sontag, 2022</a>)</p></li>
</ul>
</li>
<li><p>metalearning</p>
<ul class="simple">
<li><p>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second (<a class="reference external" href="https://arxiv.org/abs/2207.01848">hollman, ‚Ä¶, hutter, 2022</a>) - transformer takes in train + test dataset then outputs predictions</p>
<ul>
<li><p>builds on prior-data fitted networks (PFNs) (<a class="reference external" href="https://arxiv.org/abs/2112.10510">muller, ‚Ä¶, hutter, 2021</a>)</p></li>
</ul>
</li>
<li><p>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes (<a class="reference external" href="https://arxiv.org/abs/2208.01066">garg, tsipras, liang, &amp; valiant, 2022</a>) - models can succesfully metalearn functions like OLS</p>
<ul>
<li><p>e.g. during training, learn inputs-outputs from different linear functions</p></li>
<li><p>during testing, have to predict outputs for inputs from a different linear function</p></li>
<li><p>also test on slightly harder functions, like decision trees and 2-layer nets</p></li>
</ul>
</li>
</ul>
</li>
<li><p>dialog</p>
<ul class="simple">
<li><p>ChatGPT</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.11309">GODEL: Large-Scale Pre-Training for Goal-Directed Dialog</a> (baolin peng, galley, ‚Ä¶, gao , 2022) - add grounded pre-training</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1706.05125">Deal or No Deal? End-to-End Learning for Negotiation Dialogues</a> (lewis‚Ä¶batra, 2017, Meta) - controversial paper where agents ‚Äúmake up their own language‚Äù</p>
<ul>
<li><p>this is pre-transformers</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.14858">MINERVA: Solving Quantitative Reasoning Problems with Language Models</a> (google, 2022) - train on well-parsed, domain-specific data (math arxiv) to solve math-reasoning problems</p>
<ul class="simple">
<li><p>autoformalization (<a class="reference external" href="https://arxiv.org/abs/2205.12615">wu‚Ä¶, szegedy, 2022</a>) - translating from natural language math to formal language</p></li>
<li><p>produce sql/python that then finds an answer (<a class="reference external" href="https://arxiv.org/abs/2210.02875">cheng‚Ä¶zettlemoyer, smith, yu, 2022</a>)</p></li>
</ul>
</li>
<li><p>CODEX: <a class="reference external" href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a> (2021, openai)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.11640">Repair Is Nearly Generation: Multilingual Program Repair with LLMs</a> (joshi et al. 2022)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.10583">Improving automatically generated code from Codex via Automated Program Repair</a> (fan et al. 2022) - use automated program repair to tweak codex outputs to make them better</p></li>
<li><p>Generating Question Titles for Stack Overflow from Mined Code Snippets (<a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3401026?casa_token=FEWYSo9ZmNIAAAAA:-_ZIkXQVUR3xYaB3NtrzBv0jZU6IZ6O4f_W_ZDtb6TipLBV4YHB-0lbO1JU8T9wwIl_jLBS3ts0">gao et al. 2020</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2111.03922">Automatic Program Repair with OpenAI‚Äôs Codex: Evaluating QuixBugs</a> (prenner &amp; robbes, 2021)</p>
<ul>
<li><p>use prompt like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#### fix the bug in the following function</span>
<span class="o">&lt;</span><span class="n">buggy</span> <span class="n">function</span> <span class="ow">and</span><span class="o">/</span><span class="ow">or</span> <span class="n">docstring</span> <span class="n">here</span><span class="o">&gt;</span>
<span class="c1">#### fixed function</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>program synthesis <a class="reference external" href="https://arxiv.org/abs/2108.07732">arxiv.org/abs/2108.07732</a> - formalize natural language into runnable code</p></li>
</ul>
</li>
<li><p>natural language feedback (<span class="xref myst">scheurer et al. 2022</span>)</p>
<ul class="simple">
<li><p>human feedback for learning makes it much more efficient</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.02329">Can language models learn from explanations in context?</a> (lampinen et al. 2022)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf">spatial transformers</a></p></li>
</ul>
</section>
<section id="external-knowledge-tool-use-grounding">
<h3><span class="section-number">1.9.1.2. </span>external knowledge / tool use / grounding<a class="headerlink" href="#external-knowledge-tool-use-grounding" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.perplexity.ai/">https://www.perplexity.ai/</a> - nice demo adding citation to each fact</p></li>
<li><p>implementations in <a class="reference external" href="https://github.com/hwchase17/langchain">langchain</a> library</p></li>
<li><p>DRAGON: Deep Bidirectional Language-Knowledge Graph Pretraining (<a class="reference external" href="https://arxiv.org/abs/2210.09338">yasanaga, ‚Ä¶, manning, liang, leskovec, 2022</a>)</p></li>
<li><p>LaMDA (<a class="reference external" href="https://arxiv.org/abs/2201.08239">thoppilan, ‚Ä¶, quoc le, 2022, google</a>) - allows goxogle search to add world info (in a dialog model)</p>
<ul>
<li><p>this was the model that sparked the controversy about consciousness ü§î</p></li>
<li><p>A Neural Corpus Indexer for Document Retrieval (<a class="reference external" href="https://arxiv.org/abs/2206.02743">wang‚Ä¶yang, 2022</a>) - train model to directly spit out document IDs given queries</p></li>
</ul>
</li>
<li><p>webgpt (<a class="reference external" href="https://arxiv.org/abs/2112.09332">nakano, ‚Ä¶, schulman, 2022, OpenAI</a>) - allows google search to add world info</p>
<ul>
<li><p>GopherCite (<a class="reference external" href="https://arxiv.org/abs/2203.11147">menick, ‚Ä¶, mcaleese, 2022, Deepmind</a>) - generate answers + link/relevant snippet when making predictions (trained with RL from human preferences )</p></li>
</ul>
</li>
<li><p>RLPG (<a class="reference external" href="https://arxiv.org/abs/2206.12839">shrivastava, larochelle, &amp; tarlow, 2022</a>) - for code-completion, retrieves functions from a repo</p></li>
<li><p>REALM (<a class="reference external" href="https://arxiv.org/abs/2002.08909">guu, ‚Ä¶, chang, 2020</a>) - retrieves document chunks from corpus and adds them to context, for open-domain QA</p></li>
<li><p>Relational Memory-Augmented Language Models (<a class="reference external" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00476/110997/Relational-Memory-Augmented-Language-Models">liu, yogatama, &amp; blunsom, 2022</a>) - integrate knowledge base triplets with LLM</p></li>
<li><p>ACT-1: Transformer for Actions (<a class="reference external" href="https://www.adept.ai/act">2022, Adept</a>) - transformer directly interacts with computer</p></li>
</ul>
</section>
<section id="adaptation-transfer">
<h3><span class="section-number">1.9.1.3. </span>adaptation / transfer<a class="headerlink" href="#adaptation-transfer" title="Permalink to this headline">#</a></h3>
<p><em>These are transformer-specific. For more general notes, see <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transfer_learning.html">üìå transfer learning</a> or <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transfer_learning.html">üìå uncertainty</a>.</em> Most of these approaches can be combined with metalearning.</p>
<ul class="simple">
<li><p>finetuning</p>
<ul>
<li><p>finetune all DNN params</p></li>
<li><p>finetune linear layer on activations</p>
<ul>
<li><p>standard - train linear model on the embedding of the first token (usually an added <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token) (<a class="reference external" href="https://aclanthology.org/N18-1202/">peters et al. 2018</a>)</p></li>
<li><p>finetune linear model on all the activations</p>
<ul>
<li><p>e.g. <a class="reference external" href="https://arxiv.org/abs/2201.03529">evci, et al. 2022</a> - learn linear layer (using group-lasso) on features extracted from all layers</p></li>
</ul>
</li>
</ul>
</li>
<li><p>finetune specific DNN params (e.g. just the bias terms)</p>
<ul>
<li><p>Cutting Down on Prompts and Parameters (<a class="reference external" href="https://arxiv.org/abs/2106.13353">logan‚Ä¶sameer singh, riedel, 2021</a>) - finetune only the bias terms; works even with null prompts</p></li>
<li><p>BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models (<a class="reference external" href="https://arxiv.org/abs/2106.10199">zaken, ravfogel, &amp; goldberg, 2021</a>) - finetune only bias terms</p></li>
</ul>
</li>
</ul>
</li>
<li><p>adapter - finetune lightweight layers on top of pre-trained layers (between finetuning all layers, and just finetuning a new layer)</p>
<ul>
<li><p>add some new layers and retrain some specific things (all human choices)</p></li>
<li><p>side-tuning (<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-030-58580-8_41">zhang, sax‚Ä¶malik, 2020</a>) - train a ‚Äúside‚Äù network that is fused with the pretrained model via summation</p></li>
<li><p>Combining Modular Skills in Multitask Learning (<a class="reference external" href="https://arxiv.org/pdf/2202.13914.pdf">ponti, sordoni, bengio, &amp; reddy, 2022</a>) - learn adaptor with disentangled inventory of skills</p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v97/houlsby19a.html">Parameter-Efficient Transfer Learning for NLP</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2007.07779">AdapterHub: A Framework for Adapting Transformers</a></p></li>
</ul>
</li>
<li><p>predict a mask</p>
<ul>
<li><p>ablate some model weights by training a binary mask over model parameters (Zhao et al., 2020; Radiya-Dixit and Wang, 2020)</p></li>
<li><p>predict mask over attention heads</p></li>
</ul>
</li>
<li><p>prompting = few-shot learning = priming = in-context learning (starts with GPT)</p>
<ul>
<li><p>prompting without changing any model parameters</p>
<ul>
<li><p>limitation: can‚Äôt exploit sets longer than the training window</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.15943">MetaICL: Learning to Learn In Context</a> (min et al. 2022) - tune LLM to do in-context learning on a large set of training tasks (few-show prompting and training time and at test-time)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2209.00647">Visual Prompting via Image Inpainting</a> (bar‚Ä¶darrell, globerson, efros, 2022)</p></li>
<li><p>PatternExploiting Training (PET) ‚Äì Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference (<a class="reference external" href="https://aclanthology.org/2021.eacl-main.20.pdf">schick &amp; schutze, 2021</a>)</p>
<ul>
<li><p><strong>cloze questions</strong> - same as masked language modeling: task is to replace some missing words</p></li>
<li><p>use cloze-question templates (e.g. it was ‚Äúgood‚Äù or ‚Äúbad‚Äù) to get soft labels for unlabeled data and then finetune on theses</p></li>
</ul>
</li>
</ul>
</li>
<li><p>prompt-tuning (also see next section on autoprompting)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.11961">Attentional Mixtures of Soft Prompt Tuning for Parameter-efficient Multi-task Knowledge Sharing</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.08408">STT: Soft Template Tuning for Few-Shot Adaptation</a></p></li>
</ul>
</li>
</ul>
<p><strong>mt-dnn line of work</strong></p>
<ul class="simple">
<li><p>Multi-Task Deep Neural Networks for Natural Language Understanding (<a class="reference external" href="https://aclweb.org/anthology/papers/P/P19/P19-1441/">xiaodong liu ‚Ä¶ gao 2019</a>) - multi-task learning on the 9 glue tasks (first layers are shared, then some task-specific layers at top)</p></li>
<li><p>RAdam: On the Variance of the Adaptive Learning Rate and Beyond (<a class="reference external" href="https://openreview.net/pdf?id=rkgz2aEKDr">liyuan liu‚Ä¶gao, han, 2020</a>)</p>
<ul>
<li><p>usually need to do learning-rate warmup when trainin (e.g. with Adam)</p></li>
<li><p>RAdam = add a term to rectify the variance of the adaptive learning rate in Adam</p></li>
</ul>
</li>
<li><p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization (<a class="reference external" href="https://aclanthology.org/2020.acl-main.197/">jiang‚Ä¶gao, zhao, 2020</a>)</p>
<ol class="simple">
<li><p>Smoothness-inducing regularization, which effectively manages the complexity of the model</p></li>
<li><p>Bregman proximal point optimization to prevent aggressive updating</p></li>
</ol>
</li>
<li><p>Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding (<a class="reference external" href="https://aclanthology.org/2020.acl-demos.16/">xiaodong liu‚Ä¶gao, 2020</a>)</p></li>
<li><p>Posterior Differential Regularization with f-divergence for Improving Model Robustness (<a class="reference external" href="https://aclanthology.org/2021.naacl-main.85/">hao cheng, ‚Ä¶, gao 2021</a>)</p>
<ul>
<li><p>regularize model posterior difference between clean + noisy inputs (e.g. adversarially attacked inputs)</p></li>
</ul>
</li>
</ul>
</section>
<section id="pruning">
<h3><span class="section-number">1.9.1.4. </span>pruning<a class="headerlink" href="#pruning" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot (<a class="reference external" href="https://arxiv.org/abs/2301.00774">frantar &amp; alistarh, 2023</a>) - prune GPT-style models to atleast 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy</p></li>
<li><p>Cramming: Training a Language Model on a Single GPU in One Day (<a class="reference external" href="https://arxiv.org/abs/2212.14034">geiping &amp; goldstein, 2022</a>) - tricks for training BERT</p></li>
</ul>
</section>
</section>
<section id="prompting">
<h2><span class="section-number">1.9.2. </span>prompting<a class="headerlink" href="#prompting" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing (<a class="reference external" href="https://arxiv.org/pdf/2107.13586.pdf">liu‚Ä¶neubig, 2021</a>)</p>
<ul>
<li><p>from <em>feature-engineering</em> -&gt; <em>architecture engineering</em> -&gt; <em>prompt engineering</em></p></li>
<li><p><img alt="prompting_typology" src="../../_images/prompting_typology.png" /></p></li>
</ul>
</li>
<li><p>LAMA <a class="reference external" href="https://arxiv.org/abs/1909.01066">Language Models as Knowledge Bases?</a> (petroni‚Ä¶riedel, 2019) - Proposes using fill-in-the-blank (cloze) prompts for extracting knowledge from large language models</p>
<ul>
<li><p>create LAMA probe - dataset of (subject, relation, object) triplets with templates ‚Äì find that BERT can recall these relations</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2108.01928">How to Query Language Models?</a> (adolphs et al. 2021) - query LLMs by example (e.g. ‚ÄúRonaldo plays for Portugal. Who does Neuer play for?‚Äù)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1911.12543">How Can We Know What Language Models Know?</a> (jiang ‚Ä¶ neubig, 2020)</p>
<ul>
<li><p>mining-based and paraphrasing-based methods to automatically generate high-quality diverse prompts</p></li>
<li><p>ensemble methods to combine answers from different prompts (e.g. avg logits and more)</p></li>
</ul>
</li>
<li><p>Noisy Channel Language Model Prompting for Few-Shot Text Classification (<a class="reference external" href="https://arxiv.org/pdf/2108.04106.pdf">min et al. 2022</a>)</p>
<ul>
<li><p>Querying <span class="math notranslate nohighlight">\(P(question|answer)\)</span> with Bayes rule outperforms standard querying <span class="math notranslate nohighlight">\(P(answer|question)\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2201.06009">memory-assisted prompt-editing</a> (madaan‚Ä¶yang, 2022) - allows model to ‚Äúsave things to memory‚Äù that get added to prompt when needed</p></li>
<li><p>Prompting Is Programming: A Query Language For Large Language Models (<a class="reference external" href="https://arxiv.org/abs/2212.06094">Beurer-Kellner, Fischer, &amp; Vechev, 2022</a>)</p></li>
</ul>
<section id="autoprompting">
<h3><span class="section-number">1.9.2.1. </span>autoprompting<a class="headerlink" href="#autoprompting" title="Permalink to this headline">#</a></h3>
<p><img alt="prompting_hierarchy" src="../../_images/prompting_hierarchy.png" /></p>
<ul class="simple">
<li><p>natural-language prompting</p>
<ul>
<li><p>iPrompt: <a class="reference external" href="https://arxiv.org/abs/2210.01848">Explaining Patterns in Data with Language Models via Interpretable Autoprompting</a> (singh, morris, ‚Ä¶gao, 2022)</p></li>
<li><p>APE: <a class="reference external" href="https://arxiv.org/abs/2211.01910">Large Language Models Are Human-Level Prompt Engineers</a> (zhou‚Ä¶ba, 2022)</p>
<ul>
<li><p>similar to iPrompt, (1) propose prompt candidates with an LLM, (2) score the prompts by the accuracy they yield when using another LLM and (3) regenerate similar prompt candidates</p></li>
<li><p>experiments on instruction induction datasets + truthful QA</p></li>
</ul>
</li>
<li><p>FluentPrompt: Toward Human Readable Prompt Tuning (<a class="reference external" href="https://arxiv.org/abs/2212.10539">shi, ‚Ä¶, zettlemoyer, 2022</a>) - use langevin sampling + fluency constraint to generate prompt</p>
<ul>
<li><p>experiments relatively weak: 3 sentiment datasets + autoprompt is the only baseline</p></li>
</ul>
</li>
</ul>
</li>
<li><p>discrete prompting</p>
<ul>
<li><p><a class="reference external" href="https://aclanthology.org/2020.emnlp-main.346/">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a> (shin‚Ä¶sameer singh, 2020)</p>
<ul>
<li><p>select prompts from a fixed set of tokens (resulting prompts are not coherent)</p></li>
<li><p>only work on MLM</p></li>
<li><p>elicit sentiment / factual knowledge</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1908.07125">Universal Adversarial Triggers for Attacking and Analyzing NLP</a> (wallace‚Ä¶sameer singh, 2019) - find input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.15007">GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language</a> (zhu‚Ä¶james zou, 2022)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.12548">RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning</a> (deng‚Ä¶hu, 2022)</p></li>
<li><p>LM-BFF: <a class="reference external" href="https://arxiv.org/abs/2012.15723">Making Pre-trained Language Models Better Few-shot Learners</a> (gao et al. 2020)</p>
<ul>
<li><p>uses T5 to generate (i) template for the task (which might include a whole example or two) + (i) appropropriate label tokens in the vocabulary for the task (suffers from computationally intensive search + sub-optimal discrete space search)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2102.12206">PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains</a> (ben-david, ‚Ä¶, reichart, 2022)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2101.00190">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (li &amp; percy liang, 2021) ‚Äì optimizes in continuous space for language generation tasks</p>
<ul>
<li><p>learn to map some parameters <span class="math notranslate nohighlight">\(\theta\)</span> through and MLP to generate a starting hidden state <span class="math notranslate nohighlight">\(h_i\)</span> ‚Äì never actually sends the prefix through the network</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.08329">Control Prefixes for Parameter-Efficient Text Generation</a> (clive, cao, &amp; rei, 2022) - allow for adapting the prefix to each input example</p></li>
</ul>
</li>
<li><p>DART <a class="reference external" href="https://arxiv.org/abs/2108.13161">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a> (zhang‚Ä¶chen, 2022)</p>
<ul>
<li><p>reformulating NLP task into differentially optimizing the prompt template + target label (given a pre-trained model)</p></li>
<li><p>focus on smaller models (Roberta-large + GPT-2) + few training shots</p></li>
<li><p>fluency constraint to ensure association among prompt embeddings</p></li>
<li><p>P-Tuning ‚Äì <a class="reference external" href="https://arxiv.org/abs/2103.10385">GPT Understands, Too</a> (liu et al. 2021) ‚Äì use LSTM to generate prompt embeddings (don‚Äôt map to tokens)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2108.02035">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</a> (hu et al. 2021) ‚Äì add knowledge-base info into the prompt search</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2105.11259">PTR: Prompt Tuning with Rules for Text Classification</a> (han et al. 2021) ‚Äì use logic rules to construct prompts with sub-prompts for many-class text classification</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2104.06599">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</a> (qin &amp; eisner, 2021); <a class="reference external" href="https://github.com/hiaoxui/soft-prompts">github</a></p>
<ul>
<li><p>use continuous tokens and ensemble (don‚Äôt map back to words)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2101.00121">WARP: Word-level Adversarial ReProgramming</a> (Hambardzumyan et al. 2021) - add continous tokens (don‚Äôt map back to words) + some task-specific parameters for better generalization</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2104.07650">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</a> (chen et al. 2021) ‚Äì incorporate relations, visualize learned prompt vectors with t-SNE</p></li>
<li><p>misc</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2109.08306">SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis</a> ‚Äì use sentiment knowledge penalties in the prompt</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.07814">Meta-learning via Language Model In-context Tuning</a> (Chen et al. 2022) ‚Äì Given new task with new instruction</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2102.07350">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</a> (Reynolds &amp; McDonell, 2021) ‚Äì define metaprompts as general wrappers around tasks e.g. ‚ÄúThis problem asks us to‚Äù</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2210.06774">Re3: Generating Longer Stories With Recursive Reprompting and Revision</a> (yang, ‚Ä¶, klein, 2022) - generate summaries, then expand and revise with prompts</p></li>
</ul>
</li>
<li><p>critiques of prompting</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2109.01247">Do Prompt-Based Models Really Understand the Meaning of their Prompts?</a> (webson &amp; pavlick, 2022) - models can learn fine with prompts that are intentionally irrelevant</p>
<ul>
<li><p>Are Language Models Worse than Humans at Following Prompts? It‚Äôs Complicated (<a class="reference external" href="https://arxiv.org/abs/2301.07085">webson, ‚Ä¶, pavlick, 2023</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>can benefit from training for promptability</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2104.04670">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections</a> (zhong‚Ä¶klein, 2021)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2210.10258">Continued Pretraining for Better Zero- and Few-Shot Promptability</a> (wu‚Ä¶sameer singh, beltagy, 2022)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2211.15661">What learning algorithm is in-context learning? Investigations with linear models</a> - investigate prompting through synthetic experiments with transformers trained for linear regression</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2301.07067.pdf">Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning</a> (li, ‚Ä¶, oymak, 2023) - generalization bounds for in-context learning when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system</p></li>
</ul>
</li>
</ul>
</section>
<section id="llm-chaining-decoding">
<h3><span class="section-number">1.9.2.2. </span>llm chaining / decoding<a class="headerlink" href="#llm-chaining-decoding" title="Permalink to this headline">#</a></h3>
<p><strong>many notes are from this <a class="reference external" href="https://twitter.com/iraphas13/status/1551959289023016967">thread</a> on chaining models together</strong></p>
<ul class="simple">
<li><p>steering</p>
<ul>
<li><p>overviews</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.01691">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</a> (wu et al. 2022) - chaining LLM steps together: output of one step becomes the input for the next</p>
<ul>
<li><p>interactive system where users can modify chains + their intermediate results</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.10342">Language Model Cascades</a> (dohan‚Ä¶sutton, 2022) - treat chaining models as probabilistic programs</p>
<ul>
<li><p>use a probabilistic-programming language (PPL) to define a joint probability model on string-valued random variables, parameterized using LMs, and then condition this model on string-valued observations in order to compute a posterior over string-valued unknowns</p></li>
<li><p>self-PPLs extend probabilistic graphical models to support more complex joint distributions whose size and ‚Äúshape‚Äù can itself be stochastic</p>
<ul>
<li><p>e.g., a graph unrolled for a random number of iterations, until a data-dependent stopping criterion is met</p></li>
<li><p>variables are all text: questions <span class="math notranslate nohighlight">\(Q\)</span>, answers <span class="math notranslate nohighlight">\(A\)</span>, and intermediate thoughts <span class="math notranslate nohighlight">\(T\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>posthoc</p>
<ul>
<li><p>Chain of Thought Prompting (<a class="reference external" href="https://arxiv.org/abs/2201.11903">wei et al. 2022</a>)</p>
<ul>
<li><p>in few-shot prompts, don‚Äôt just provide answer but also reasoning</p></li>
<li><p>model output then provides reasoning + answer</p></li>
<li><p>Self-Consistency Improves Chain of Thought Reasoning in Language Models (<a class="reference external" href="https://arxiv.org/abs/2203.11171">wang, wei, schuurmans, quoc le, ‚Ä¶ zhou, 2022</a>) - sample a diverse set of reasoning paths from a language model via chain of thought prompting then return the most consistent final answer in the set</p></li>
<li><p>Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them (<a class="reference external" href="https://arxiv.org/abs/2210.09261">suzgun, ‚Ä¶, quoc le, ‚Ä¶, jason wei, 2022</a>)</p></li>
</ul>
</li>
<li><p>scratchpads <a class="reference external" href="https://arxiv.org/abs/2112.00114">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a> (nye et al. 2021)</p></li>
<li><p>selection inference (<a class="reference external" href="https://arxiv.org/abs/2205.09712">creswell et al. 2022</a>) - generate set of facts, then iteratively generate inferences from the facts to yield the final answer</p></li>
<li><p>least-to-most prompting (<a class="reference external" href="https://arxiv.org/abs/2205.10625">zhou‚Ä¶quoc le et al. 2022</a>) - prompt LLM with context showing how to reduce into subproblems; then LLM sequentially solves the subproblems, using the previous answers</p></li>
<li><p>Generated Knowledge Prompting for Commonsense Reasoning (<a class="reference external" href="https://arxiv.org/abs/2110.08387">liu‚Ä¶hasjishirzi, 2021</a>) - generate knowledge from an LLM then provide it as additional input when answering a question</p></li>
<li><p>maieutic prompting (<a class="reference external" href="https://arxiv.org/abs/2205.11822">jung et al. 2022</a>) - generate a tree of all explanation of the form ‚ÄúTrue, because‚Ä¶‚Äù, ‚ÄúFalse, because‚Ä¶‚Äù then query LLM with these as prompts</p>
<ul>
<li><p>then use Max-SAT to try to satisfy as many relations between the model explanations as possible to come up with the true answer</p></li>
</ul>
</li>
</ul>
</li>
<li><p>training</p>
<ul>
<li><p>verifiers (<a class="reference external" href="https://arxiv.org/abs/2110.14168">cobbe et al. 2021</a>) - train model to judge whether an answer and thought are likely to be ‚Äúvalid‚Äù</p></li>
<li><p>subgoal search (<a class="reference external" href="https://t.co/PCR4yexHti">czechowski et al. 2021</a>) - train model to generate subgoals then solve them in a graph</p></li>
<li><p>STaR ‚ÄúSelf-taught reasoner‚Äù (<a class="reference external" href="https://arxiv.org/abs/2203.14465">zelikman‚Ä¶goodman, 2022</a>)</p>
<ul>
<li><p>first, finetune on observed <span class="math notranslate nohighlight">\((Q, T, A)\)</span> triplets</p></li>
<li><p>then, impute unknown <span class="math notranslate nohighlight">\(T_i\)</span> given dataset of pairs <span class="math notranslate nohighlight">\((Q_i, A_i)\)</span> by sampling until finding a <span class="math notranslate nohighlight">\(T_i\)</span> which leads to the correct answer</p></li>
</ul>
</li>
</ul>
</li>
<li><p>robotics-specific</p>
<ul>
<li><p>zero-shot planning (<a class="reference external" href="https://arxiv.org/abs/2201.07207">huang, abbeel, pathak, &amp; mordatch, 2022</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.00598">socratic models</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.05608">Inner Monologue</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2103.01197">global workspace</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p>increasing attendable context size with augmented models</p>
<ul>
<li><p>RETRO (<a class="reference external" href="https://arxiv.org/abs/2112.04426">borgeaud et al. 2022</a>) - nearest neighbors to model‚Äôs input are retrieved, encoded, and conditioned on with chunked cross-attention</p></li>
<li><p>memorizing transformers (<a class="reference external" href="https://arxiv.org/abs/2203.08913">wu‚Ä¶szegedy, 2022</a>) - knn-based learned indexing + retrieval at training time.</p>
<ul>
<li><p>at test time, you just need to index the entire context and the model will be able to use it</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="misc">
<h2><span class="section-number">1.9.3. </span>misc<a class="headerlink" href="#misc" title="Permalink to this headline">#</a></h2>
<section id="direct-weight-inspection">
<h3><span class="section-number">1.9.3.1. </span>direct weight inspection<a class="headerlink" href="#direct-weight-inspection" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>nice paper list <a class="reference external" href="https://www.neelnanda.io/mechanistic-interpretability/favourite-papers">here</a></p></li>
</ul>
<p><strong><a class="reference external" href="https://transformer-circuits.pub/2021/framework/index.html">thread</a> (elhage‚Ä¶olah, 2021)</strong></p>
<ul class="simple">
<li><p>all layers are same dimension and each attention block <strong>adds</strong> a vector to it</p></li>
<li><p>Although they‚Äôre parameterized as separate matrices, <span class="math notranslate nohighlight">\(W_O W_V\)</span> and <span class="math notranslate nohighlight">\(W_Q^T W_K\)</span> can always be thought of as individual, low-rank matrices</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \in \mathbb R^{d_{embed} \times d_{sequence}}\)</span>: <span class="math notranslate nohighlight">\(d_{embed}\)</span> can be hundreds - tens of thousands</p></li>
<li><p><span class="math notranslate nohighlight">\(W_Q, W_K, W_V \in \mathbb R^{d_{attn} \times d_{embed}}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_Q^TW_k \in \mathbb R ^{d_{embed} \times d_{embed}}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(W_O \in \mathbb R^{d_{embed} \times d_{attn}}\)</span>: projects attention values back to embedding dimention</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_O W_V \in \mathbb R ^{d_{embed} \times d_{embed}}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(W_E \in \mathbb R^{d_{embed} \times d_{vocab}}\)</span> embeds initial tokens and <span class="math notranslate nohighlight">\(W_U \in \mathbb R^{d_{vocab} \times d_{embed}}\)</span> undoes the embedding</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d_{vocab}\)</span> can be very large, e.g. 50k</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(A = \text{softmax}(x^TW_Q^TW_kx) \in \mathbb R^{d_{sequence} \times d_{sequence}}\)</span></p></li>
</ul>
</li>
<li><p>if we have a 0-layer net (e.g. predict next token with linear layer given current token), we just learn bigram log-likelihood</p></li>
<li><p>2 circuits</p>
<ul>
<li><p>QK circuit determines which ‚Äúsource‚Äù token the present ‚Äúdestination‚Äù token attends back to and copies information from</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_{E}^{T} W_{Q}^{T} W_{K} W_{E} \in \mathbb R ^{d_{vocab} \times d_{vocab}}\)</span></p></li>
</ul>
</li>
<li><p>OV circuit describes what the resulting effect on the ‚Äúout‚Äù predictions for the next token is</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_{U} W_{O} W_{V} W_{E} \in \mathbb R ^{d_{vocab} \times d_{vocab}}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>if a single head increases the probability of both <code class="docutils literal notranslate"><span class="pre">keep‚Ä¶</span> <span class="pre">in</span> <span class="pre">mind</span></code> and <code class="docutils literal notranslate"><span class="pre">keep‚Ä¶</span> <span class="pre">at</span> <span class="pre">bay</span></code>, it <em>must</em> also increase the probability of <code class="docutils literal notranslate"><span class="pre">keep‚Ä¶</span> <span class="pre">in</span> <span class="pre">bay</span></code> and <code class="docutils literal notranslate"><span class="pre">keep‚Ä¶</span> <span class="pre">at</span> <span class="pre">mind</span></code></p></li>
<li><p><strong>induction heads</strong> search previous examples of present token</p>
<ul>
<li><p>If they don‚Äôt find it, they attend to the first token and do nothing</p></li>
<li><p>if they do find it, they then look at the <em>next</em> token and copy it. This allows them to repeat previous sequences of tokens, both exactly and approximately</p></li>
<li><p>sometimes can do some kind of ‚Äúfuzzy‚Äù matching</p></li>
</ul>
</li>
<li><p>tensor/kronecker product <span class="math notranslate nohighlight">\(\bigotimes\)</span>:</p>
<ul>
<li><p>Left-right multiplying: Multiplying <span class="math notranslate nohighlight">\(x\)</span> by a tensor product <span class="math notranslate nohighlight">\(A \otimes W\)</span> is equivalent to simultaneously left and right multiplying: <span class="math notranslate nohighlight">\((A \otimes W) x=A x W^{T}\)</span></p></li>
<li><p>When we add them, it is equivalent to adding the results of this multiplication: <span class="math notranslate nohighlight">\(\left(A_{1} \otimes W_{1}+A_{2} \otimes W_{2}\right) x=A_{1} x W_{1}^{T}+A_{2} x W_{2}^{T}\)</span></p></li>
</ul>
</li>
</ul>
<p><strong><a class="reference external" href="https://transformer-circuits.pub/2022/solu/index.html">Softmax Linear Units</a></strong></p>
<ul class="simple">
<li><p>replacing activation function with softmax linear unit increases fraction of MLP neurons which are ‚Äúinterpretable‚Äù, i.e. correspond to meaningful features</p>
<ul>
<li><p>however, may ‚Äúhide‚Äù some non-neuron-aligned features by decreasing their magnitude and then later recovering it with LayerNorm</p></li>
</ul>
</li>
<li><p>the presence of nonlinear activation functions createse an incentive for features to align with this basis and not get superposed</p>
<ul>
<li><p>if the gains to sparse coding are large enough, this incentive will get overwhelmed</p></li>
</ul>
</li>
<li><p>ways to combat polysemanticity</p>
<ul>
<li><p>activation sparsity</p></li>
<li><p>lateral inhibition / co-occurrence sparsity</p></li>
<li><p>weight sparsity</p></li>
<li><p>superlinear activation functions</p></li>
<li><p>increase neurons per param</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\text{SoLU}(x) = x \cdot \text{softmax}(x)\)</span></p>
<ul>
<li><p>adds lateral inhibition, superlinearity, approximate sparsity</p></li>
<li><p>changes GeLU, which is approximately <span class="math notranslate nohighlight">\(\text{sigmoid}(1.7x) \cdot x\)</span></p></li>
<li><p>just changing to SoLU decrease performance, had to add LayerNorm afterwards</p></li>
</ul>
</li>
<li><p>Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors (<a class="reference external" href="https://arxiv.org/abs/2103.15949">yun, chen, olshausen, lecun, 2021</a>) - investigate LLM embeddings of different words using dictionary learning</p>
<ul>
<li><p>LLMs produce interesting contextualized word embeddings</p></li>
<li><p>dictionary elements (of activations across layers) correspond to meaningful things</p></li>
</ul>
</li>
<li><p>A Circuit for Indirect Object Identification in GPT-2 small (<a class="reference external" href="https://arxiv.org/abs/2211.00593">wang, ‚Ä¶, steinhardt, 2022</a>)</p>
<ul>
<li><p>explanation encompasses 26 attention heads grouped into 7 main classes</p></li>
<li><p>task: indirect object identification - ‚ÄúWhen Mary and John went to the store, John gave a drink to ___‚Äù should be ‚ÄúMary‚Äù</p></li>
<li><p>circuit</p>
<ul>
<li><p>identify all previous names</p></li>
<li><p>remove duplicated names</p></li>
<li><p>output remaining name</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2211.07349">Finding Skill Neurons in Pre-trained Transformer-based Language Models</a> - some individual neurons are predictive of the final task (dubbed ‚Äúskill neurons‚Äô)</p></li>
</ul>
</section>
<section id="attention-variants">
<h3><span class="section-number">1.9.3.2. </span>attention variants<a class="headerlink" href="#attention-variants" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Tree Transformer: Integrating Tree Structures into Self-Attention (<a class="reference external" href="https://arxiv.org/pdf/1909.06639.pdf">wang, .., chen, 2019</a>)</p></li>
<li><p>Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform (<a class="reference external" href="https://arxiv.org/abs/2210.01989">zhuang‚Ä¶shang, 2022</a>)</p></li>
</ul>
</section>
<section id="editing">
<h3><span class="section-number">1.9.3.3. </span>editing<a class="headerlink" href="#editing" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Locating and Editing Factual Associations in GPT (<a class="reference external" href="https://arxiv.org/abs/2202.05262">meng, bau et al. 2022</a> )</p>
<ul>
<li><p><em>localize factual associations</em> - causal intervention for identifying neuron activations that are decisive in a model‚Äôs factual predictions</p>
<ul>
<li><p>‚Äúcausal traces‚Äù - run net multiple times, introducing corruptions and then restore states from original non-corrupted forward pass to see which states can restore the original results</p></li>
<li><p>a small number of states contain info that can flip the model from one state to another</p></li>
</ul>
</li>
<li><p><em>change factual associations</em> - modify feedforward weights to update specific factual associations using Rank-One Model Editing (ROME)</p></li>
<li><p><a class="reference external" href="https://memit.baulab.info/">Mass Editing Memory in a Transformer</a> (meng‚Ä¶, bau, 2022)</p></li>
</ul>
</li>
<li><p>Knowledge Neurons in Pretrained Transformers (<a class="reference external" href="https://arxiv.org/abs/2104.08696">dai et al. 2021</a>) - integrated gradients wrt to each neuron in BERT</p></li>
<li><p>Memory-Based Model Editing at Scale (<a class="reference external" href="https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf">mitchell‚Ä¶manning, finn, 2022</a>)</p>
<ul>
<li><p>keep track of list of edits in external memory and use them as appropriate context at test time (don‚Äôt finetune the model)</p></li>
<li><p>Fast model editing at scale (<a class="reference external" href="https://arxiv.org/abs/2110.11309">mitchell‚Ä¶finn, manning, 2022</a>)</p>
<ul>
<li><p>a collection of small auxiliary editing networks that use a single desired input-output pair to edit a pre-trained model</p></li>
<li><p>MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="debugging-interpretation">
<h3><span class="section-number">1.9.3.4. </span>debugging / interpretation<a class="headerlink" href="#debugging-interpretation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2207.04154">TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues</a> (slack‚Ä¶lakkaraju, sameer singh, 2022) - natural language interface to query model (by converting to commands such as filtering the data / calculating importance)</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2202.01875">Rethinking Explainability as a Dialogue: A Practitioner‚Äôs Perspective</a> (lakkaraju, slack, ‚Ä¶, sameer singh, 2022) - interviews with high-stakes users suggest they would like to be able to interact with systems via dialog</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.03401?context=cs">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</a> (ye &amp; durrett, 2022)</p></li>
<li><p>AdaTest <a class="reference external" href="https://aclanthology.org/2022.acl-long.230/">Adaptive Testing and Debugging of NLP Models</a> (ribeiro &amp; lundberg, 2022)</p>
<ul>
<li><p>goal: easily specify, discover, and fix undesirable behaviors in an NLP model</p></li>
<li><p>2-step iterative algorithm</p>
<ol class="simple">
<li><p>LLM generates many tests targeting the model‚Äôs failures</p>
<ul>
<li><p>example of a test: <code class="docutils literal notranslate"><span class="pre">f(‚ÄúI</span> <span class="pre">am</span> <span class="pre">a</span> <span class="pre">black</span> <span class="pre">woman‚Äù)</span> <span class="pre">‚â†</span> <span class="pre">neg</span></code></p></li>
<li><p>user selects and organizes the tests and reprompts the LLM to find more</p></li>
</ul>
</li>
<li><p>User fixes the tests (e.g. via finetuning)</p></li>
</ol>
</li>
<li><p>Checklist <a class="reference external" href="https://arxiv.org/abs/2005.04118">Beyond Accuracy: Behavioral Testing of NLP models with CheckList</a> (ribeiro‚Ä¶sameer singh, 2020)</p>
<ul>
<li><p>matrix of general linguistic capabilities + test types</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=B6wzhbPhsZ9">Fixing Model Bugs with Natural Language Patches</a> (murty, manning, lundberg, &amp; ribeiro 2022)</p>
<ul>
<li><p>specify patches with natural language rather than hard rule, allowing them to better handle text</p></li>
<li><p>finetune a model to combine original model output with output from a patch-conditioned interpreter head</p></li>
</ul>
</li>
</ul>
</section>
<section id="symbolic-reasoning">
<h3><span class="section-number">1.9.3.5. </span>symbolic reasoning<a class="headerlink" href="#symbolic-reasoning" title="Permalink to this headline">#</a></h3>
<p><em>See also notes on <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_comp_neuro.html">üìå comp neuro</a>.</em></p>
<ul class="simple">
<li><p>GPT-3 <a class="reference external" href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a> - simply adding ‚ÄúLet‚Äôs think step by step‚Äù before each answer increases the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with GPT-3</p></li>
<li><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8491571/">Compositional processing emerges in neural networks solving math problems</a> (russin, roland fernandez, ‚Ä¶, smolensky, gao, 2021)</p></li>
<li><p>neurocompositional computing (<a class="reference external" href="https://arxiv.org/abs/2205.01128">smolensky‚Ä¶gao, 2022</a>)</p>
<ul>
<li><p>longer tutorial (<a class="reference external" href="https://www.microsoft.com/en-us/research/uploads/prod/2022/04/Neurocompositional_computing__tutorial.pdf">smolensky, ‚Ä¶, gao, 2022</a>)</p></li>
<li><p><em>central paradox of cognition</em> is that brain both uses continuous neural symbols but is compositional (<a class="reference external" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.4352&amp;rep=rep1&amp;type=pdf">smolensky et al. 1992</a>)</p>
<ul>
<li><p>Compositionality</p></li>
<li><p>Continuity - the encoding and processing of information is formalized with real numbers that vary continuously</p></li>
</ul>
</li>
<li><p>3 challenges</p>
<ul>
<li><p>compositional generalization</p></li>
<li><p>data efficiency</p></li>
<li><p>comprehensibility</p></li>
</ul>
</li>
<li><p>solution - NECST: Neurally-Encoded Compositionally-Structured Tensor computing (<a class="reference external" href="https://psycnet.apa.org/record/2006-07970-000">smolensky &amp; legendre, 2006</a>) - basically leverages TPR</p>
<ul>
<li><p>TPR roles and fillers can both be made continuous</p></li>
</ul>
</li>
<li><p>neural space vs symbolic space (many different things (e.g. sentences) can mean the same thing)</p>
<ul>
<li><p>word vectors can be thought of as ‚Äúsoft symbols‚Äù</p></li>
</ul>
</li>
<li><p>want to move from symbolic repr. to neural repr. while keeping interpretability</p>
<ul>
<li><p>system should output intermediate steps in addition to answer</p></li>
<li><p>thinking fast (system 1: fast, intuitive) + slow (system 2: slower, logical, derivative)</p></li>
</ul>
</li>
<li><p>concrete proposals</p>
<ul>
<li><p>transformer activation vector should encode graph of flow through the network</p>
<ul>
<li><p>ex. task: regurgitate a sequence</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>TPR: Tensor product variable binding and the representation of symbolic structures in connectionist systems (<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/000437029090007M?via%3Dihub">paul smolensky, 1990</a>) - activation patterns are ‚Äúsymbols‚Äù and internal structure allows them to be processed like symbols</p>
<ul>
<li><p>tensor product representation = TPR</p></li>
<li><p><a class="reference external" href="https://www.mit.edu/~jda/teaching/6.884/slides/oct_02.pdf">TPR slides</a></p></li>
<li><p>TPR of a structure is the sum of the TPR of its constituents</p>
<ul>
<li><p>tensor product operation allows constituents to be uniquely identified, even after the sum (if roles are linearly independent)</p></li>
</ul>
</li>
<li><p><strong>filler</strong> - one vector that embeds the content of the constituent</p></li>
<li><p><strong>role</strong> - second vector that embeds the structural role it fills</p></li>
</ul>
</li>
<li><p>NECSTransformer: <a class="reference external" href="https://www.microsoft.com/en-us/research/publication/enhancing-the-transformer-with-explicit-relational-encoding-for-math-problem-solving/">Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving</a> (schlag, ‚Ä¶, gao, 2019)</p></li>
<li><p>TP-attention</p></li>
<li><p>beat SOAon free-form math word-problems</p></li>
<li><p>in addition to K, Q, V, also add a role-vector</p>
<ul>
<li><p>do element-wise multiplication of outputted vector with role-vector</p></li>
</ul>
</li>
<li><p>TPR built as tensor product of 2 vectors:</p>
<ul>
<li><p>filler - the vector returned by attention</p>
<ul>
<li><p>ex. one head learns ‚Äúsecond-argument-of‚Äù</p></li>
</ul>
</li>
<li><p>role - a relation conceptually labeling an edge of the attention graph</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/research/publication/natural-to-formal-language-generation-using-tensor-product-representations/">TP-N2F: Tensor Product Representation for Natural To Formal Language Generation - Microsoft Research</a> (chen‚Ä¶gao, 2019)</p></li>
</ul>
</section>
<section id="sparse-experts-ensembles-mixture-of-experts-moe">
<h3><span class="section-number">1.9.3.6. </span>sparse experts / ensembles / mixture of experts (MoE)<a class="headerlink" href="#sparse-experts-ensembles-mixture-of-experts-moe" title="Permalink to this headline">#</a></h3>
<p>mixture of experts models have become popular because of the need for (1) fast speed / low memory at test time while still (2) having a large model during training</p>
<ul class="simple">
<li><p>note: nowadays often the ‚Äúexperts‚Äù are different MLPs following the self-attention layers</p></li>
<li><p>A Review of Sparse Expert Models in Deep Learning (<a class="reference external" href="https://arxiv.org/abs/2209.01667">fedus, jeff dean, zoph, 2022</a>)</p>
<ul>
<li><p>sparsity decouples the parameter count from the compute per example allowing for extremely large, but efficient models</p></li>
<li><p>routing algorithm - determines where to send examples</p>
<ul>
<li><p>discreteness makes it difficult</p>
<ul>
<li><p>some works use RL to learn routing</p></li>
<li><p>standard approach uses gumbel-softmax</p></li>
<li><p>usually get matrix of similarities between input tokens and experts and route based on these</p>
<ul>
<li><p>sometimes route to topk experts rather than top1</p></li>
</ul>
</li>
</ul>
</li>
<li><p>load balancing - usually add an auxiliary loss to encourage equal tokens being sent to different experts</p></li>
</ul>
</li>
</ul>
</li>
<li><p>non-specialized experts</p>
<ul>
<li><p>Early versions (<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6797059">Jacobs, michael jordan, nowlan, &amp; hinton, 1991</a>) had independent feed-forward networks serving as experts</p></li>
<li><p>Sparsely-gated MOE layer (<a class="reference external" href="https://arxiv.org/abs/1701.06538">Shazeer‚Ä¶quoc le, hinton, dean, 2017</a>) have been studied with token-based routing with backprop</p></li>
<li><p>replace FFN in transformers with expert layers</p>
<ul>
<li><p>GShard <a class="reference external" href="https://arxiv.org/abs/2006.16668">Lepikhin et al. (2021)</a>, which appplies this concept to machine translation</p></li>
<li><p>Switch transformers (<a class="reference external" href="https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf">Fedus et al. (2022)</a>) simplifies the architecture to activation of only one expert per layer</p></li>
</ul>
</li>
<li><p>BASE Layers <a class="reference external" href="https://proceedings.mlr.press/v139/lewis21a.html">Lewis et al. (2021)</a> - find an alternative approach to routing by formulating it as a linear assignment problem</p></li>
<li><p>Hash layers <a class="reference external" href="https://arxiv.org/abs/2106.04426">Roller et al. (2021)</a> use a fixed hash as the gating function</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.sscardapane.it/assets/files/nnds2022/Lecture_8_Dynamic_NNs.pdf">routing notes</a> - make hard decision but still want to learn probabilities</p>
<ul>
<li><p>straight-through estimator (STE) - take the argmax during the forward pass, while considering the orig- inal probabilities in the backward pass</p>
<ul>
<li><p>highly biased</p></li>
</ul>
</li>
<li><p>gumbel-softmax- allows for better sampling</p></li>
</ul>
</li>
<li><p>specialized experts as fully independent models (sometimes for multi-task learning)</p>
<ul>
<li><p>DEmix Layers <a class="reference external" href="https://arxiv.org/abs/2108.05036">Gururangan et al.</a> (2022) ‚Äì  DEMix layers ‚Äì placed in the feedforward layers of the Transformer ‚Äì contain experts which specialize on specific domains. Routing at train time is determined only by the domain label, but all experts are activated at inference time and mixed according to weights estimated from a validation set</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.07689">Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners</a> (gupta‚Ä¶awadallah, gao, 2022) - use task description to improve routing</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.06266">Pfeiffer et al. (2022)</a> - multilingual expert model with language-specific routing</p></li>
<li><p>task-level MoE <a class="reference external" href="https://arxiv.org/abs/2110.03742">Kudugunta et al. (2021</a>) ‚Äì multi-task expert model with task-specific routing</p></li>
<li><p>ELMS ‚Äì Branch-Train-Merge (<a class="reference external" href="https://arxiv.org/abs/2208.03306">li et al. 2022</a>)</p>
<ul>
<li><p>parallel language model of smaller expert LMs</p></li>
<li><p>each can be added/removed, ensembled, or parameter-averaged at any time for efficient scaling and rapid customization</p></li>
<li><p>improves perplexities, when controlling for training cost</p>
<ul>
<li><p>require expert domain specialization</p></li>
</ul>
</li>
</ul>
</li>
<li><p>scaling up</p>
<ul>
<li><p>OPT-MOE (<a class="reference external" href="https://arxiv.org/abs/2112.10684">artetxe et al. 2021</a>)</p></li>
<li><p>AutoMoE (<a class="reference external" href="https://arxiv.org/abs/2210.07535">jawahar, mukherjee, liu‚Ä¶gao, 2022</a>)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.02813">Towards Understanding Mixture of Experts in Deep Learning</a> (chen‚Ä¶gu, li, 2022)</p></li>
<li><p>ensembles (some of these are non-transformer papers)</p>
<ul>
<li><p>model soups (<a class="reference external" href="https://proceedings.mlr.press/v162/wortsman22a.html">wortsman‚Ä¶schmidt, 20221</a>) - average weights of finetuned models</p>
<ul>
<li><p>snapshot ensembles - average different checkpoints during training (<a class="reference external" href="https://arxiv.org/abs/1704.00109">huang et al. 2017</a>)</p></li>
<li><p>stochastic weight averaging (<a class="reference external" href="https://arxiv.org/abs/1803.05407v3">izmailov, ‚Ä¶, wilson, 2019</a>) - average multiple checkpoints during training</p></li>
<li><p>batch ensemble (<a class="reference external" href="https://arxiv.org/pdf/2002.06715.pdf">wen et al. 2020</a>) - have several rank-1 keys that index different weights hidden within one neural net</p></li>
<li><p>Editing Models with Task Arithmetic (<a class="reference external" href="https://arxiv.org/abs/2212.04089">ilharco, rebeiro, wortsman‚Ä¶farhadi, 2022</a>) - add task vectors (difference between weights before/after finetuning on a task) for different tasks to induce different multi-task behaviors</p></li>
</ul>
</li>
<li><p>fit many models into one</p>
<ul>
<li><p>superposition of many models into one (<a class="reference external" href="https://proceedings.neurips.cc/paper/2019/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html">cheung‚Ä¶olshausen, 2019</a>) - both during training/testing models are indexed via a high-dim key for each task</p></li>
<li><p>supermasks in superposition (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html">wortsman, ‚Ä¶, yosinski, farhadi, 2020</a>) - randomly fixed based net + for each task finds subnet that chieves good performance</p>
<ul>
<li><p>if task identity not given, correct subnet inferred by minimizing output entropy</p></li>
</ul>
</li>
<li><p>Git Re-Basin: Merging Models modulo Permutation Symmetries (<a class="reference external" href="https://arxiv.org/abs/2209.04836">ainsworth, hayase, &amp; srinivasa, 2022</a>) - algo to merge models even when they haven‚Äôt been pretrained together</p></li>
</ul>
</li>
<li><p>early exit - popular way to speed up inference</p>
<ul>
<li><p>Multi-exit vision transformer for dynamic inference (<a class="reference external" href="https://arxiv.org/abs/2106.15183">Bakhtiarnia, A., Zhang, Q. and Iosifidis, A., 2021</a>)</p>
<ul>
<li><p>early layers have large activation map so early exist classifier must be complex</p></li>
<li><p>solution: ViT class token allows early-exit classifier to have constant complexity</p></li>
</ul>
</li>
<li><p>DeeBERT: Dynamic early exiting for accelerating BERT inference (<a class="reference external" href="https://arxiv.org/abs/2004.12993">xin‚Ä¶lin, 2020</a>)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="llm-querying-causal-inference">
<h3><span class="section-number">1.9.3.7. </span>llm querying / causal inference<a class="headerlink" href="#llm-querying-causal-inference" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>decoding</p>
<ul>
<li><p>greedy - iteratively pick highest-probability token</p></li>
<li><p>nucleus sampling: <a class="reference external" href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration</a> (holtzman‚Ä¶choi, 2019)</p></li>
<li><p>contrastive decoding (<a class="reference external" href="https://arxiv.org/abs/2210.15097">li et al. 2022</a>) - decode based on the difference between a large and small LLM</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2212.03827">Discovering Latent Knowledge in Language Models Without Supervision</a> (burns, ye, klein, &amp; steinhardt, 2022) - identify whether text is true or false directly from a model‚Äôs <em>unlabeled activations</em></p></li>
<li><p><a class="reference external" href="https://www.frontiersin.org/articles/10.3389/frai.2021.659622/full">InferBERT: A Transformer-Based Causal Inference Framework for Enhancing Pharmacovigilance</a> (wang‚Ä¶liu, 2021) - learn + test feature relationships from attention weights</p></li>
<li><p><a class="reference external" href="https://direct.mit.edu/coli/article/47/2/333/98518/CausaLM-Causal-Model-Explanation-Through">CausaLM: Causal Model Explanation Through Counterfactual Language Models</a> (2021) - produce example-level causal model explanations using models finetuned on auxiliary adversarial tasks derived from the causal graph of the problem</p></li>
<li><p>Investigating Gender Bias in Language Models Using Causal Mediation Analysis (<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf">vig, ‚Ä¶, shieber, 2020</a>)</p>
<ul>
<li><p>Applies causal mediation analysis to identify decisive neurons and attention heads responsible for gender bias in large language models</p></li>
<li><p>Identifies a small handful of decisive attention heads in this case</p></li>
</ul>
</li>
<li><p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals (<a class="reference external" href="https://arxiv.org/pdf/2006.00995.pdf">elazar, ‚Ä¶, goldberg, 2021</a>) - measure the importance of specific info within a model by introducing a causal intervention to erase that information, then observing the causal effects</p></li>
</ul>
</section>
<section id="dataset-explanation">
<h3><span class="section-number">1.9.3.8. </span>dataset explanation<a class="headerlink" href="#dataset-explanation" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2210.01848">iPrompt: Explaining Patterns in Data with Language Models via Interpretable Autoprompting</a> (singh, morris, ‚Ä¶gao, 2022) - prompting approach</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.10782">Instruction Induction: From Few Examples to Natural Language Task Descriptions</a> (honovich‚Ä¶bowman, levy 2022) - directly query model with prompt to search for task description</p></li>
<li><p>Describing Differences between Text Distributions with Natural Language (<a class="reference external" href="https://arxiv.org/abs/2201.12323">zhong, snell, klein, &amp; steinhardt, 2022</a>)</p>
<ul>
<li><p>finetune a model to directly describe difference between 2 distrs</p></li>
<li><p>technically this is just learning a classifier, where the classifier is a natural-language string</p></li>
<li><p>method</p>
<ul>
<li><p>proposer network generates hypotheses</p>
<ul>
<li><p>verifier networks looks at all samples in the dataset (since proposer couldn‚Äôt fit them all in context) and returns how accurate the hypotheses were</p></li>
<li><p>some tricks</p>
<ul>
<li><p>select samples which are ‚Äúrepresentative‚Äù of a class by predicting with another LLM</p></li>
<li><p>have a pool of 302 manual hypotheses they used for seeding</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.15007">GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language</a> (zhu‚Ä¶james zou, 2022)</p></li>
</ul>
</li>
</ul>
</section>
<section id="cool-tasks">
<h3><span class="section-number">1.9.3.9. </span>cool tasks<a class="headerlink" href="#cool-tasks" title="Permalink to this headline">#</a></h3>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.15474">Forecasting Future World Events with Neural Networks</a> (zou‚Ä¶hendrycks, 2022) - takes tasks from metaculus</p></li>
<li><p>forecasting paper titles (<a class="reference external" href="https://csinva.io/gpt-paper-title-generator/">blog post</a>)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.11857">Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey</a> (du et al. 2022)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2210.05050">Neurosymbolic Programming for Science</a> (sun‚Ä¶costilla-reyes, 2022)</p></li>
<li><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1359644621002816">AI-based language models powering drug discovery and development</a> (liu et al. 2021)</p></li>
<li><p>scientific organization (<a class="reference external" href="https://galactica.org/static/paper.pdf">galactica</a>)</p>
<ul>
<li><p>related but smaller models</p>
<ul class="simple">
<li><p>SciBERT (<a class="reference external" href="https://arxiv.org/abs/1903.10676">beltagy‚Ä¶cohan, 2019</a>)</p></li>
<li><p>BioLM (<a class="reference external" href="https://aclanthology.org/2020.clinicalnlp-1.17/">lewis‚Ä¶stoyanov, 2020</a>)</p></li>
<li><p>ScholarBERT (<a class="reference external" href="https://arxiv.org/abs/2205.11342">hong‚Ä¶foster, 2022</a>) - large dataset, 770M-param model</p></li>
</ul>
</li>
<li><p>all data is processed in a common markdown format</p>
<ul class="simple">
<li><p>task-specific tokens to support different types of knowledge (e.g. citations, step-by-step reasoning, different modalities, e.g. proteins)</p></li>
</ul>
</li>
<li><p>chemical compounds (train on 2 mil / 110 mil from PubChem Compound, authors still want it to focus on text)</p>
<ul>
<li><p>predict IUPAC name from SMILES formula e.g. <code class="docutils literal notranslate"><span class="pre">CC(C)(C)C(=O)N(CC1=NC(=CS1)C(=O)OC)C2CCCCC2</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">methyl</span> <span class="pre">2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino]</span> <span class="pre">methyl]thiazole-4-</span> </code></p></li>
<li><p><a class="reference external" href="https://moleculenet.org/datasets-1">moleculenet</a> (<a class="reference external" href="https://arxiv.org/abs/1703.00564">wu et al. 2017</a>) classification benchmark (6 tasks)</p>
<ul>
<li><p>training set examples are trained as text during fitting</p>
<ul class="simple">
<li><p>HIV - classify whether comopund inhibits HIV replication</p></li>
<li><p>BACE C - binding results (classification + regression) for BACE</p></li>
<li><p>BBBP - blood-brain barrier penetration(permeability) (binary classification)</p></li>
<li><p>Tox21 - qualitative toxicity on 12 targets (12-class multilabel binary)</p></li>
<li><p>SIDER - 27-class multi-class disorders in different organ systems</p></li>
<li><p>ClinTox - binary toxicity classification</p></li>
</ul>
</li>
<li><p>ex. for BBBP (one of the 6 tasks) - question is posed in different ways during training</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Here is a SMILES formula:   
   [START_I_SMILES]O=C(O)CCCC1=CC=C(N(CCCl)CCCl)C=C1[END_I_SMILES]
   
Question: Will the chemical compound penetrate the blood-brain barrier?
Answer: No
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p>protein sequences</p>
<ul class="simple">
<li><p>from 227 million in UniProt, look at only 0.5 million subset (called Swiss-Prot)</p></li>
<li><p>evaluate protein sequence perplexity</p></li>
<li><p>protein keyword prediction (predict keywords in UniProt, like ‚ÄúATP-Binding‚Äù, ‚ÄúCell membrane‚Äù)</p></li>
<li><p>protein function description - compare free-form description to GT UniProt function description</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="llm-limitations-perspectives">
<h3><span class="section-number">1.9.3.10. </span>llm limitations / perspectives<a class="headerlink" href="#llm-limitations-perspectives" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>Dissociating language and thought in large language models: a cognitive perspective (<a class="reference external" href="https://arxiv.org/pdf/2301.06627.pdf">mahowald, ‚Ä¶, tenenbaum, fedorenko, 2023</a>)</p>
<ul>
<li><p>2 competences</p>
<ol class="simple">
<li><p>formal linguistic competence - knowledge of rules and patterns of a given language</p></li>
<li><p>functional linguistic competence - cognitive abilities required for language understanding and use in the real world, e.g. formal reasoning, world knowledge, situation modeling, communicative intent</p>
<ul class="simple">
<li><p>much of world knowledge is implied: people are much more likely to communicate new or unusual information rather than commonly known facts</p></li>
</ul>
</li>
</ol>
<ul class="simple">
<li><p>language and thought are robustly dissociable in the brain</p>
<ul>
<li><p>aphasia studies: despite the nearly complete loss of linguistic abilities, some individuals with severe aphasia have intact non-linguistic cognitive abilities: they can play chess, compose music, solve arithmetic problems and logic puzzles, ‚Ä¶</p></li>
<li><p>fMRI studies: the language network is extremely selective for language processing: it responds robustly and reliably when people listen to, read, or generate sentences , but not when they perform arithmetic tasks, engage in logical reasoning, understand computer programs, listen to music, ‚Ä¶</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2209.12407">Merrill et al. [2022]</a> - semantic information is in-principle learnable from language data</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.02957">Piantadosi and Hill [2022]</a> - an argument that models can genuinely learn meaning</p></li>
<li><p>Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks (<a class="reference external" href="http://arxiv.org/abs/2205.05718">collins‚Ä¶tenebaum, 2022</a>)</p>
<ul class="simple">
<li><p>as situation gets more OOD, LLM gets worse compared to human, e.g. <code class="docutils literal notranslate"><span class="pre">Get</span> <span class="pre">your</span> <span class="pre">sofa</span> <span class="pre">onto</span> <span class="pre">the</span> <span class="pre">roof</span> <span class="pre">of</span> <span class="pre">your</span> <span class="pre">house,</span> <span class="pre">without</span> <span class="pre">using</span> <span class="pre">a</span> <span class="pre">pulley,</span> <span class="pre">a</span> <span class="pre">ladder,</span> <span class="pre">a</span> <span class="pre">crane...</span></code></p></li>
</ul>
</li>
<li><p>recommendations</p>
<ul class="simple">
<li><p>modularity, curated data / diverse objectives, new benchmarks</p></li>
</ul>
</li>
</ul>
</li>
<li><p>speculative <a class="reference external" href="https://arxiv.org/abs/2108.07258">foundation models paper</a> (stanford, 2022)</p></li>
</ul>
</section>
</section>
<section id="basics">
<h2><span class="section-number">1.9.4. </span>basics<a class="headerlink" href="#basics" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>attention</strong> = vector of importance weights</p>
<ul>
<li><p>to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or ‚Äú<em>attends to</em>‚Äù other elements and take the sum of their values weighted by the attention vector as the approximation of the target</p></li>
</ul>
</li>
<li><p>vanilla transformer: multihead attention, add + norm, position-wise ffn, add + norm</p></li>
<li><p>self-attention layer <a class="reference external" href="https://github.com/mertensu/transformer-tutorial">implementation</a>, <a class="reference external" href="https://homes.cs.washington.edu/~thickstn/docs/transformers.pdf">mathematics</a>, and <strong>chandan‚Äôs self-attention <a class="reference external" href="https://slides.com/chandansingh-2/deck-51f404">cheat-sheet</a></strong></p></li>
</ul>
<section id="mathematical-overview-of-transformers-formal-algorithms-for-transformers">
<h3><span class="section-number">1.9.4.1. </span>mathematical overview of transformers (<a class="reference external" href="https://arxiv.org/abs/2207.09238?utm_source=substack&amp;utm_medium=email">Formal Algorithms for Transformers</a>)<a class="headerlink" href="#mathematical-overview-of-transformers-formal-algorithms-for-transformers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>tasks</p>
<ul>
<li><p><em>sequence modeling</em>: learn <span class="math notranslate nohighlight">\(p(x)\)</span>, usually factorized as <span class="math notranslate nohighlight">\(p(x_i|x_1,...,x_{i-1})\)</span></p></li>
<li><p><em>sequence-to-sequence</em>: learn <span class="math notranslate nohighlight">\(p(z|x)\)</span>, e.g. transalation, speech-to-text, question answering</p></li>
</ul>
</li>
<li><p>preprocessing</p>
<ul>
<li><p>embedding matrix takes in one-hot tokens and linearly maps them to a vector</p></li>
<li><p>positional embedding of a token is usually added to the token embedding to form a token‚Äôs initial embedding</p></li>
</ul>
</li>
<li><p>attention types</p>
<ul>
<li><p><em>Bidirectional / unmasked self-attention</em> - primary/context vectors are the same</p></li>
<li><p><em>Unidirectional / masked self-attention</em> - mask scores from before a given word</p></li>
<li><p><em>Cross-attention</em> - primary/context vectors can come from different places</p></li>
</ul>
</li>
<li><p>non-attention</p>
<ul>
<li><p>layernorm: controls mean/variance of activations</p>
<ul>
<li><p>RMSnorm: simpler version, sets mean/offset to zero</p></li>
</ul>
</li>
</ul>
</li>
<li><p>unembedding</p>
<ul>
<li><p>linear layer (with softmax) that outputs size of original vocab</p>
<ul>
<li><p>sometimes fixed to be transpose of the embedding matrix</p></li>
</ul>
</li>
</ul>
</li>
<li><p>predictions</p>
<ul>
<li><p>predict next word using single linear layer on hidden state from previous word</p></li>
<li><p>finetune classification head often only using linear layer on first token from sequence</p></li>
</ul>
</li>
<li><p>architectures</p>
<ul>
<li><p>initially, encoder-decoder was common, but now often no decoder</p></li>
</ul>
</li>
</ul>
</section>
<section id="visual-explanation-notes-on-article-by-jay-allamar">
<h3><span class="section-number">1.9.4.2. </span>visual explanation (notes on article by jay allamar)<a class="headerlink" href="#visual-explanation-notes-on-article-by-jay-allamar" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>**self-attention ** - layer that lets word learn its relation to other layers</p>
<ul>
<li><p>for each word, want score telling how much importance to place on each other word (queries <span class="math notranslate nohighlight">\(\cdot\)</span> keys)</p></li>
<li><p>we get an encoding for each word</p>
<ul>
<li><p>the encoding of each word returns a weighted sum of the values of the words (the current word gets the highest weight)</p></li>
<li><p>softmax this and use it to do weighted sum of values<img alt="Screen Shot 2019-08-17 at 2.51.53 PM" src="../../_images/attention.png" /></p></li>
</ul>
</li>
<li><p>(optional) implementation details</p>
<ul>
<li><p><strong>multi-headed attention</strong> - just like having many filters, get many encodings for each word</p>
<ul>
<li><p>each one can take input as the embedding from the previous attention layer</p></li>
</ul>
</li>
<li><p><strong>position vector</strong> - add this into the embedding of each word (so words know how far apart they are) - usually use sin/cos rather than actual position number</p></li>
<li><p><strong>padding mask</strong> - add zeros to the end of the sequence</p></li>
<li><p><strong>look-ahead mask</strong> - might want to mask to only use previous words (e.g. if our final task is decoding)</p></li>
<li><p><strong>residual + normalize</strong> - after self-attention layer, often have residual connection to previous input, which gets added then normalized</p></li>
</ul>
</li>
<li><p>decoder - each word only allowed to attend to previous positions</p></li>
<li><p>3 components</p>
<ul>
<li><p>queries</p></li>
<li><p>keys</p></li>
<li><p>values</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>attention</strong></p>
<ul>
<li><p>encoder reads input and ouputs context vector after each word</p></li>
<li><p>decoder at each step uses a different weighted combination of these context vectors</p>
<ul>
<li><p>specifically, at each step, decoder concatenates its hidden state w/ the attention vector (the weighted combination of the context vectors)</p></li>
<li><p>this is fed to a feedforward net to output a word</p></li>
<li><p><img alt="Screen Shot 2019-04-11 at 7.57.14 PM" src="../../_images/nmt.png" /></p></li>
</ul>
</li>
<li><p>at a high level we have <span class="math notranslate nohighlight">\(Q, K, V\)</span> and compute <span class="math notranslate nohighlight">\(\text{softmax}(QK^T)V\)</span></p>
<ul>
<li><p>instead could simplify it and do <span class="math notranslate nohighlight">\(\text{softmax}(XX^T)V\)</span> - this would then be based on kernel</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>transformer</strong></p>
<ul>
<li><p>uses many self-attention layers</p></li>
<li><p>many stacked layers in encoder + decoder (not rnn: self-attention + feed forward)</p></li>
<li><p>details</p>
<ul>
<li><p>initial encoding: each word -&gt; vector</p></li>
<li><p>each layer takes a list of fixed size (hyperparameter e.g. length of longest sentence) and outputs a list of that same fixed size (so one output for each word)</p>
<ul>
<li><p>can easily train with a masked word to predict the word at the predicted position in the encoding</p></li>
</ul>
</li>
</ul>
</li>
<li><p>multi-headed attention has several of each of these (then just concat them)</p></li>
</ul>
</li>
</ul>
</section>
<section id="huggingface-tutorial">
<h3><span class="section-number">1.9.4.3. </span>huggingface tutorial<a class="headerlink" href="#huggingface-tutorial" title="Permalink to this headline">#</a></h3>
<p>Broadly, models can be grouped into three categories:</p>
<ul class="simple">
<li><p>GPT-like (also called <em>auto-regressive</em> Transformer models)</p></li>
<li><p>BERT-like (also called <em>auto-encoding</em> Transformer models)</p></li>
<li><p>BART/T5-like (also called <em>sequence-to-sequence</em> Transformer models)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/course/chapter2/5?fw=pt">Handling multiple sequences - Hugging Face Course</a></p>
<ul>
<li><p>pad sequences to have the same length (need to modify attention masks to ignore the padded values)</p></li>
</ul>
</li>
</ul>
<section id="pre-transformer-nlp-models">
<h4><span class="section-number">1.9.4.3.1. </span>pre-transformer nlp models<a class="headerlink" href="#pre-transformer-nlp-models" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>rnns</p>
<ul>
<li><p>when training rnn, accumulate gradients over sequence and then update all at once</p></li>
<li><p><strong>stacked rnns</strong> have outputs of rnns feed into another rnn</p></li>
<li><p>bidirectional rnn - one rnn left to right and another right to left (can concatenate, add, etc.)</p></li>
</ul>
</li>
<li><p>standard seq2seq</p>
<ul>
<li><p>encoder reads input and outputs context vector (the hidden state)</p></li>
<li><p>decoder (rnn) takes this context vector and generates a sequence</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes/research_ovws"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ovw_ml_medicine.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1.8. </span>ml in medicine</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ovw_causal_inference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.10. </span>causal inference</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Chandan Singh<br/>
  
      &copy; Copyright None.<br/>
    <div class="extra_footer">
      <p>
Many of these images are taken from resources on the web.
</p>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>