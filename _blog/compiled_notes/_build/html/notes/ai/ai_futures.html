
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1.5. ai futures</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/ai/ai_futures';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.6. cognitive science" href="cogsci.html" />
    <link rel="prev" title="1.4. philosophy" href="philosophy.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="ai.html">1. ai</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="knowledge_rep.html">1.1. representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="psychology.html">1.2. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="fairness_sts.html">1.3. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="philosophy.html">1.4. philosophy</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">1.5. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="cogsci.html">1.6. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="llms.html">1.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="logic.html">1.8. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="search.html">1.9. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="decisions_rl.html">1.10. decisions, rl</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/math.html">2. math</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/linear_algebra.html">2.1. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/optimization.html">2.2. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/differential_equations.html">2.3. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/chaos.html">2.4. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/math_basics.html">2.5. math basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/signals.html">2.6. signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/calculus.html">2.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/proofs.html">2.8. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/analysis.html">2.9. real analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml/ml.html">3. ml</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml/unsupervised.html">3.1. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/structure_ml.html">3.2. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/learning_theory.html">3.3. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/deep_learning.html">3.4. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/comp_vision.html">3.5. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/kernels.html">3.6. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/nlp.html">3.7. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/feature_selection.html">3.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/evaluation.html">3.9. evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/classification.html">3.10. classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stat/stat.html">4. stat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stat/time_series.html">4.1. time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/graphical_models.html">4.2. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/causal_inference.html">4.3. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/game_theory.html">4.4. game theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/data_analysis.html">4.7. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/testing.html">4.8. testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">5. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">5.1. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">5.2. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">5.3. development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">5.4. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">5.5. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">5.6. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">5.7. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">5.8. brain basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">6. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">6.1. cs theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">6.2. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">6.3. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">6.4. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">6.5. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">6.6. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">6.7. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">6.8. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">6.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">6.10. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">6.11. reproducibility</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../research_ovws/research_ovws.html">7. research_ovws</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_disentanglement.html">7.1. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_uncertainty.html">7.2. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_generalization.html">7.3. generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_causal_inference.html">7.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_omics.html">7.5. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">7.6. ml in medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_llms.html">7.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">7.8. transfer learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interp.html">7.9. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_complexity.html">7.10. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_dl_theory.html">7.11. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_scat.html">7.12. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interesting_science.html">7.13. interesting science</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/ai/ai_futures.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>ai futures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agi-thoughts">1.5.1. 🤖 AGI thoughts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-compatible">1.5.2. human compatible</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-if-we-succeed">1.5.2.1. what if we succeed?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#harms-of-ai">1.5.2.2. harms of ai</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-alignment">1.5.2.3. value alignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-solns">1.5.2.4. possible solns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-minds">1.5.3. possible minds</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-brockman">1.5.3.1. intro (brockman)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrong-but-more-relevant-than-ever-seth-lloyd">1.5.3.2. wrong but more relevant than ever (seth lloyd)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-limitations-of-opaque-learning-machines-judea-pearl">1.5.3.3. the limitations of opaque learning machines (judea pearl)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-purpose-put-into-the-machine-stuart-russell">1.5.3.4. the purpose put into the machine (stuart russell)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-third-law-george-dyson">1.5.3.5. the third law (george dyson)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-we-do-daniel-dennett">1.5.3.6. what can we do? (daniel dennett)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-unity-of-intelligence-frank-wilczek">1.5.3.7. the unity of intelligence (frank wilczek)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-aspire-to-more-than-making-ourselves-obsolete-max-tegmark">1.5.3.8. lets aspire to more than making ourselves obsolete (max tegmark)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dissident-messages-jaan-taliin">1.5.3.9. dissident messages (jaan taliin)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tech-prophecy-and-the-underappreciated-causal-power-of-ideas-steven-pinker">1.5.3.10. tech prophecy and the underappreciated causal power of ideas (steven pinker)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-reward-and-punishment-david-deutsch">1.5.3.11. beyond reward and punishment (david deutsch)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-artificial-use-of-human-beings-tom-griffiths">1.5.3.12. the artificial use of human beings (tom griffiths)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-invisible-visible-hans-ulrich-obrist">1.5.3.13. making the invisible visible (hans ulrich obrist)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorists-dream-of-objectivity-peter-galison">1.5.3.14. algorists dream of objectivity (peter galison)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rights-of-machines-george-church">1.5.3.15. the rights of machines (george church)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-artistic-use-of-cybernetic-beings-caroline-jones">1.5.3.16. the artistic use of cybernetic beings (caroline jones)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-for-wiener-shannon-and-for-us-david-kaiser">1.5.3.17. Information for wiener, Shannon, and for Us (david kaiser)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-neil-gershenfield">1.5.3.18. Scaling (Neil Gershenfield)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ai-futures">
<h1><span class="section-number">1.5. </span>ai futures<a class="headerlink" href="#ai-futures" title="Link to this heading">#</a></h1>
<section id="agi-thoughts">
<h2><span class="section-number">1.5.1. </span>🤖 AGI thoughts<a class="headerlink" href="#agi-thoughts" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>nice AGI definition:  AI systems are fully substitutable for human labor (or have a comparably large impact)</p></li>
<li><p>AI risk by deliberate human actors (i.e. concentrating power) seems to be a greater risk than unintended use (i.e. loss of control) [see some thought-out risks <a class="reference external" href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">here</a>]</p>
<ul>
<li><p>Caveat: AGI risk may still be high - nefarious use can easily be worse than accidental misuse</p></li>
<li><p>alignment research seems to be technically more interesting than safety research…</p></li>
</ul>
</li>
<li><p>Data limitations (e.g. in medicine) will limit rapid general advancements</p></li>
</ul>
</section>
<section id="human-compatible">
<h2><span class="section-number">1.5.2. </span>human compatible<a class="headerlink" href="#human-compatible" title="Link to this heading">#</a></h2>
<p><strong>A set of notes based on the book <em>human compatible</em>, by Stuart Russell (2019)</strong></p>
<section id="what-if-we-succeed">
<h3><span class="section-number">1.5.2.1. </span>what if we succeed?<a class="headerlink" href="#what-if-we-succeed" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>candidates for biggest event in the future of humanity</p>
<ul>
<li><p>we all die</p></li>
<li><p>we all live forever</p></li>
<li><p>we conquer the universe</p></li>
<li><p>we are visited by a superior alien civilization</p></li>
<li><p>we invent superintelligent AI</p></li>
</ul>
</li>
<li><p><em>defn</em>: humans are intelligent to the extent that our actions can be expected to achieve our objectives (given what we perceive)</p>
<ul>
<li><p>machines are <em>beneficial</em> to the extent that <em>their</em> actions can be expected to achieve <em>our</em> objectives</p></li>
</ul>
</li>
<li><p>Baldwin effect - learning can make evolution easier</p></li>
<li><p><strong>utility</strong> for things like money is <em>diminishing</em></p>
<ul>
<li><p>rational agents maximize <strong>expected utility</strong></p></li>
</ul>
</li>
<li><p>McCarthy helped usher in <em>knowledge-based systems</em>, which use <em>first-order logic</em></p>
<ul>
<li><p>however, these didn’t incorporate uncertainty</p></li>
<li><p>modern AI uses utilities and probabilities instead of goals and logic</p></li>
<li><p>bayesian networks are like probabilistic propositional logic, along with bayesian logic, probabilistic programming languages</p></li>
</ul>
</li>
<li><p>language already encodes a great deal about what we know</p></li>
<li><p><em>inductive logic programming</em> - propose new concepts and definitions in order to identify theories that are both accurate and concise</p></li>
<li><p>want to be able to learn many useful abstractions</p></li>
<li><p>a superhuman ai could do a lot</p>
<ul>
<li><p>e.g. help with evacuating by individually guiding every person/vehicle</p></li>
<li><p>carry out experiments and compare against all existing results easily</p></li>
<li><p>high-level goal: raise the standard of living for everyone everywhere?</p></li>
<li><p>AI tutoring</p></li>
</ul>
</li>
<li><p>EU GDPR’s “right to an explanation” wording is actually much weaker: “meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject”</p></li>
<li><p>whataboutery - a method for deflecting questions where one always asks “what about X?” rather than engaging</p></li>
</ul>
</section>
<section id="harms-of-ai">
<h3><span class="section-number">1.5.2.2. </span>harms of ai<a class="headerlink" href="#harms-of-ai" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>ex. surveillance, persuasion, and control</p></li>
<li><p>ex. lethal autonomous weapons (these are scalable)</p></li>
<li><p>ex. automated blackmail</p></li>
<li><p>ex. deepfakes / fake media</p></li>
<li><p>ex. automation - how to solve this? Universal basic income?</p></li>
</ul>
</section>
<section id="value-alignment">
<h3><span class="section-number">1.5.2.3. </span>value alignment<a class="headerlink" href="#value-alignment" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>ex. king midas</p></li>
<li><p>ex. driving dangerously</p></li>
<li><p>ex. in optimizing sea oxygen levels, takes them out of the air</p></li>
<li><p>ex. in curing cancer, gives everyone tumors</p></li>
<li><p>note: for an AI, it might be easier to convince of a different objective than actually solve the objective</p></li>
<li><p>basically any optimization objective will lead AI to disable its own off-switch</p></li>
</ul>
</section>
<section id="possible-solns">
<h3><span class="section-number">1.5.2.4. </span>possible solns<a class="headerlink" href="#possible-solns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>oracle AI - can only answer yes/no/probabilistic questions,  otherwise no output to the real world</p></li>
<li><p>inverse RL</p>
<ul>
<li><p>ai should be uncertain about utitilies</p></li>
<li><p>utilties should be inferred from human preferences</p></li>
<li><p>in systems that interact, need to express preferences in terms of game theory</p></li>
</ul>
</li>
<li><p>complications</p>
<ul>
<li><p>can be difficult to parse human instruction into preferences</p></li>
<li><p>people are different</p></li>
<li><p>AI loyal to one person might harm others</p></li>
<li><p>ai ethics</p>
<ul>
<li><p>consequentalism - choices should be judged according to expected consequences</p></li>
<li><p>deontological ethics, vritue ethics - concerned with the moral character of actions + individuals</p></li>
<li><p>hard to compare utilties across people</p></li>
<li><p>utilitarianism has issues when there is negative utility</p></li>
</ul>
</li>
<li><p>preferences can change</p></li>
</ul>
</li>
<li><p>AI should be regulated</p></li>
<li><p>deep learning is a lot like our sensory systems - logic is still need to act on these abstractions</p></li>
</ul>
</section>
</section>
<section id="possible-minds">
<h2><span class="section-number">1.5.3. </span>possible minds<a class="headerlink" href="#possible-minds" title="Link to this heading">#</a></h2>
<p><strong>notes from <em>possible minds</em>, a collection of essays edited by John Brockman (2019)</strong></p>
<section id="intro-brockman">
<h3><span class="section-number">1.5.3.1. </span>intro (brockman)<a class="headerlink" href="#intro-brockman" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>new technologies = new perceptions</p></li>
<li><p>we create tools and we mold ourselves through our use of them</p></li>
<li><p>Wiener: “We must cease to kiss the whip that lashes us”</p>
<ul>
<li><p>initial book <em>The human use of human beings</em></p></li>
<li><p>he was mostly analog, fell out of fashion</p></li>
<li><p>initially inspired the field</p></li>
</ul>
</li>
<li><p>ai has gone down and up for a while</p></li>
<li><p>gofai - good old-fashioned ai</p></li>
<li><p>things people thought would be hard, like chess, were easy</p></li>
</ul>
</section>
<section id="wrong-but-more-relevant-than-ever-seth-lloyd">
<h3><span class="section-number">1.5.3.2. </span>wrong but more relevant than ever (seth lloyd)<a class="headerlink" href="#wrong-but-more-relevant-than-ever-seth-lloyd" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>current AI is way worse than people think it is</p></li>
<li><p>wiener was very pessimistic - wwII / cold war</p></li>
<li><p>singularity is not coming…</p></li>
</ul>
</section>
<section id="the-limitations-of-opaque-learning-machines-judea-pearl">
<h3><span class="section-number">1.5.3.3. </span>the limitations of opaque learning machines (judea pearl)<a class="headerlink" href="#the-limitations-of-opaque-learning-machines-judea-pearl" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>3 levels of reasoning</p>
<ul>
<li><p>statistical</p></li>
<li><p>causal</p></li>
<li><p>counterfactual - lots of counterfactuals but language is good and providing lots of them</p></li>
</ul>
</li>
<li><p>“explaining away” = “backwards blocking” in the conditioning literature</p></li>
<li><p>starts causal inference, but doesn’t work for large systems</p></li>
<li><p>dl is more about speed than learning</p></li>
<li><p>dl is not interpretable</p></li>
<li><p>example: ask someone why they are divorced?</p>
<ul>
<li><p>income, age, etc…</p></li>
<li><p>something about relationship…</p></li>
</ul>
</li>
<li><p>correlations, causes, explanations (moral/rational) - biologically biased towards this?</p>
<ul>
<li><p>beliefs + desires cause actions</p></li>
</ul>
</li>
<li><p>pretty cool that different people follow  norms (e.g. come to class at 4pm)</p>
<ul>
<li><p>could you do this with ai?</p></li>
</ul>
</li>
<li><p>facebook chatbot ex.</p></li>
<li><p>paperclip machine, ads on social media</p></li>
<li><p>states/companies are like ais</p></li>
<li><p><strong>equifinality</strong> - perturb behavior (like use grayscale images instead of color) and they can still do it (like stability)</p></li>
</ul>
</section>
<section id="the-purpose-put-into-the-machine-stuart-russell">
<h3><span class="section-number">1.5.3.4. </span>the purpose put into the machine (stuart russell)<a class="headerlink" href="#the-purpose-put-into-the-machine-stuart-russell" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>want safety in ai - need to specify right objective with no uncertainty</p></li>
<li><p><strong>value alignment</strong> - putting in the right purpose</p></li>
<li><p>ai research studies the ability to achieve objectives, not the design of those objectives</p>
<ul>
<li><p>“better at making decisions - not making better decisions”</p></li>
</ul>
</li>
<li><p>want provable beneficial ai</p></li>
<li><p>can’t just maximize rewards - optimal solution is to control human to give more rewards</p></li>
<li><p>cooperative inverse-rl - robot learns reward function from human</p>
<ul>
<li><p>this way, uncertainty about rewards lets robot preserve its off-switch</p></li>
<li><p>human actions don’t always reflect their true preferences</p></li>
</ul>
</li>
</ul>
</section>
<section id="the-third-law-george-dyson">
<h3><span class="section-number">1.5.3.5. </span>the third law (george dyson)<a class="headerlink" href="#the-third-law-george-dyson" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>2 eras: before/after digital computers</p>
<ul>
<li><p>before: thomas hobbes, gottfried wilhelm leibniz</p></li>
<li><p>after:</p>
<ul>
<li><p>alan turing - intelligent machines</p></li>
<li><p>john von neumann - reproducing machines</p></li>
<li><p>claude shannon - communicate reliably</p></li>
<li><p>norbert weiner - when would machines take control</p></li>
</ul>
</li>
</ul>
</li>
<li><p>analog computing - all about error corrections</p></li>
<li><p>nature uses digitial coding for proteins but analog for brain</p></li>
<li><p>social graphs can use digital code for analog computing</p>
<ul>
<li><p>analog systems seem to control what they are mapping (e.g. decentralized traffic map)</p></li>
</ul>
</li>
<li><p>3 laws of ai</p>
<ul>
<li><p>ashby’s law - any effective control system must be as complex as the system it controls</p></li>
<li><p>von neumman’s law - defining characteristic of a complex system is that it constitutes its own simplest behavioral description</p></li>
<li><p>3rd law - any system simple enough to be understandable will not be complicated enough to behave intelligently and vice versa</p></li>
</ul>
</li>
</ul>
</section>
<section id="what-can-we-do-daniel-dennett">
<h3><span class="section-number">1.5.3.6. </span>what can we do? (daniel dennett)<a class="headerlink" href="#what-can-we-do-daniel-dennett" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>dennett wrote from bacteria to bach &amp; back</p></li>
<li><p>praise: willingness to admit he is wrong / stay levelheaded</p></li>
<li><p>rereading stuff opens new doors</p></li>
<li><p>import to treat AI as tools - real danger is humans being slaves to the AI coming about naturally</p>
<ul>
<li><p>analogy to our dependence on fruit for vitamin C whereas other animals synthesize it</p></li>
<li><p>tech has made it easy to tamper with evidence etc.</p></li>
<li><p>Wiener: “In the long run, there is no distinction between arming ourselves and arming our enemies.”</p></li>
</ul>
</li>
<li><p>current AI is parasitic on human intelligence</p></li>
<li><p>we are robots made of robots made of robots…with no magical ingredients thrown in along the way</p></li>
<li><p>current humanoid embellishments are <em>false advertising</em></p></li>
<li><p>need a way to test safety/interpretability of systems, maybe with human judges</p></li>
<li><p>people automatically personify things</p></li>
<li><p>we need intelligent tools, not conscious ones - more like oracles</p></li>
<li><p>very hard to build in morality into ais - even death might not seem bad</p></li>
</ul>
</section>
<section id="the-unity-of-intelligence-frank-wilczek">
<h3><span class="section-number">1.5.3.7. </span>the unity of intelligence (frank wilczek)<a class="headerlink" href="#the-unity-of-intelligence-frank-wilczek" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>can an ai be conscious/creative/evil?</p></li>
<li><p>mind is emergent property of matter <span class="math notranslate nohighlight">\(\implies\)</span> all intelligence is machine intelligence</p></li>
<li><p>david hume: ‘reason is, and ought only to be, the slave of the passions’</p></li>
<li><p>no sharp divide between natural and artificial intelligence: seem to work on the same physics</p></li>
<li><p>intelligence seems to be an emergent behavior</p></li>
<li><p>key differences between brains and computers: brains can self-repair, have higher connectivity, but lower efficiency overall</p></li>
<li><p>most profound advantage of brain: connectivity and interactive development</p></li>
<li><p>ais will be good at exploring</p></li>
<li><p>defining general intelligence - maybe using language?</p></li>
<li><p>earth’s environment not great for ais</p></li>
<li><p>ai could control world w/ just info, not just physical means</p></li>
<li><p>affective economy - sale of emotions (like talking to starbucks barista)</p></li>
<li><p>people seem to like to live in human world</p>
<ul>
<li><p>ex. work in cafes, libraries, etc.</p></li>
</ul>
</li>
<li><p>future life institute - funded by elon…maybe just trying to make money</p></li>
</ul>
</section>
<section id="lets-aspire-to-more-than-making-ourselves-obsolete-max-tegmark">
<h3><span class="section-number">1.5.3.8. </span>lets aspire to more than making ourselves obsolete (max tegmark)<a class="headerlink" href="#lets-aspire-to-more-than-making-ourselves-obsolete-max-tegmark" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>sometimes listed as scaremonger</p></li>
<li><p>maybe consciousness could be much more hype - like waking up from being drowsy</p></li>
<li><p>survey of AI experts said 50% chance of general ai surpassing human intelligence by 2040-2050</p></li>
<li><p>finding purpose if we aren’t needed for anything?</p></li>
<li><p>importance of keeping ai beneficial</p></li>
<li><p>possible AIs will replace all jobs</p></li>
<li><p>curiosity is dangerous</p></li>
<li><p>3 reasons ai danger is downplayed</p>
<ol class="arabic simple">
<li><p>people downplay danger because it makes their research seem good - “It is difficult to get a man to understand something, when his salary depends on his not understanding it” - Upton Sinclair</p>
<ul>
<li><p><strong>luddite</strong> - person opposoed to new technology or ways of working - stems from secret organization of english textile workers who protested</p></li>
</ul>
</li>
<li><p>it’s an abstract threat</p></li>
<li><p>it feels hopeless to think about</p></li>
</ol>
</li>
<li><p>AI safety research must precede AI developments</p></li>
<li><p>the real risk with AGI isn’t malice but competence</p></li>
<li><p>intelligence = ability to accomplish complex goals</p></li>
<li><p>how good are people at predicting the future of technology?</p></li>
<li><p>joseph weizenbbam wrote psychotherapist bot that was pretty bad but scared him</p></li>
</ul>
</section>
<section id="dissident-messages-jaan-taliin">
<h3><span class="section-number">1.5.3.9. </span>dissident messages (jaan taliin)<a class="headerlink" href="#dissident-messages-jaan-taliin" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>voices that stand up slowly end up convincing people</p></li>
<li><p>ai is different than tech that has come before - it can self-multiply</p></li>
<li><p>human brain has caused lots of changes in the world - ai will be similar</p></li>
<li><p>people seem to be tipping more towards the fact that the risk is large</p></li>
<li><p>short-term risks: automation + bias</p></li>
<li><p>one big risk: AI environmental risk: how to constrain ai to not render our environment uninhabitable for biological forms</p></li>
<li><p>need to stop thinking of the world as a zero-sum game</p></li>
<li><p>famous survery: katja grace at the future of humanity institute</p></li>
</ul>
</section>
<section id="tech-prophecy-and-the-underappreciated-causal-power-of-ideas-steven-pinker">
<h3><span class="section-number">1.5.3.10. </span>tech prophecy and the underappreciated causal power of ideas (steven pinker)<a class="headerlink" href="#tech-prophecy-and-the-underappreciated-causal-power-of-ideas-steven-pinker" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“just as darwin made it possible for a thoughtful observer of the natural world to do without creationism, Turing and others made it possible for a thoughtful observer of the cognitive world to do without spiritualism”</p></li>
<li><p>entropy view: ais is trying to stave off entropy by following specific goals</p></li>
<li><p>ideas drive human history</p></li>
<li><p>2 possible demises</p>
<ul>
<li><p>surveillance state</p>
<ul>
<li><p>automatic speech recognition</p></li>
<li><p>pinker thinks this isn’t a big deal because freedom of thought is driven by norms and institutions not tech</p></li>
<li><p>tech’s biggest threat seems to be amplifying dubious voices not surpressing enlightened ones</p></li>
<li><p>more tech has correlated w/ more democracy</p></li>
</ul>
</li>
<li><p>ai takes over</p>
<ul>
<li><p>seems too much like technological determinism</p></li>
<li><p>intelligence is the ability to deploy novel means to attain a goal - doesn’t specify what the goal is</p></li>
<li><p>knowledge are things we know - ours are mostly find food, mates, etc. machines will have other ones</p></li>
</ul>
</li>
</ul>
</li>
<li><p>if humans are smart enough to make ai, they are smart enough to test it</p></li>
<li><p>“threat isn’t machine but what can be made of it”</p></li>
</ul>
</section>
<section id="beyond-reward-and-punishment-david-deutsch">
<h3><span class="section-number">1.5.3.11. </span>beyond reward and punishment (david deutsch)<a class="headerlink" href="#beyond-reward-and-punishment-david-deutsch" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>david deutsch - founder of quantum computing</p></li>
<li><p>thinking - involves coming up w/ new hypotheses, not just being bayesian</p></li>
<li><p>knowledge itself wasn’t hugely evolutionarily beneficial in the beginning, but retaining cultural knowledge was</p>
<ul>
<li><p>in the beginning, people didn’t really learn - just remembered cultural norms</p></li>
<li><p>no one aspired to anything new</p></li>
</ul>
</li>
<li><p>so far, the way ais have been developed (e.g. chess-playing) is restricting a search space, but AGI wants them to come up with a new search space</p></li>
<li><p>we usually don’t follow laws because of punishments - neither will AGIs</p></li>
<li><p>open society is the only stable kind</p></li>
<li><p>will be hard to test / optimize for directly</p></li>
<li><p>AGI could still be deterministic</p></li>
<li><p>tension between imitation and learning? (immitation/innovation)</p></li>
<li><p>people falsely believe AGI should be able to learn on its own, like Nietzche’s <em>causa sui</em>, buy humans don’t do this</p></li>
<li><p>culture might make you more model-free</p></li>
</ul>
</section>
<section id="the-artificial-use-of-human-beings-tom-griffiths">
<h3><span class="section-number">1.5.3.12. </span>the artificial use of human beings (tom griffiths)<a class="headerlink" href="#the-artificial-use-of-human-beings-tom-griffiths" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>believes key to ml is human learning</p></li>
<li><p>we now have good models of images/text, but not of</p></li>
<li><p>value alignment</p></li>
<li><p>inverse rl: look at actions of intelligent agent, learn reward</p></li>
<li><p>accuracy (heuristics) vs generalizability (often assumes rationality)</p>
<ul>
<li><p>however, people are often not rational - people follow simple heuristics</p></li>
<li><p>ex. don’t calculate probabilities, just try to remember examples</p></li>
</ul>
</li>
<li><p>people usually tradeoff time with how important a decision is - <strong>bounded optimality</strong></p></li>
<li><p>could ai actually produce more leisure?</p></li>
</ul>
</section>
<section id="making-the-invisible-visible-hans-ulrich-obrist">
<h3><span class="section-number">1.5.3.13. </span>making the invisible visible (hans ulrich obrist)<a class="headerlink" href="#making-the-invisible-visible-hans-ulrich-obrist" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>need to use art to better interpret visualizations, like deepdream</p></li>
<li><p>ai as a tool, like photoshop</p></li>
<li><p>tweaking simulations is art (again in a deep-dream like way)</p></li>
<li><p>meta-objectives are important</p></li>
<li><p>art - an early alarm system to think about the future, evocative</p></li>
<li><p>design - has a clearer purpose, invisible</p>
<ul>
<li><p>fluxist movement - do it yourself, like flash mob, spontanous, not snobby</p></li>
</ul>
</li>
<li><p>this progress exhibit - guggenheim where they hand you off to people getting older</p></li>
<li><p>art - tracks what people appreciate over time</p></li>
<li><p>everything except museums + pixels are pixels</p></li>
<li><p>marcel duchamp 1917 - urinal in art museum was worth a ton</p></li>
</ul>
</section>
<section id="algorists-dream-of-objectivity-peter-galison">
<h3><span class="section-number">1.5.3.14. </span>algorists dream of objectivity (peter galison)<a class="headerlink" href="#algorists-dream-of-objectivity-peter-galison" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>science historian</p></li>
<li><p>stories of dangerous technologies have been repeated (e.g. nanoscience, recombinant DNA)</p></li>
<li><p>review in psychology found objective models outperformed groups of human clinicians (“prediction procedures: the clinical-statistical controversy”)</p></li>
<li><p>people initially started w/ drawing things</p>
<ul>
<li><p>then shifted to more objective measures (e.g. microscope)</p></li>
<li><p>then slight shift away (e.g. humans outperformed algorithms at things)</p></li>
</ul>
</li>
<li><p>objectivity is not everything</p></li>
<li><p>art w/ a nervous system</p></li>
<li><p>animations with charcters that have goals</p></li>
</ul>
</section>
<section id="the-rights-of-machines-george-church">
<h3><span class="section-number">1.5.3.15. </span>the rights of machines (george church)<a class="headerlink" href="#the-rights-of-machines-george-church" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>machines should increasingly get rights as those of humans</p></li>
<li><p>potential for AI to make humans smarter as well</p></li>
</ul>
</section>
<section id="the-artistic-use-of-cybernetic-beings-caroline-jones">
<h3><span class="section-number">1.5.3.16. </span>the artistic use of cybernetic beings (caroline jones)<a class="headerlink" href="#the-artistic-use-of-cybernetic-beings-caroline-jones" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>how to strech people beyond our simple, selfish parameters</p></li>
<li><p>cybernetics seance art</p></li>
<li><p>more grounded in hardware</p></li>
<li><p>culture-based evolution</p></li>
<li><p>uncanny valley - if things look too humanlike, we find them creepy</p>
<ul>
<li><p>this doesn’t happen for kids (until ~10 years)</p></li>
</ul>
</li>
<li><p>neil mendoza animal-based aft reflections</p></li>
<li><p>is current ai more advanced than game of life?</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="information-for-wiener-shannon-and-for-us-david-kaiser">
<h3><span class="section-number">1.5.3.17. </span>Information for wiener, Shannon, and for Us (david kaiser)<a class="headerlink" href="#information-for-wiener-shannon-and-for-us-david-kaiser" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>wiener: society can only be understood based on analyzing messages</p>
<ul>
<li><p>information = semantic information</p></li>
<li><p>shannon: information = entropy (not reduction in entropy?)</p></li>
<li><p>predictions</p>
<ul>
<li><p>information can not be conserved (effective level of info will be perpetually advancing)</p></li>
<li><p>information is unsuited to being commodities</p>
<ul>
<li><p>can easily be replicated</p></li>
<li><p>science started having citations in 17th century because before that people didn’t want to publish</p>
<ul>
<li><p>turned info into currency</p></li>
</ul>
</li>
<li><p>art world has struggled w/ this</p>
<ul>
<li><p>80s: appropration art - only changed title</p></li>
</ul>
</li>
<li><p>literature for a long time had no copyrights</p></li>
<li><p>algorithms hard to patent</p></li>
</ul>
</li>
</ul>
</li>
<li><p>wiener’s warning: machines would dominate us only when individuals are the same</p>
<ul>
<li><p>style and such become more similar as we are more connected</p>
<ul>
<li><p>twitter would be the opposite of that</p></li>
<li><p>amazon could make things more homogenous</p></li>
</ul>
</li>
<li><p>fashion changes consistently</p>
<ul>
<li><p>maybe arbitrary way to identify in/out groups</p></li>
</ul>
</li>
<li><p>comparison to markets</p></li>
<li><p>cities seem to increase diversity - more people to interact with</p></li>
</ul>
</li>
</ul>
</li>
<li><p>dl should seek more semantic info not statistical info</p></li>
</ul>
</section>
<section id="scaling-neil-gershenfield">
<h3><span class="section-number">1.5.3.18. </span>Scaling (Neil Gershenfield)<a class="headerlink" href="#scaling-neil-gershenfield" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>ai is more about scaling laws rathern that fashions</p></li>
<li><p>mania: success to limited domains</p></li>
<li><p>depression: failure to ill-posed problems</p></li>
<li><p>knowledge vs information: which is in the world, which is in your head?</p></li>
<li><p>problem 1: communication - important that knowledge can be replicated w/ no loss (shannon)</p></li>
<li><p>problem 2: computation - import knowledge can be stored (von Neumann)</p></li>
<li><p>problem 3: generalization - how to come up w/ rules for reasoning?</p></li>
<li><p>next: fabrication - how to make things?</p>
<ul>
<li><p>ex. body uses only 20 amino acids</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/ai"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="philosophy.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1.4. </span>philosophy</p>
      </div>
    </a>
    <a class="right-next"
       href="cogsci.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.6. </span>cognitive science</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agi-thoughts">1.5.1. 🤖 AGI thoughts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-compatible">1.5.2. human compatible</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-if-we-succeed">1.5.2.1. what if we succeed?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#harms-of-ai">1.5.2.2. harms of ai</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-alignment">1.5.2.3. value alignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-solns">1.5.2.4. possible solns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#possible-minds">1.5.3. possible minds</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-brockman">1.5.3.1. intro (brockman)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wrong-but-more-relevant-than-ever-seth-lloyd">1.5.3.2. wrong but more relevant than ever (seth lloyd)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-limitations-of-opaque-learning-machines-judea-pearl">1.5.3.3. the limitations of opaque learning machines (judea pearl)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-purpose-put-into-the-machine-stuart-russell">1.5.3.4. the purpose put into the machine (stuart russell)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-third-law-george-dyson">1.5.3.5. the third law (george dyson)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-we-do-daniel-dennett">1.5.3.6. what can we do? (daniel dennett)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-unity-of-intelligence-frank-wilczek">1.5.3.7. the unity of intelligence (frank wilczek)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lets-aspire-to-more-than-making-ourselves-obsolete-max-tegmark">1.5.3.8. lets aspire to more than making ourselves obsolete (max tegmark)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dissident-messages-jaan-taliin">1.5.3.9. dissident messages (jaan taliin)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tech-prophecy-and-the-underappreciated-causal-power-of-ideas-steven-pinker">1.5.3.10. tech prophecy and the underappreciated causal power of ideas (steven pinker)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-reward-and-punishment-david-deutsch">1.5.3.11. beyond reward and punishment (david deutsch)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-artificial-use-of-human-beings-tom-griffiths">1.5.3.12. the artificial use of human beings (tom griffiths)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-the-invisible-visible-hans-ulrich-obrist">1.5.3.13. making the invisible visible (hans ulrich obrist)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorists-dream-of-objectivity-peter-galison">1.5.3.14. algorists dream of objectivity (peter galison)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rights-of-machines-george-church">1.5.3.15. the rights of machines (george church)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-artistic-use-of-cybernetic-beings-caroline-jones">1.5.3.16. the artistic use of cybernetic beings (caroline jones)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-for-wiener-shannon-and-for-us-david-kaiser">1.5.3.17. Information for wiener, Shannon, and for Us (david kaiser)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-neil-gershenfield">1.5.3.18. Scaling (Neil Gershenfield)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>