
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.6. optimization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/math/optimization';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3.7. calculus" href="calculus.html" />
    <link rel="prev" title="3.5. signals" href="signals.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../research_ovws/research_ovws.html">1. research_ovws</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">1.1. transfer learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_disentanglement.html">1.2. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_omics.html">1.3. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_llms.html">1.4. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_complexity.html">1.5. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interesting_science.html">1.6. interesting science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_dl_theory.html">1.7. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_scat.html">1.8. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">1.9. ml in medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_causal_inference.html">1.10. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_uncertainty.html">1.11. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interp.html">1.12. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_generalization.html">1.13. generalization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">2. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">2.1. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">2.2. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">2.3. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">2.4. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">2.5. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">2.6. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">2.7. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">2.8. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">2.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">2.10. reproducibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">2.11. cs theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="math.html">3. math</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="differential_equations.html">3.1. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="proofs.html">3.2. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="analysis.html">3.3. real analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_algebra.html">3.4. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="signals.html">3.5. signals</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">3.6. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="calculus.html">3.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="chaos.html">3.8. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="math_basics.html">3.9. math basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../stat/stat.html">4. stat</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../stat/graphical_models.html">4.1. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/data_analysis.html">4.2. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/testing.html">4.3. testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/causal_inference.html">4.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/time_series.html">4.7. time series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stat/game_theory.html">4.8. game theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml/ml.html">5. ml</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml/kernels.html">5.1. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/nlp.html">5.2. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/comp_vision.html">5.3. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/structure_ml.html">5.4. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/classification.html">5.5. classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/unsupervised.html">5.6. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/deep_learning.html">5.7. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/feature_selection.html">5.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/learning_theory.html">5.9. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/evaluation.html">5.10. evaluation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ai/ai.html">6. ai</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ai/llms.html">6.1. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/search.html">6.2. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/decisions_rl.html">6.3. decisions, rl</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/fairness_sts.html">6.4. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/cogsci.html">6.5. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/ai_futures.html">6.6. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/logic.html">6.7. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/philosophy.html">6.8. philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/psychology.html">6.9. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/knowledge_rep.html">6.10. representations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">7. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">7.1. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">7.2. brain basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">7.3. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">7.4. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">7.5. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">7.6. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">7.7. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">7.8. development</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/math/optimization.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization">3.6.1. convex optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets-boyd-2">3.6.1.1. convex sets (boyd 2)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#geometry">3.6.1.1.1. geometry</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-funcs-boyd-3">3.6.1.2. convex funcs (boyd 3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-problems-boyd-4">3.6.1.3. optimization problems (boyd 4)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.6.1.4. optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.6.1.5. convex optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-optimization">3.6.1.6. linear optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-optimization">3.6.1.7. quadratic optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-program">3.6.1.8. geometric program</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#duality-boyd-5">3.6.2. duality (boyd 5)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-conditions">3.6.2.1. optimality conditions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thms-of-alternatives">3.6.2.2. thms of alternatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approx-fitting-boyd-6">3.6.3. approx + fitting (boyd 6)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#norm-approx-problem">3.6.3.1. norm approx problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-norm-problem">3.6.3.2. least norm problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-approximation">3.6.3.3. regularized approximation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-approximation">3.6.3.4. robust approximation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-fitting">3.6.3.5. function fitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-minimization-boyd-9">3.6.4. unconstrained minimization (boyd 9)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-problems">3.6.4.1. unconstrained problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#descent-methods">3.6.4.2. descent methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gd-method">3.6.4.3. gd method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steepest-descent-method">3.6.4.4. steepest descent method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">3.6.4.5. newton’s method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-algorithms">3.6.5. basic algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-j-11">3.6.6. expectation maximization - j 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">3.6.7. misc</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-optimization">3.6.7.1. nn optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-hard">3.6.7.1.1. why is it hard?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complicated-is-simpler">3.6.7.1.2. complicated is simpler</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-integer-programming-mip">3.6.7.2. mixed integer programming (mip)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<h1><span class="section-number">3.6. </span>optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h1>
<section id="convex-optimization">
<h2><span class="section-number">3.6.1. </span>convex optimization<a class="headerlink" href="#convex-optimization" title="Link to this heading">#</a></h2>
<section id="convex-sets-boyd-2">
<h3><span class="section-number">3.6.1.1. </span>convex sets (boyd 2)<a class="headerlink" href="#convex-sets-boyd-2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>affine set</em>: <span class="math notranslate nohighlight">\(x_1, x_2 \in C, \theta \in \mathbb{R} \implies \theta x_1 + (1 - \theta) x_2 \in C\)</span></p>
<ul>
<li><p><em>affine hull</em>: aff C = {<span class="math notranslate nohighlight">\(\sum \theta_i x_i | x_i \in C, \sum \theta_i =1 \)</span>}</p></li>
</ul>
</li>
<li><p><em>convex set</em>: <span class="math notranslate nohighlight">\(x_1, x_2 \in C, 0 \leq \theta \leq 1 \implies \theta x_1 + (1 - \theta) x_2 \)</span></p>
<ul>
<li><p><em>convex hull</em>: conv C = {<span class="math notranslate nohighlight">\(\sum \theta_i x_i \: | x_i \in C, \theta_i \geq 0, \sum \theta_i = 1\)</span>}</p></li>
</ul>
</li>
<li><p><em>cone</em>: <span class="math notranslate nohighlight">\(\theta \geq 0 \implies \theta x \in C\)</span></p></li>
<li><p>operations that preserve convexity</p>
<ul>
<li><p>intersection (finite intersection of half-spaces)</p></li>
<li><p>pointwise max of affine funcs</p></li>
<li><p>composition</p></li>
<li><p>affine</p></li>
<li><p>perspective</p></li>
<li><p>linar fractional = projective</p></li>
</ul>
</li>
<li><p>generalized inequalities:</p>
<ul>
<li><p><em>proper cone</em> K: convex, closed, pointed, solid</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \preceq_K y \iff y-x \in K\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>separating hyperplane thm</em>: C, D convex <span class="math notranslate nohighlight">\(C \cap D =\emptyset \implies \exists a \neq 0, b \: s.t. \\ a^Tx \leq b \forall x \in C, \\a^Tx \geq b \forall x \in D\)</span></p></li>
<li><p><em>supporting hyperplane thm</em>: {<span class="math notranslate nohighlight">\(x|a^tx = a^t x_0\)</span>} where <span class="math notranslate nohighlight">\(x_0\)</span> on boundary of convex C</p></li>
<li><p>dual cone <span class="math notranslate nohighlight">\(K^*\)</span> = {<span class="math notranslate nohighlight">\(y|x^Ty \geq 0 \: \forall x \in K\)</span>}</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\preceq_{K^*}\)</span> is dual of <span class="math notranslate nohighlight">\(\preceq_K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x \preceq_K y \iff \lambda^T x \leq \lambda^T y \quad \forall \: \lambda \succeq_{K^*} 0\)</span></p></li>
</ul>
</li>
</ul>
<section id="geometry">
<h4><span class="section-number">3.6.1.1.1. </span>geometry<a class="headerlink" href="#geometry" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><em>ellipsoid</em>: {<span class="math notranslate nohighlight">\(x \in \mathbb{R}^n | (x-x_c)^T P^{-1} (x-x_c) \leq 1\)</span>} where P symmetric, PSD</p>
<ul>
<li><p>{<span class="math notranslate nohighlight">\(x_c + Au | ||u||_2 \leq 1\)</span>}</p></li>
</ul>
</li>
<li><p><em>hyperplane</em>: {<span class="math notranslate nohighlight">\(x|a^Tx = b\)</span>} ~ creates a halfspace</p></li>
<li><p><em>norm cone</em>: {<span class="math notranslate nohighlight">\((x, t)| \: ||x|| \leq t\)</span>}</p></li>
<li><p><em>polyhedron</em>: {x | Ax=b, Cx=d} = <span class="math notranslate nohighlight">\(\{ \sum_i^k \theta_i v_i \; | \sum_i^m \theta_i = 1, \theta_i \geq 0 \} \: m \leq k\)</span></p></li>
<li><p><em>simplex</em>: conv{<span class="math notranslate nohighlight">\(v_{0:k}\)</span>}</p></li>
</ul>
</section>
</section>
<section id="convex-funcs-boyd-3">
<h3><span class="section-number">3.6.1.2. </span>convex funcs (boyd 3)<a class="headerlink" href="#convex-funcs-boyd-3" title="Link to this heading">#</a></h3>
<ul>
<li><p>definitions</p>
<ol class="arabic simple">
<li><p><em>Jensen’s inequality</em> <span class="math notranslate nohighlight">\(0 \leq \theta \leq 1\)</span></p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(\theta x_1 + (1 - \theta) x_2) \leq \theta f(x_1) + (1 - \theta) f(x_2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(E[X]) \leq E[f(X)]\)</span></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(x_2) \geq f(x_1) + \nabla f(x_1)^T (x_2 - x_1)\)</span></p></li>
</ol>
<ul class="simple">
<li><p>can show this by restricting to an arbitrary line</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>consider epi f - also use things that preseve convexity</p></li>
</ol>
</li>
<li><p>concepts</p>
<ul class="simple">
<li><p><em>epigraph</em> epi f = <span class="math notranslate nohighlight">\(\{ (x, t) \; | \: x \in dom f, f(x) \leq t \}\)</span></p></li>
<li><p><em>extended value extension</em>: <span class="math notranslate nohighlight">\(\tilde{f}(x) = f(x)\)</span> if <span class="math notranslate nohighlight">\(x \in dom f\)</span> else <span class="math notranslate nohighlight">\(\infty\)</span></p></li>
<li><p><em>wide sense function</em> - can take on values <span class="math notranslate nohighlight">\(\pm \infty\)</span></p>
<ul>
<li><p>= dom f = {<span class="math notranslate nohighlight">\(x | f(x) &lt; \infty\)</span>}</p></li>
</ul>
</li>
<li><p><em>wide sense convex func</em>: <span class="math notranslate nohighlight">\(f(x) = inf \{ t \in \mathbb{R} | (x, t) \in F\}\)</span> where <span class="math notranslate nohighlight">\(F \subseteq \mathbb{R}^{n+1}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(x) = inf \{ t \in \mathbb{R} | (x, t) \in F \}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>-sublevel set of convex func is convex</p></li>
</ul>
</li>
<li><p>operations that preserve convexity</p>
<ul class="simple">
<li><p>nonnegative weighted sums ~ multiplies for logs</p></li>
<li><p>affine map</p></li>
<li><p>pointwise max of convex</p></li>
<li><p>composition</p></li>
<li><p>perspective</p></li>
<li><p>minimization ~ sometimes</p></li>
</ul>
</li>
<li><p><em>conjugate</em> of f</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f^*(y) = \underset{x \in dom f}{sup} \: y^T x - f(x)\)</span></p></li>
<li><p>dom <span class="math notranslate nohighlight">\(f^*\)</span> = {<span class="math notranslate nohighlight">\(y|f^*(y)\)</span> is finite}</p></li>
<li><p>called <em>Legendre transform</em> when f differentiable</p></li>
<li><p><em>fenchel’s inequality</em>: <span class="math notranslate nohighlight">\(f(x) + f^*(y) \geq x^ty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f^{**} = f\)</span> iff convex, closed</p></li>
</ul>
</li>
<li><p>ex. <span class="math notranslate nohighlight">\(f(S) = log \: det X^{-1}\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f^*(Y) = \underset{X}{sup} [tr(YX) + log \: det X]\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(= -n - log \: det(-Y) \)</span> if <span class="math notranslate nohighlight">\(-Y \in S^n_{++}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>can use conj. to go other way: <span class="math notranslate nohighlight">\(f(y) = \underset{x}{sup}(y^Tx - f^*(x))\)</span></p></li>
</ul>
</section>
<section id="optimization-problems-boyd-4">
<h3><span class="section-number">3.6.1.3. </span>optimization problems (boyd 4)<a class="headerlink" href="#optimization-problems-boyd-4" title="Link to this heading">#</a></h3>
</section>
<section id="id1">
<h3><span class="section-number">3.6.1.4. </span>optimization<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>standard form: <span class="math notranslate nohighlight">\(p^* = min \: f_0(x)\\s.t. \: f_i(x) \leq 0 \\ h_i(x) = 0\)</span></p></li>
<li><p>equivalent problems</p>
<ul>
<li><p>change of vars</p></li>
<li><p>constraint transformations</p></li>
<li><p>slack vars</p></li>
<li><p>eliminating equalities</p></li>
<li><p>eliminating linear equalities</p></li>
<li><p>introducing equalities</p></li>
<li><p>optimizing over some vars ~ ex. quadratic</p></li>
<li><p>epigraph form: <span class="math notranslate nohighlight">\(min \: t \: s.t. \: f_0 \leq t\)</span></p></li>
<li><p>implicit + explicit constraints</p></li>
</ul>
</li>
</ul>
</section>
<section id="id2">
<h3><span class="section-number">3.6.1.5. </span>convex optimization<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>standard form: $<span class="math notranslate nohighlight">\(p^* = min \: f_0(x)\\s.t. \: f_i(x) \leq 0 \\ a_i^Tx = b_i\)</span>$ where all f are convex</p></li>
<li><p>optimality criteria (special cases of KKT)</p>
<ul>
<li><p>x optimal if</p>
<ol class="arabic simple">
<li><p>x is feasible</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla f_0 (x)^T(y-x) \geq 0 \: \forall y \)</span> feasible</p></li>
</ol>
</li>
<li><p>if unconstrained <span class="math notranslate nohighlight">\(\nabla f_0 (x) = 0\)</span></p></li>
<li><p>if equality only Ax=b, <span class="math notranslate nohighlight">\(\nabla f_0 (x) \perp N(A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x \succeq 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f_0 (x) \succeq 0; x_i (\nabla f_0 (x))_i = 0\)</span></p></li>
</ul>
</li>
<li><p>equivalent convex problems</p>
<ul>
<li><p>eliminating equality constraints</p></li>
<li><p>introducing equality constraints</p></li>
<li><p>slack vars ~ for linear inequalities</p></li>
<li><p>epigraph form</p></li>
<li><p>minimizing over some vars</p></li>
</ul>
</li>
</ul>
</section>
<section id="linear-optimization">
<h3><span class="section-number">3.6.1.6. </span>linear optimization<a class="headerlink" href="#linear-optimization" title="Link to this heading">#</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[\begin{split}p^* = min \: c^T x + d\\s.t. \: Gx \succeq h \\ Ax=b\end{split}\]</div>
</li>
<li><p>standard form <span class="math notranslate nohighlight">\(x \succeq 0\)</span> is the only inequality</p></li>
<li><p>standard dual: max <span class="math notranslate nohighlight">\(-b^T \nu\)</span> s.t. <span class="math notranslate nohighlight">\(A^T \nu + c \geq 0\)</span></p></li>
<li><p><em>linear-fractional</em> program</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(min \: \frac{c^Tx + d}{e^tx+f} \\ s.t \: Gx \succeq h \\ Ax = b\)</span> ~ can be converted to LP</p></li>
</ul>
</li>
</ul>
</section>
<section id="quadratic-optimization">
<h3><span class="section-number">3.6.1.7. </span>quadratic optimization<a class="headerlink" href="#quadratic-optimization" title="Link to this heading">#</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[\begin{split}min \: 1/2 x^TPx + q^Txr \\s.t. \: Gx \succeq h$$ where $P \in S_+^n$
- *QCQP* - inequality constraints also convex
  - ex. $min \: ||Ax-b||_2^2$
- *SOCP* - $$min f^Tx \\ s.t. \: ||A_ix+b_i||_2 \leq c_i^T + d_i \\ Fx=g\end{split}\]</div>
</li>
</ul>
</section>
<section id="geometric-program">
<h3><span class="section-number">3.6.1.8. </span>geometric program<a class="headerlink" href="#geometric-program" title="Link to this heading">#</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}min \: f_0(x) \\ s.t. \: f_i(x) \leq 1 \: i = 1:m \\ h_i(x) = 1 \: i = 1:p$$ where $f_{0:m}$ posynomials, $h_i$ monomials
  - *monomial* $f(x) = c x_1^{a_1} \cdot x_n^{a_n}, c&gt;0$
  - posynomial ~ sum of monomials ~ can transform into convex w/ $y_i = log x_i$\end{split}\\\begin{split}### generalized inequality
- $$min \: f_0(x)\\s.t. \: f_i(x) \preceq_{K_i} 0, i=1:m\\Ax=b\end{split}\end{aligned}\end{align} \]</div>
</li>
<li><p><em>conic form problem</em>: $<span class="math notranslate nohighlight">\(min \: c^Tx \\ s.t. \: Fx +g \preceq_K 0\\Ax=b\)</span><span class="math notranslate nohighlight">\( ~ set \)</span>K=S_+^K$</p></li>
<li><p><em>SDP</em> = <em>semi-definite program</em>: $<span class="math notranslate nohighlight">\(min \: c^T x\\s.t. \: x_1F_1+...+x_nF_n+G \preceq 0\\Ax=b\)</span><span class="math notranslate nohighlight">\( ~ where \)</span>F_1, …, F_n \in S^k$</p>
<ul class="simple">
<li><p>standard form: <span class="math notranslate nohighlight">\(min \: tr(CX) \\ s.to \: tr(A_iX)=b_i \\ X \succeq 0\)</span></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="duality-boyd-5">
<h2><span class="section-number">3.6.2. </span>duality (boyd 5)<a class="headerlink" href="#duality-boyd-5" title="Link to this heading">#</a></h2>
<ul>
<li><p>consider <span class="math notranslate nohighlight">\(\min \: f_0 (x) \\ s.t. \: f_i(x) \leq 0 \\ h_i(x) = 0\)</span></p></li>
<li><p><em>lagrangian</em> <span class="math notranslate nohighlight">\(L(x, \lambda, \nu) = f_0(x) + \sum \lambda_i f_i(x) + \sum \nu_i h_i(x)\)</span></p></li>
<li><p><em>dual function</em> <span class="math notranslate nohighlight">\(g(\lambda, \nu) = \underset{x \in D}{\inf} L(x, \lambda, \nu)\)</span> ~ g always concave</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda \succeq 0 \implies g(\lambda, \nu) \leq p^*\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\((\lambda, \nu)\)</span> <em>dual feasible</em> if</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\lambda \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\lambda, \nu) \in dom \: g\)</span></p></li>
</ol>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(p^* = - \infty\)</span>, dual infeasible</p></li>
<li><p>when <span class="math notranslate nohighlight">\(d^*=\infty\)</span>, primal infeasible</p></li>
</ul>
</li>
<li><p>dual related to to conjugate func</p>
<ul class="simple">
<li><p>ex. min f(x) s.t. <span class="math notranslate nohighlight">\(x = 0 \implies g(\nu) = -f^*(-\nu)\)</span></p></li>
</ul>
</li>
<li><p><em>lagrange dual problem</em>: <span class="math notranslate nohighlight">\(\max \: g(\lambda, \nu)\\s.t. \: \lambda \succeq 0\)</span></p></li>
<li><p><em>weak duality</em>: <span class="math notranslate nohighlight">\(d^* \leq p^*\)</span></p>
<ul class="simple">
<li><p><em>optimal duality gap</em>: <span class="math notranslate nohighlight">\(p^* - d^*\)</span></p></li>
</ul>
</li>
<li><p><em>strong duality</em>: <span class="math notranslate nohighlight">\(d^* = p^*\)</span> ~ requires more than convexity</p></li>
<li><p><em>slater’s condition</em> ~ if problem convex <span class="math notranslate nohighlight">\(\implies\)</span> strong duality + <span class="math notranslate nohighlight">\(\exists\)</span> dual optimal point</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\exists x \in relint \: D\\f_i(x) &lt; 0\\Ax = b\)</span> ~ point is strictly feasible</p></li>
<li><p>to weaken this, affine <span class="math notranslate nohighlight">\(f_i\)</span> can be <span class="math notranslate nohighlight">\(\leq 0\)</span></p></li>
</ul>
</li>
<li><p><em>sion’s minimax thm</em>: <span class="math notranslate nohighlight">\(x \to f(x, y)\)</span> ~ conditions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\implies \underset{x}{min} \: \underset{y}{sup} \: f(x,y) = \underset{y}{sup} \: \underset{x}{min} \: f(x,y)\)</span></p></li>
</ul>
</li>
</ul>
<section id="optimality-conditions">
<h3><span class="section-number">3.6.2.1. </span>optimality conditions<a class="headerlink" href="#optimality-conditions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>duality gap</em>: <span class="math notranslate nohighlight">\(f_0(x) - g(\lambda, \nu)\)</span></p></li>
<li><p>can use stopping condition duality gap <span class="math notranslate nohighlight">\(\leq \epsilon_{abs}\)</span> to be <span class="math notranslate nohighlight">\(\epsilon_{abs}\)</span> - suboptimal</p></li>
<li><p>strong duality yields <em>complementary slackness</em></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lambda_i f_i(x^*)=0\)</span></p></li>
</ul>
</li>
<li><p><em>KKT optimality conditions</em> ~ assume <span class="math notranslate nohighlight">\(f_0, f_i, h_i\)</span> differentiable, strong duality</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(f_i(x^*) \leq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(h_i(x^*) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_i^* \geq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_i^*f(x_i^*) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla f_0 (x^*) + \sum \lambda_i^* \nabla f_i (x_i^*) + \sum \nu_i^* \nabla h_i (x^*) = 0\)</span></p></li>
</ol>
</li>
</ul>
</section>
<section id="thms-of-alternatives">
<h3><span class="section-number">3.6.2.2. </span>thms of alternatives<a class="headerlink" href="#thms-of-alternatives" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>weak alternative</em> - at most one of 2 is true</p></li>
<li><p><em>strong alternative</em> - exactly one is true</p>
<ul>
<li><p>ex. Fredholm alternative</p></li>
<li><p>ex. Farkas’s lamma</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\exists x \: Ax \leq 0, c^Tx &lt; 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\exists y \: y \geq 0, A^Ty + c = 0\)</span></p></li>
</ol>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="approx-fitting-boyd-6">
<h2><span class="section-number">3.6.3. </span>approx + fitting (boyd 6)<a class="headerlink" href="#approx-fitting-boyd-6" title="Link to this heading">#</a></h2>
<section id="norm-approx-problem">
<h3><span class="section-number">3.6.3.1. </span>norm approx problem<a class="headerlink" href="#norm-approx-problem" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>minimize <span class="math notranslate nohighlight">\(||Ax-b||\)</span></p></li>
<li><p>ex. weighted norm approx. min||W(Ax-b)||</p></li>
<li><p>ex. least squares min ||Ax-b||<span class="math notranslate nohighlight">\(_2^2\)</span></p></li>
<li><p>ex. chebyshev approx norm min||Ax-b||<span class="math notranslate nohighlight">\(_\infty\)</span></p></li>
<li><p>ex. penalty function approx problem: <span class="math notranslate nohighlight">\(min \: \phi(r_1) + ... + \phi(r_m)\\s.t. \: r=Ax-b\)</span></p></li>
</ul>
</section>
<section id="least-norm-problem">
<h3><span class="section-number">3.6.3.2. </span>least norm problem<a class="headerlink" href="#least-norm-problem" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>min <span class="math notranslate nohighlight">\(||x||\\s.t. \: Ax=b\)</span> ~ min <span class="math notranslate nohighlight">\(||x_0+ Zu||\)</span>, Z cols basis for N(A)</p></li>
</ul>
</section>
<section id="regularized-approximation">
<h3><span class="section-number">3.6.3.3. </span>regularized approximation<a class="headerlink" href="#regularized-approximation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>min <span class="math notranslate nohighlight">\(||Ax-b|| + \gamma ||x||\)</span></p></li>
<li><p>min <span class="math notranslate nohighlight">\(||Ax-b||^2 + \gamma ||x||^2\)</span></p></li>
<li><p><em>Tikhonov</em>: <span class="math notranslate nohighlight">\(min \: ||Ax-b||_2^2 + \gamma ||x||_2^2\)</span></p></li>
<li><p>examples</p>
<ul>
<li><p>ex. regularize w/ ||Dx||</p></li>
<li><p>ex. lasso</p></li>
<li><p>ex. quadratic smoothing</p></li>
<li><p>ex. total variation</p></li>
</ul>
</li>
</ul>
</section>
<section id="robust-approximation">
<h3><span class="section-number">3.6.3.4. </span>robust approximation<a class="headerlink" href="#robust-approximation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A = \bar{A} + U\)</span> ~ random w/ mean 0</p></li>
</ul>
<ol class="arabic simple">
<li><p>stochastic robust approx problem: <span class="math notranslate nohighlight">\(min \: E||Ax-b||\)</span></p></li>
<li><p>(worst-case) robust approx prob: <span class="math notranslate nohighlight">\(min \: sup ||Ax-b|| \: | A \in \mathcal{A}\)</span></p></li>
</ol>
</section>
<section id="function-fitting">
<h3><span class="section-number">3.6.3.5. </span>function fitting<a class="headerlink" href="#function-fitting" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(u) = x_1 f_1 (u) + .... + x_n f_n (u)\)</span> ~ <span class="math notranslate nohighlight">\(f_i\)</span> are basis funcs, <span class="math notranslate nohighlight">\(x_i\)</span> are coefficients</p></li>
<li><p>sparse descriptions + basis pursuit</p></li>
<li><p>interpolation</p></li>
</ul>
</section>
</section>
<section id="unconstrained-minimization-boyd-9">
<h2><span class="section-number">3.6.4. </span>unconstrained minimization (boyd 9)<a class="headerlink" href="#unconstrained-minimization-boyd-9" title="Link to this heading">#</a></h2>
<section id="unconstrained-problems">
<h3><span class="section-number">3.6.4.1. </span>unconstrained problems<a class="headerlink" href="#unconstrained-problems" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x^* = \text{argmin} \: f(x) \implies \nabla f(x^*) = 0\)</span></p></li>
<li><p>examples</p>
<ul>
<li><p>ex. quadratic: <span class="math notranslate nohighlight">\(\min \: 1/2 x^TPX + q^Tx + r\)</span></p>
<ul>
<li><p>solved w/ <span class="math notranslate nohighlight">\(Px^* + q = 0\)</span>, if <span class="math notranslate nohighlight">\(P \succeq 0\)</span>, unique soln <span class="math notranslate nohighlight">\(-P^{-1}q\)</span></p></li>
</ul>
</li>
<li><p>ex. unconstrained geometric program</p></li>
<li><p>ex. analytic center of linear inequalities</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\min \: f(x) = -\sum \: \log (b_i - a_i^Tx)\)</span> where dom f = <span class="math notranslate nohighlight">\(\{x|a_i^Tx&lt; b_i, i = 1:m\}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>3 definitions of convexity</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(0 \leq \theta \leq 1\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f(\theta x_1 + (1 - \theta) x_2) \leq \theta f(x_1) + (1 - \theta) f(x_2)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(x_2) \geq f(x_1) + \nabla f(x_1)^T (x_2 - x_1)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\color{red}0 \preceq \color{green}{\underset{\text{strong convexity}}{mI}} \preceq \nabla^2 \color{cornflowerblue}{f(x)} \preceq \underset{\text{smoothness}}{MI}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\kappa = M/m\)</span> bounds <em>condition number</em> of <span class="math notranslate nohighlight">\(\nabla^2 f = \frac{\lambda_{\max}(\nabla^2 f)}{\lambda_{\min}(\nabla^2 f)}\)</span></p></li>
<li><p><em>strongly convex</em>: <span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq mI\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\implies f(x_2) \geq f(x_1) + \nabla f(x_1)^T(x_2-x_1) + m/2 ||x_2-x_1||_2^2\)</span></p></li>
<li><p>minimizing yields <span class="math notranslate nohighlight">\(p^* \geq f(x) - 1/(2m) ||\nabla f(x)||_2^2\)</span></p></li>
<li><p>if the gradient of f at x is small enough, then the difference between f(x) and p⋆ is small</p></li>
</ul>
</li>
<li><p><em>smooth</em>: <span class="math notranslate nohighlight">\(\exists \: M, \: \nabla^2f(x) \preceq MI\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\implies f(y) \leq f(x) + \nabla f(x)^T(y-x) + M/2 ||y-x||_2^2\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>cond(<em>C</em>) = <span class="math notranslate nohighlight">\(W_{\max}^2 / W_{\min}^2\)</span></p>
<ul>
<li><p><em>width</em> of convex set <span class="math notranslate nohighlight">\(C \subset \mathbb{R}^n\)</span> in direction q with <span class="math notranslate nohighlight">\(||q||_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W(C, q) = \underset{z \in C}{\sup} \: q^Tz - \underset{z \in C}{\inf} \: q^Tz\)</span></p></li>
</ul>
</li>
<li><p><em>alpha-level subset</em>: <span class="math notranslate nohighlight">\(C_\alpha = \{x|f(x) \leq \alpha\}\)</span></p></li>
</ul>
</section>
<section id="descent-methods">
<h3><span class="section-number">3.6.4.2. </span>descent methods<a class="headerlink" href="#descent-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>update rule <span class="math notranslate nohighlight">\(x = x + t \Delta x\)</span></p></li>
<li><p><em>exact line search</em>: <span class="math notranslate nohighlight">\(t = \underset{s \geq 0}{\text{argmin}} \:f(x+s \Delta x)\)</span></p></li>
<li><p><em>backtracking line search</em></p>
<ul>
<li><p>given a descent direction <span class="math notranslate nohighlight">\(\Delta x \text{ for } f, x \in dom \: f, \alpha \in (0, 0.5), \beta \in (0, 1)\)</span></p></li>
<li><p>t:=1, <span class="math notranslate nohighlight">\(\alpha \in (0, 0.5), \beta \in (0, 1)\)</span></p></li>
<li><p>while <span class="math notranslate nohighlight">\(f(x + t \Delta x) &gt; f(x) + \alpha t \nabla f(x)^T \Delta x\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(t *= \beta\)</span></p></li>
</ul>
</li>
<li><p>![Screen Shot 2018-07-30 at 10.23.19 PM-3014637](assets/Screen Shot 2018-07-30 at 10.23.19 PM-3014637.png)</p></li>
</ul>
</li>
</ul>
</section>
<section id="gd-method">
<h3><span class="section-number">3.6.4.3. </span>gd method<a class="headerlink" href="#gd-method" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>convergence</p>
<ul>
<li><p>can bound number of iterations required to be less than <span class="math notranslate nohighlight">\(\epsilon\)</span></p></li>
</ul>
</li>
<li><p>examples</p>
<ul>
<li><p>a quadratic problem in <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p>non-quadratic problem in <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p>a problem in <span class="math notranslate nohighlight">\(R^{100}\)</span></p></li>
<li><p>gradient method and condition number</p></li>
</ul>
</li>
<li><p>conclusions</p>
<ul>
<li><p>gd often exhibits approximately linear convergence</p></li>
<li><p>convergence rate depends greatly on <span class="math notranslate nohighlight">\(cond (\nabla^2 f(x))\)</span> or sublevel sets</p></li>
</ul>
</li>
</ul>
</section>
<section id="steepest-descent-method">
<h3><span class="section-number">3.6.4.4. </span>steepest descent method<a class="headerlink" href="#steepest-descent-method" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>examples</p>
<ul>
<li><p>euclidean norm: <span class="math notranslate nohighlight">\(\Delta x_{sd} = - \nabla f(x)\)</span></p></li>
<li><p>quadratic norm <span class="math notranslate nohighlight">\(||z||_P = (z^TPz)^{1/2} = ||P^{1/2}z||_2\)</span> where <span class="math notranslate nohighlight">\(P \in S_{++}^n\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\Delta x_{sd} = -P^{-1} \nabla f(x)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\ell_1\)</span> norm: <span class="math notranslate nohighlight">\(\Delta_{sd} = -\frac{\partial f(x)}{\partial x_i} e_i\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="newton-s-method">
<h3><span class="section-number">3.6.4.5. </span>newton’s method<a class="headerlink" href="#newton-s-method" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><em>Newton step</em> <span class="math notranslate nohighlight">\(\Delta x_{nt} = - \nabla^2 f(x)^{-1} \nabla f(x)\)</span></p>
<ul>
<li><p>PSD <span class="math notranslate nohighlight">\(\implies \nabla f(x)^T \Delta x_{nt} = - \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) &lt; 0\)</span></p></li>
</ul>
</li>
<li><p><em>Newton’s method</em></p>
<ol class="arabic simple">
<li><p>compute the newton step <span class="math notranslate nohighlight">\(\Delta x_{nt}\)</span> and decrement <span class="math notranslate nohighlight">\(\lambda^2 = \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x)\)</span></p></li>
<li><p>stopping criterion: quit if <span class="math notranslate nohighlight">\(\lambda^2 / 2 \leq \epsilon\)</span></p></li>
<li><p>line search: choose step size t w/ backtracking line search</p></li>
<li><p>update: <span class="math notranslate nohighlight">\(x += t \Delta x_{nt}\)</span></p></li>
</ol>
</li>
</ul>
</section>
</section>
<section id="basic-algorithms">
<h2><span class="section-number">3.6.5. </span>basic algorithms<a class="headerlink" href="#basic-algorithms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>types: <em>batch</em> (have full data) vs <em>online</em></p></li>
</ul>
<ol class="arabic simple">
<li><p>gradient descent = <em>batch gradient descent</em></p>
<ul class="simple">
<li><p><em>gradient</em> - vector that points to direction of maximum increase</p></li>
<li><p>at every step, subtract gradient multiplied by learning rate: <span class="math notranslate nohighlight">\(x_k = x_{k-1} - \alpha \nabla_x F(x_{k-1})\)</span></p></li>
<li><p>alpha = 0.05 seems to work</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\theta) = 1/2 (\theta ^T X^T X \theta - 2 \theta^T X^T y + y^T y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_\theta J(\theta) = X^T X \theta - X^T Y\)</span></p>
<ul>
<li><p>= <span class="math notranslate nohighlight">\(\sum_i  x_i  (x_i^T - y_i)\)</span></p></li>
<li><p>this represents residuals * examples</p></li>
</ul>
</li>
</ul>
</li>
<li><p>stochastic gradient descent</p>
<ul class="simple">
<li><p>don’t use all training examples - approximates gradient</p>
<ul>
<li><p>single-sample</p></li>
<li><p>mini-batch (usually better in offline case)</p></li>
</ul>
</li>
<li><p><em>coordinate-descent</em> algorithm</p></li>
<li><p><em>online</em> algorithm - update theta while training data is changing</p></li>
<li><p>when to stop?</p>
<ul>
<li><p>predetermined number of iterations</p></li>
<li><p>stop when improvement drops below a threshold</p></li>
</ul>
</li>
<li><p>each pass of the whole data = 1 epoch</p></li>
<li><p>benefits</p>
<ol class="arabic simple">
<li><p>less prone to getting stuck to shallow local minima</p></li>
<li><p>don’t need huge ram</p></li>
<li><p>faster</p></li>
</ol>
</li>
</ul>
</li>
<li><p>newton’s method for optimization</p>
<ul class="simple">
<li><p><em>second-order</em> optimization - requires 1st &amp; 2nd derivatives</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_{k+1} = \theta_k - H_K^{-1} g_k\)</span></p></li>
<li><p>update with inverse of Hessian as alpha - this is an approximation to a taylor series</p></li>
<li><p>finding inverse of Hessian can be hard / expensive</p></li>
</ul>
</li>
<li><p>ADMM - <em>alternating direction method of multipliers</em> (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle</p></li>
</ol>
</section>
<section id="expectation-maximization-j-11">
<h2><span class="section-number">3.6.6. </span>expectation maximization - j 11<a class="headerlink" href="#expectation-maximization-j-11" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>method to maximize likelihood on model with observed X and hidden Z</p>
<ol class="arabic simple">
<li><p><em>expectation step</em> - values of unobserved latent variables are filled in</p>
<ul>
<li><p>calculates prob of latent variables given observed variables and current param values</p></li>
</ul>
</li>
<li><p><em>maximization step</em> - parameters are adjusted based on filled-in variables</p></li>
</ol>
</li>
<li><p>goal: maximize <em>complete log-likelihood</em>, but don’t know z</p>
<ul>
<li><p><em>expected complete log-likelihood</em> <span class="math notranslate nohighlight">\(E_{p'}[l(\theta; x,z)] = \sum_z p'(z|x,\theta) \cdot \log \: p(x,z|\theta)\)</span></p>
<ul>
<li><p>p’ distribution is assignment to z vars</p></li>
</ul>
</li>
<li><p>deriving <em>auxilary function</em> <span class="math notranslate nohighlight">\(\mathcal L(q, \theta, x) = \sum_z p'(z|x) \log \frac{p(x,z|\theta)}{p'(z|x)}\)</span> - lower bound for the log likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{align} l(\theta; x) &amp;= \log \: p(x|\theta) &amp; \text{incomplete log-likelihood} \\&amp;= \log \sum_z p(x,z|\theta) &amp;\text{complete log-likelihood}\\&amp;= \log\sum_z p'(z|x) \frac{p(x,z|\theta)}{p'(z|x)} &amp;\text{multiplying by 1} \\ &amp;\geq \sum_z p'(z|x) \log \frac{p(x,z|\theta)}{p'(z|x)} &amp;\text{Jensen's inequality}\\&amp;\triangleq \mathcal L (p', \theta) \end{align}\)</span></p></li>
<li><p>this removes dependence on z</p></li>
</ul>
</li>
<li><p>steps</p>
<ul>
<li><p>E: <span class="math notranslate nohighlight">\(p'(z|x, \theta) = \underset{p'}{\text{argmax}}\: \mathcal L(p',\theta, x)\)</span></p></li>
<li><p>M: <span class="math notranslate nohighlight">\(\theta = \underset{\theta}{\text{argmax}} \: \mathcal L(p', \theta, x)\)</span></p></li>
<li><p>equivalent to maximizing expected complete log-likelihood</p></li>
<li><p><em>stochastically</em> converges to <em>local</em> minimum</p></li>
</ul>
</li>
<li><p>alternatively, can look at kl-divergences</p></li>
</ul>
</section>
<section id="misc">
<h2><span class="section-number">3.6.7. </span>misc<a class="headerlink" href="#misc" title="Link to this heading">#</a></h2>
<section id="nn-optimization">
<h3><span class="section-number">3.6.7.1. </span>nn optimization<a class="headerlink" href="#nn-optimization" title="Link to this heading">#</a></h3>
<section id="why-is-it-hard">
<h4><span class="section-number">3.6.7.1.1. </span>why is it hard?<a class="headerlink" href="#why-is-it-hard" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>plateaus</p></li>
<li><p>winding canyons</p></li>
<li><p>cliffs</p></li>
<li><p>local maxima to dodge</p></li>
<li><p>saddle points (local max and local min)</p></li>
<li><p>most popular</p>
<ul>
<li><p>sgd</p></li>
<li><p>sgd + nesterov momentum</p></li>
<li><p>adam</p></li>
<li><p>adagrad - maintains a per-parameter learning rate that improves performance on problems with sparse gradients</p></li>
<li><p>rmsprop - (ignore) per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing)</p></li>
</ul>
</li>
<li><p>adam - “adaptive moment estimation” (kingma_2015)</p>
<ul>
<li><p>keep track of per-parameter learning rate (based on first moment of gradients tracked) and per-parameter second moment (based on variance of gradients tracked)</p></li>
<li><p>alpha - learning rate</p></li>
<li><p>beta1 - exponential decay rate for first moment estimate</p>
<ul>
<li><p>default 0.9</p></li>
</ul>
</li>
<li><p>beta2 - exponential decay rate for 2nd moment estimates (should be higher when gradients sparser)</p>
<ul>
<li><p>default 0.999</p></li>
</ul>
</li>
<li><p>epsilon - small number to prevent division by zero</p>
<ul>
<li><p>default 1e-8 - usually requires tuning (ex. inception requires 1e-1) <img alt="Screen Shot 2018-10-11 at 8.07.56 AM" src="../../_images/adam.png" />visualization</p></li>
</ul>
</li>
</ul>
</li>
<li><p>requires low dims</p>
<ul>
<li><p>goodfellow 2015 “Qualitatively characterizing neural network optimization problems” plots loss on line from starting point to ending point</p></li>
<li><p>could do PCA on params</p></li>
</ul>
</li>
</ul>
</section>
<section id="complicated-is-simpler">
<h4><span class="section-number">3.6.7.1.2. </span>complicated is simpler<a class="headerlink" href="#complicated-is-simpler" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>ex. <span class="math notranslate nohighlight">\(x^3 \sin(x)\)</span> is simpler than just <span class="math notranslate nohighlight">\(x\)</span> on the domain [−0.01, 0.01]</p></li>
<li><p>dropout is like ridge</p></li>
</ul>
</section>
</section>
<section id="mixed-integer-programming-mip">
<h3><span class="section-number">3.6.7.2. </span>mixed integer programming (mip)<a class="headerlink" href="#mixed-integer-programming-mip" title="Link to this heading">#</a></h3>
<p>Notes from this <a class="reference external" href="https://www.gurobi.com/resource/mip-basics/">overview</a></p>
<ul class="simple">
<li><p>ex: minimize <span class="math notranslate nohighlight">\(c^{\top} x\)</span> s.t.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathrm{A} \mathrm{x}=\mathrm{b}\)</span> (linear constraints)</p></li>
<li><p><span class="math notranslate nohighlight">\(l\leq x \leq u\)</span> (bound constraints)</p></li>
<li><p>some or all <span class="math notranslate nohighlight">\(x_j\)</span> must take integer values (integrality
constraints)</p></li>
</ul>
</li>
<li><p>solving: branch-and-bound</p>
<ul>
<li><p>integer constraints make this difficult so start by solving the relaxation without integer constraints</p></li>
<li><p>pick one of the variables <span class="math notranslate nohighlight">\(x_b\)</span> that was not integer to be our <em>branching variable</em></p>
<ul>
<li><p>say it’s value was 5.7, now solve 2 MIPS, one with constraint <span class="math notranslate nohighlight">\(x_b \leq 5\)</span> an <span class="math notranslate nohighlight">\(x_b \geq 6\)</span></p>
<ul>
<li><p>return better solution of these two</p></li>
</ul>
</li>
<li><p>iterate by solving relaxation and then adding more branching constraints</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="signals.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3.5. </span>signals</p>
      </div>
    </a>
    <a class="right-next"
       href="calculus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3.7. </span>calculus</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization">3.6.1. convex optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets-boyd-2">3.6.1.1. convex sets (boyd 2)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#geometry">3.6.1.1.1. geometry</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-funcs-boyd-3">3.6.1.2. convex funcs (boyd 3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-problems-boyd-4">3.6.1.3. optimization problems (boyd 4)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.6.1.4. optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">3.6.1.5. convex optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-optimization">3.6.1.6. linear optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-optimization">3.6.1.7. quadratic optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-program">3.6.1.8. geometric program</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#duality-boyd-5">3.6.2. duality (boyd 5)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimality-conditions">3.6.2.1. optimality conditions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thms-of-alternatives">3.6.2.2. thms of alternatives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approx-fitting-boyd-6">3.6.3. approx + fitting (boyd 6)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#norm-approx-problem">3.6.3.1. norm approx problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-norm-problem">3.6.3.2. least norm problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularized-approximation">3.6.3.3. regularized approximation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#robust-approximation">3.6.3.4. robust approximation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-fitting">3.6.3.5. function fitting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-minimization-boyd-9">3.6.4. unconstrained minimization (boyd 9)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unconstrained-problems">3.6.4.1. unconstrained problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#descent-methods">3.6.4.2. descent methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gd-method">3.6.4.3. gd method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steepest-descent-method">3.6.4.4. steepest descent method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-s-method">3.6.4.5. newton’s method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-algorithms">3.6.5. basic algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-j-11">3.6.6. expectation maximization - j 11</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">3.6.7. misc</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-optimization">3.6.7.1. nn optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-hard">3.6.7.1.1. why is it hard?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complicated-is-simpler">3.6.7.1.2. complicated is simpler</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-integer-programming-mip">3.6.7.2. mixed integer programming (mip)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>