
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>3.6. optimization</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../_static/togglebutton.js"></script>
    <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
    <script type="text/javascript" src="../../_static/copybutton.js"></script>
    <script type="text/javascript">var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script type="text/javascript" src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" type="text/javascript" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script type="text/javascript">
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" type="text/javascript" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="3.7. calculus" href="calculus.html" />
    <link rel="prev" title="3.5. signals" href="signals.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   welcome ðŸ‘‹
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../research_ovws/research_ovws.html">
   1. research_ovws
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_comp_neuro.html">
     1.1. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">
     1.2. transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_disentanglement.html">
     1.3. disentanglement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_complexity.html">
     1.4. complexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_interesting_science.html">
     1.5. interesting science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_dl_theory.html">
     1.6. dl theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_scat.html">
     1.7. scattering transform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">
     1.8. ml in medicine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_causal_inference.html">
     1.9. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_dl_for_neuro.html">
     1.10. dl for neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_uncertainty.html">
     1.11. uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_interp.html">
     1.12. interpretability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_kernels.html">
     1.13. kernels
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../cs/cs.html">
   2. cs
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/retrieval.html">
     2.1. info retrieval
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/python_ref.html">
     2.2. python ref
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/data_structures.html">
     2.3. data structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/software.html">
     2.4. software engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/quantum.html">
     2.5. quantum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/algo.html">
     2.6. algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/graphs.html">
     2.7. graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/os.html">
     2.8. os
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/arch.html">
     2.9. architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/java_ref.html">
     2.10. java ref
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/comp_theory.html">
     2.11. cs theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/cpp_ref.html">
     2.12. c/c++ ref
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="math.html">
   3. math
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="differential_equations.html">
     3.1. differential equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="proofs.html">
     3.2. proofs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="analysis.html">
     3.3. real analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_algebra.html">
     3.4. linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="signals.html">
     3.5. signals
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.6. optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="calculus.html">
     3.7. calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="chaos.html">
     3.8. chaos
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="math_basics.html">
     3.9. math basics
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../stat/stat.html">
   4. stat
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/graphical_models.html">
     4.1. graphical models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/data_analysis.html">
     4.2. data analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/testing.html">
     4.3. testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/causal_inference.html">
     4.4. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/info_theory.html">
     4.5. info theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/linear_models.html">
     4.6. linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/time_series.html">
     4.7. time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/game_theory.html">
     4.8. game theory
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../ml/ml.html">
   5. ml
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/nlp.html">
     5.1. nlp
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/comp_vision.html">
     5.2. computer vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/structure_ml.html">
     5.3. structure learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/classification.html">
     5.4. classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/unsupervised.html">
     5.5. unsupervised
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/deep_learning.html">
     5.6. deep learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/feature_selection.html">
     5.7. feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/learning_theory.html">
     5.8. learning theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/evaluation.html">
     5.9. evaluation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../ai/ai.html">
   6. ai
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/search.html">
     6.1. search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/fairness_sts.html">
     6.2. fairness, sts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/cogsci.html">
     6.3. cogsci
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/ai_futures.html">
     6.4. ai futures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/logic.html">
     6.5. logic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/philosophy.html">
     6.6. philosophy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/psychology.html">
     6.7. psychology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/knowledge_rep.html">
     6.8. representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/decisions.html">
     6.9. decisions
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../neuro/neuro.html">
   7. neuro
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/brain_basics.html">
     7.1. brain basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/vissci.html">
     7.2. vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/comp_neuro.html">
     7.3. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/sensory_input.html">
     7.4. sensory input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/memory.html">
     7.5. memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/motor.html">
     7.6. motor system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/development.html">
     7.7. development
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notes/math/optimization.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/csinva/csinva.github.io"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convex-optimization">
   3.6.1. convex optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convex-sets-boyd-2">
     3.6.1.1. convex sets (boyd 2)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#geometry">
       3.6.1.1.1. geometry
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convex-funcs-boyd-3">
     3.6.1.2. convex funcs (boyd 3)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-problems-boyd-4">
     3.6.1.3. optimization problems (boyd 4)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     3.6.1.4. optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     3.6.1.5. convex optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-optimization">
     3.6.1.6. linear optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quadratic-optimization">
     3.6.1.7. quadratic optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#geometric-program">
     3.6.1.8. geometric program
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#duality-boyd-5">
   3.6.2. duality (boyd 5)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimality-conditions">
     3.6.2.1. optimality conditions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thms-of-alternatives">
     3.6.2.2. thms of alternatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approx-fitting-boyd-6">
   3.6.3. approx + fitting (boyd 6)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#norm-approx-problem">
     3.6.3.1. norm approx problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#least-norm-problem">
     3.6.3.2. least norm problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularized-approximation">
     3.6.3.3. regularized approximation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#robust-approximation">
     3.6.3.4. robust approximation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#function-fitting">
     3.6.3.5. function fitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unconstrained-minimization-boyd-9">
   3.6.4. unconstrained minimization (boyd 9)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unconstrained-problems">
     3.6.4.1. unconstrained problems
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#descent-methods">
     3.6.4.2. descent methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gd-method">
     3.6.4.3. gd method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#steepest-descent-method">
     3.6.4.4. steepest descent method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-s-method">
     3.6.4.5. newtonâ€™s method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-algorithms">
   3.6.5. basic algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation-maximization-j-11">
   3.6.6. expectation maximization - j 11
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nn-optimization">
   3.6.7. nn optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-is-it-hard">
     3.6.7.1. why is it hard?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#complicated-is-simpler">
     3.6.7.2. complicated is simpler
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="optimization">
<h1>3.6. optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="convex-optimization">
<h2>3.6.1. convex optimization<a class="headerlink" href="#convex-optimization" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="convex-sets-boyd-2">
<h3>3.6.1.1. convex sets (boyd 2)<a class="headerlink" href="#convex-sets-boyd-2" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><em>affine set</em>: <span class="math notranslate nohighlight">\(x_1, x_2 \in C, \theta \in \mathbb{R} \implies \theta x_1 + (1 - \theta) x_2 \in C\)</span></p>
<ul>
<li><p><em>affine hull</em>: aff C = {<span class="math notranslate nohighlight">\(\sum \theta_i x_i | x_i \in C, \sum \theta_i =1 \)</span>}</p></li>
</ul>
</li>
<li><p><em>convex set</em>: <span class="math notranslate nohighlight">\(x_1, x_2 \in C, 0 \leq \theta \leq 1 \implies \theta x_1 + (1 - \theta) x_2 â€‹\)</span></p>
<ul>
<li><p><em>convex hull</em>: conv C = {<span class="math notranslate nohighlight">\(\sum \theta_i x_i \: | x_i \in C, \theta_i \geq 0, \sum \theta_i = 1\)</span>}</p></li>
</ul>
</li>
<li><p><em>cone</em>: <span class="math notranslate nohighlight">\(\theta \geq 0 \implies \theta x \in Câ€‹\)</span></p></li>
<li><p>operations that preserve convexity</p>
<ul>
<li><p>intersection (finite intersection of half-spaces)</p></li>
<li><p>pointwise max of affine funcs</p></li>
<li><p>composition</p></li>
<li><p>affine</p></li>
<li><p>perspective</p></li>
<li><p>linar fractional = projective</p></li>
</ul>
</li>
<li><p>generalized inequalities:</p>
<ul>
<li><p><em>proper cone</em> K: convex, closed, pointed, solid</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x \preceq_K y \iff y-x \in K\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>separating hyperplane thm</em>: C, D convex <span class="math notranslate nohighlight">\(C \cap D =\emptyset \implies \exists a \neq 0, b \: s.t. \\ a^Tx \leq b \forall x \in C, \\a^Tx \geq b \forall x \in D\)</span></p></li>
<li><p><em>supporting hyperplane thm</em>: {<span class="math notranslate nohighlight">\(x|a^tx = a^t x_0\)</span>} where <span class="math notranslate nohighlight">\(x_0\)</span> on boundary of convex C</p></li>
<li><p>dual cone <span class="math notranslate nohighlight">\(K^*\)</span> = {<span class="math notranslate nohighlight">\(y|x^Ty \geq 0 \: \forall x \in K\)</span>}</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\preceq_{K^*}\)</span> is dual of <span class="math notranslate nohighlight">\(\preceq_K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x \preceq_K y \iff \lambda^T x \leq \lambda^T y \quad \forall \: \lambda \succeq_{K^*} 0\)</span></p></li>
</ul>
</li>
</ul>
<div class="section" id="geometry">
<h4>3.6.1.1.1. geometry<a class="headerlink" href="#geometry" title="Permalink to this headline">Â¶</a></h4>
<ul class="simple">
<li><p><em>ellipsoid</em>: {<span class="math notranslate nohighlight">\(x \in \mathbb{R}^n | (x-x_c)^T P^{-1} (x-x_c) \leq 1\)</span>} where P symmetric, PSD</p>
<ul>
<li><p>{<span class="math notranslate nohighlight">\(x_c + Au | ||u||_2 \leq 1\)</span>}</p></li>
</ul>
</li>
<li><p><em>hyperplane</em>: {<span class="math notranslate nohighlight">\(x|a^Tx = b\)</span>} ~ creates a halfspace</p></li>
<li><p><em>norm cone</em>: {<span class="math notranslate nohighlight">\((x, t)| \: ||x|| \leq t\)</span>}</p></li>
<li><p><em>polyhedron</em>: {x | Ax=b, Cx=d} = <span class="math notranslate nohighlight">\(\{ \sum_i^k \theta_i v_i \; | \sum_i^m \theta_i = 1, \theta_i \geq 0 \} \: m \leq k\)</span></p></li>
<li><p><em>simplex</em>: conv{<span class="math notranslate nohighlight">\(v_{0:k}\)</span>}</p></li>
</ul>
</div>
</div>
<div class="section" id="convex-funcs-boyd-3">
<h3>3.6.1.2. convex funcs (boyd 3)<a class="headerlink" href="#convex-funcs-boyd-3" title="Permalink to this headline">Â¶</a></h3>
<ul>
<li><p>definitions</p>
<ol class="simple">
<li><p><em>Jensenâ€™s inequality</em> <span class="math notranslate nohighlight">\(0 \leq \theta \leq 1\)</span></p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(\theta x_1 + (1 - \theta) x_2) \leq \theta f(x_1) + (1 - \theta) f(x_2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(E[X]) \leq E[f(X)]\)</span></p></li>
</ul>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(x_2) \geq f(x_1) + \nabla f(x_1)^T (x_2 - x_1)\)</span></p></li>
</ol>
<ul class="simple">
<li><p>can show this by restricting to an arbitrary line</p></li>
</ul>
<ol class="simple">
<li><p>consider epi f - also use things that preseve convexity</p></li>
</ol>
</li>
<li><p>concepts</p>
<ul class="simple">
<li><p><em>epigraph</em> epi f = <span class="math notranslate nohighlight">\(\{ (x, t) \; | \: x \in dom f, f(x) \leq t \}\)</span></p></li>
<li><p><em>extended value extension</em>: <span class="math notranslate nohighlight">\(\tilde{f}(x) = f(x)\)</span> if <span class="math notranslate nohighlight">\(x \in dom f\)</span> else <span class="math notranslate nohighlight">\(\infty\)</span></p></li>
<li><p><em>wide sense function</em> - can take on values <span class="math notranslate nohighlight">\(\pm \infty\)</span></p>
<ul>
<li><p>= dom f = {<span class="math notranslate nohighlight">\(x | f(x) &lt; \infty\)</span>}</p></li>
</ul>
</li>
<li><p><em>wide sense convex func</em>: <span class="math notranslate nohighlight">\(f(x) = inf \{ t \in \mathbb{R} | (x, t) \in F\}\)</span> where <span class="math notranslate nohighlight">\(F \subseteq \mathbb{R}^{n+1}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(x) = inf \{ t \in \mathbb{R} | (x, t) \in F \}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>-sublevel set of convex func is convex</p></li>
</ul>
</li>
<li><p>operations that preserve convexity</p>
<ul class="simple">
<li><p>nonnegative weighted sums ~ multiplies for logs</p></li>
<li><p>affine map</p></li>
<li><p>pointwise max of convex</p></li>
<li><p>composition</p></li>
<li><p>perspective</p></li>
<li><p>minimization ~ sometimes</p></li>
</ul>
</li>
<li><p><em>conjugate</em> of f</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f^*(y) = \underset{x \in dom f}{sup} \: y^T x - f(x)\)</span></p></li>
<li><p>dom <span class="math notranslate nohighlight">\(f^*\)</span> = {<span class="math notranslate nohighlight">\(y|f^*(y)\)</span> is finite}</p></li>
<li><p>called <em>Legendre transform</em> when f differentiable</p></li>
<li><p><em>fenchelâ€™s inequality</em>: <span class="math notranslate nohighlight">\(f(x) + f^*(y) \geq x^ty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f^{**} = f\)</span> iff convex, closed</p></li>
</ul>
</li>
<li><p>ex. <span class="math notranslate nohighlight">\(f(S) = log \: det X^{-1}\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f^*(Y) = \underset{X}{sup} [tr(YX) + log \: det X]\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(= -n - log \: det(-Y) \)</span> if <span class="math notranslate nohighlight">\(-Y \in S^n_{++}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>can use conj. to go other way: <span class="math notranslate nohighlight">\(f(y) = \underset{x}{sup}(y^Tx - f^*(x))\)</span></p></li>
</ul>
</div>
<div class="section" id="optimization-problems-boyd-4">
<h3>3.6.1.3. optimization problems (boyd 4)<a class="headerlink" href="#optimization-problems-boyd-4" title="Permalink to this headline">Â¶</a></h3>
</div>
<div class="section" id="id1">
<h3>3.6.1.4. optimization<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>standard form: <span class="math notranslate nohighlight">\(p^* = min \: f_0(x)\\s.t. \: f_i(x) \leq 0 \\ h_i(x) = 0\)</span></p></li>
<li><p>equivalent problems</p>
<ul>
<li><p>change of vars</p></li>
<li><p>constraint transformations</p></li>
<li><p>slack vars</p></li>
<li><p>eliminating equalities</p></li>
<li><p>eliminating linear equalities</p></li>
<li><p>introducing equalities</p></li>
<li><p>optimizing over some vars ~ ex. quadratic</p></li>
<li><p>epigraph form: <span class="math notranslate nohighlight">\(min \: t \: s.t. \: f_0 \leq t\)</span></p></li>
<li><p>implicit + explicit constraints</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id2">
<h3>3.6.1.5. convex optimization<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>standard form: $<span class="math notranslate nohighlight">\(p^* = min \: f_0(x)\\s.t. \: f_i(x) \leq 0 \\ a_i^Tx = b_i\)</span>$ where all f are convex</p></li>
<li><p>optimality criteria (special cases of KKT)</p>
<ul>
<li><p>x optimal if</p>
<ol class="simple">
<li><p>x is feasible</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla f_0 (x)^T(y-x) \geq 0 \: \forall y \)</span> feasible</p></li>
</ol>
</li>
<li><p>if unconstrained <span class="math notranslate nohighlight">\(\nabla f_0 (x) = 0\)</span></p></li>
<li><p>if equality only Ax=b, <span class="math notranslate nohighlight">\(\nabla f_0 (x) \perp N(A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x \succeq 0\)</span>, <span class="math notranslate nohighlight">\(\nabla f_0 (x) \succeq 0; x_i (\nabla f_0 (x))_i = 0\)</span></p></li>
</ul>
</li>
<li><p>equivalent convex problems</p>
<ul>
<li><p>eliminating equality constraints</p></li>
<li><p>introducing equality constraints</p></li>
<li><p>slack vars ~ for linear inequalities</p></li>
<li><p>epigraph form</p></li>
<li><p>minimizing over some vars</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="linear-optimization">
<h3>3.6.1.6. linear optimization<a class="headerlink" href="#linear-optimization" title="Permalink to this headline">Â¶</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[\begin{split}p^* = min \: c^T x + d\\s.t. \: Gx \succeq h \\ Ax=b\end{split}\]</div>
</li>
<li><p>standard form <span class="math notranslate nohighlight">\(x \succeq 0\)</span> is the only inequality</p></li>
<li><p>standard dual: max <span class="math notranslate nohighlight">\(-b^T \nu\)</span> s.t. <span class="math notranslate nohighlight">\(A^T \nu + c \geq 0\)</span></p></li>
<li><p><em>linear-fractional</em> program</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(min \: \frac{c^Tx + d}{e^tx+f} \\ s.t \: Gx \succeq h \\ Ax = b\)</span> ~ can be converted to LP</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="quadratic-optimization">
<h3>3.6.1.7. quadratic optimization<a class="headerlink" href="#quadratic-optimization" title="Permalink to this headline">Â¶</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[\begin{split}min \: 1/2 x^TPx + q^Txr \\s.t. \: Gx \succeq h$$ where $P \in S_+^n$
- *QCQP* - inequality constraints also convex
  - ex. $min \: ||Ax-b||_2^2$
- *SOCP* - $$min f^Tx \\ s.t. \: ||A_ix+b_i||_2 \leq c_i^T + d_i \\ Fx=g\end{split}\]</div>
</li>
</ul>
</div>
<div class="section" id="geometric-program">
<h3>3.6.1.8. geometric program<a class="headerlink" href="#geometric-program" title="Permalink to this headline">Â¶</a></h3>
<ul>
<li><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}min \: f_0(x) \\ s.t. \: f_i(x) \leq 1 \: i = 1:m \\ h_i(x) = 1 \: i = 1:p$$ where $f_{0:m}$ posynomials, $h_i$ monomials
  - *monomial* $f(x) = c x_1^{a_1} \cdot x_n^{a_n}, c&gt;0$
  - posynomial ~ sum of monomials ~ can transform into convex w/ $y_i = log x_i$\end{split}\\\begin{split}### generalized inequality
- $$min \: f_0(x)\\s.t. \: f_i(x) \preceq_{K_i} 0, i=1:m\\Ax=b\end{split}\end{aligned}\end{align} \]</div>
</li>
<li><p><em>conic form problem</em>: $<span class="math notranslate nohighlight">\(min \: c^Tx \\ s.t. \: Fx +g \preceq_K 0\\Ax=b\)</span><span class="math notranslate nohighlight">\( ~ set \)</span>K=S_+^K$</p></li>
<li><p><em>SDP</em> = <em>semi-definite program</em>: $<span class="math notranslate nohighlight">\(min \: c^T x\\s.t. \: x_1F_1+...+x_nF_n+G \preceq 0\\Ax=b\)</span><span class="math notranslate nohighlight">\( ~ where \)</span>F_1, â€¦, F_n \in S^k$</p>
<ul class="simple">
<li><p>standard form: <span class="math notranslate nohighlight">\(min \: tr(CX) \\ s.to \: tr(A_iX)=b_i \\ X \succeq 0\)</span></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="duality-boyd-5">
<h2>3.6.2. duality (boyd 5)<a class="headerlink" href="#duality-boyd-5" title="Permalink to this headline">Â¶</a></h2>
<ul>
<li><p>consider <span class="math notranslate nohighlight">\(\min \: f_0 (x) \\ s.t. \: f_i(x) \leq 0 \\ h_i(x) = 0\)</span></p></li>
<li><p><em>lagrangian</em> <span class="math notranslate nohighlight">\(L(x, \lambda, \nu) = f_0(x) + \sum \lambda_i f_i(x) + \sum \nu_i h_i(x)\)</span></p></li>
<li><p><em>dual function</em> <span class="math notranslate nohighlight">\(g(\lambda, \nu) = \underset{x \in D}{\inf} L(x, \lambda, \nu)\)</span> ~ g always concave</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda \succeq 0 \implies g(\lambda, \nu) \leq p^*\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\((\lambda, \nu)\)</span> <em>dual feasible</em> if</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\lambda, \nu) \in dom \: g\)</span></p></li>
</ol>
<ul class="simple">
<li><p>when <span class="math notranslate nohighlight">\(p^* = - \infty\)</span>, dual infeasible</p></li>
<li><p>when <span class="math notranslate nohighlight">\(d^*=\infty\)</span>, primal infeasible</p></li>
</ul>
</li>
<li><p>dual related to to conjugate func</p>
<ul class="simple">
<li><p>ex. min f(x) s.t. <span class="math notranslate nohighlight">\(x = 0 \implies g(\nu) = -f^*(-\nu)\)</span></p></li>
</ul>
</li>
<li><p><em>lagrange dual problem</em>: <span class="math notranslate nohighlight">\(\max \: g(\lambda, \nu)\\s.t. \: \lambda \succeq 0\)</span></p></li>
<li><p><em>weak duality</em>: <span class="math notranslate nohighlight">\(d^* \leq p^*\)</span></p>
<ul class="simple">
<li><p><em>optimal duality gap</em>: <span class="math notranslate nohighlight">\(p^* - d^*\)</span></p></li>
</ul>
</li>
<li><p><em>strong duality</em>: <span class="math notranslate nohighlight">\(d^* = p^*\)</span> ~ requires more than convexity</p></li>
<li><p><em>slaterâ€™s condition</em> ~ if problem convex <span class="math notranslate nohighlight">\(\implies\)</span> strong duality + <span class="math notranslate nohighlight">\(\exists\)</span> dual optimal point</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\exists x \in relint \: D\\f_i(x) &lt; 0\\Ax = b\)</span> ~ point is strictly feasible</p></li>
<li><p>to weaken this, affine <span class="math notranslate nohighlight">\(f_i\)</span> can be <span class="math notranslate nohighlight">\(\leq 0\)</span></p></li>
</ul>
</li>
<li><p><em>sionâ€™s minimax thm</em>: <span class="math notranslate nohighlight">\(x \to f(x, y)\)</span> ~ conditions</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\implies \underset{x}{min} \: \underset{y}{sup} \: f(x,y) = \underset{y}{sup} \: \underset{x}{min} \: f(x,y)\)</span></p></li>
</ul>
</li>
</ul>
<div class="section" id="optimality-conditions">
<h3>3.6.2.1. optimality conditions<a class="headerlink" href="#optimality-conditions" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><em>duality gap</em>: <span class="math notranslate nohighlight">\(f_0(x) - g(\lambda, \nu)\)</span></p></li>
<li><p>can use stopping condition duality gap <span class="math notranslate nohighlight">\(\leq \epsilon_{abs}\)</span> to be <span class="math notranslate nohighlight">\(\epsilon_{abs}\)</span> - suboptimal</p></li>
<li><p>strong duality yields <em>complementary slackness</em></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\lambda_i f_i(x^*)=0\)</span></p></li>
</ul>
</li>
<li><p><em>KKT optimality conditions</em> ~ assume <span class="math notranslate nohighlight">\(f_0, f_i, h_i\)</span> differentiable, strong duality</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(f_i(x^*) \leq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(h_i(x^*) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_i^* \geq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda_i^*f(x_i^*) = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla f_0 (x^*) + \sum \lambda_i^* \nabla f_i (x_i^*) + \sum \nu_i^* \nabla h_i (x^*) = 0\)</span></p></li>
</ol>
</li>
</ul>
</div>
<div class="section" id="thms-of-alternatives">
<h3>3.6.2.2. thms of alternatives<a class="headerlink" href="#thms-of-alternatives" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><em>weak alternative</em> - at most one of 2 is true</p></li>
<li><p><em>strong alternative</em> - exactly one is true</p>
<ul>
<li><p>ex. Fredholm alternative</p></li>
<li><p>ex. Farkasâ€™s lamma</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\exists x \: Ax \leq 0, c^Tx &lt; 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\exists y \: y \geq 0, A^Ty + c = 0\)</span></p></li>
</ol>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="approx-fitting-boyd-6">
<h2>3.6.3. approx + fitting (boyd 6)<a class="headerlink" href="#approx-fitting-boyd-6" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="norm-approx-problem">
<h3>3.6.3.1. norm approx problem<a class="headerlink" href="#norm-approx-problem" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>minimize <span class="math notranslate nohighlight">\(||Ax-b||\)</span></p></li>
<li><p>ex. weighted norm approx. min||W(Ax-b)||</p></li>
<li><p>ex. least squares min ||Ax-b||<span class="math notranslate nohighlight">\(_2^2\)</span></p></li>
<li><p>ex. chebyshev approx norm min||Ax-b||<span class="math notranslate nohighlight">\(_\infty\)</span></p></li>
<li><p>ex. penalty function approx problem: <span class="math notranslate nohighlight">\(min \: \phi(r_1) + ... + \phi(r_m)\\s.t. \: r=Ax-b\)</span></p></li>
</ul>
</div>
<div class="section" id="least-norm-problem">
<h3>3.6.3.2. least norm problem<a class="headerlink" href="#least-norm-problem" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>min <span class="math notranslate nohighlight">\(||x||\\s.t. \: Ax=b\)</span> ~ min <span class="math notranslate nohighlight">\(||x_0+ Zu||\)</span>, Z cols basis for N(A)</p></li>
</ul>
</div>
<div class="section" id="regularized-approximation">
<h3>3.6.3.3. regularized approximation<a class="headerlink" href="#regularized-approximation" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>min <span class="math notranslate nohighlight">\(||Ax-b|| + \gamma ||x||\)</span></p></li>
<li><p>min <span class="math notranslate nohighlight">\(||Ax-b||^2 + \gamma ||x||^2\)</span></p></li>
<li><p><em>Tikhonov</em>: <span class="math notranslate nohighlight">\(min \: ||Ax-b||_2^2 + \gamma ||x||_2^2\)</span></p></li>
<li><p>examples</p>
<ul>
<li><p>ex. regularize w/ ||Dx||</p></li>
<li><p>ex. lasso</p></li>
<li><p>ex. quadratic smoothing</p></li>
<li><p>ex. total variation</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="robust-approximation">
<h3>3.6.3.4. robust approximation<a class="headerlink" href="#robust-approximation" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A = \bar{A} + U\)</span> ~ random w/ mean 0</p></li>
</ul>
<ol class="simple">
<li><p>stochastic robust approx problem: <span class="math notranslate nohighlight">\(min \: E||Ax-b||\)</span></p></li>
<li><p>(worst-case) robust approx prob: <span class="math notranslate nohighlight">\(min \: sup ||Ax-b|| \: | A \in \mathcal{A}\)</span></p></li>
</ol>
</div>
<div class="section" id="function-fitting">
<h3>3.6.3.5. function fitting<a class="headerlink" href="#function-fitting" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(u) = x_1 f_1 (u) + .... + x_n f_n (u)\)</span> ~ <span class="math notranslate nohighlight">\(f_i\)</span> are basis funcs, <span class="math notranslate nohighlight">\(x_i\)</span> are coefficients</p></li>
<li><p>sparse descriptions + basis pursuit</p></li>
<li><p>interpolation</p></li>
</ul>
</div>
</div>
<div class="section" id="unconstrained-minimization-boyd-9">
<h2>3.6.4. unconstrained minimization (boyd 9)<a class="headerlink" href="#unconstrained-minimization-boyd-9" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="unconstrained-problems">
<h3>3.6.4.1. unconstrained problems<a class="headerlink" href="#unconstrained-problems" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x^* = \text{argmin} \: f(x) \implies \nabla f(x^*) = 0\)</span></p></li>
<li><p>examples</p>
<ul>
<li><p>ex. quadratic: <span class="math notranslate nohighlight">\(\min \: 1/2 x^TPX + q^Tx + r\)</span></p>
<ul>
<li><p>solved w/ <span class="math notranslate nohighlight">\(Px^* + q = 0\)</span>, if <span class="math notranslate nohighlight">\(P \succeq 0\)</span>, unique soln <span class="math notranslate nohighlight">\(-P^{-1}q\)</span></p></li>
</ul>
</li>
<li><p>ex. unconstrained geometric program</p></li>
<li><p>ex. analytic center of linear inequalities</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\min \: f(x) = -\sum \: \log (b_i - a_i^Tx)\)</span> where dom f = <span class="math notranslate nohighlight">\(\{x|a_i^Tx&lt; b_i, i = 1:m\}\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>3 definitions of convexity</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(0 \leq \theta \leq 1\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f(\theta x_1 + (1 - \theta) x_2) \leq \theta f(x_1) + (1 - \theta) f(x_2)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f(x_2) \geq f(x_1) + \nabla f(x_1)^T (x_2 - x_1)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\color{red}0 \preceq \color{green}{\underset{\text{strong convexity}}{mI}} \preceq \nabla^2 \color{cornflowerblue}{f(x)} \preceq \underset{\text{smoothness}}{MI}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\kappa = M/m\)</span> bounds <em>condition number</em> of <span class="math notranslate nohighlight">\(\nabla^2 f = \frac{\lambda_{\max}(\nabla^2 f)}{\lambda_{\min}(\nabla^2 f)}\)</span></p></li>
<li><p><em>strongly convex</em>: <span class="math notranslate nohighlight">\(\nabla^2 f(x) \succeq mI\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\implies f(x_2) \geq f(x_1) + \nabla f(x_1)^T(x_2-x_1) + m/2 ||x_2-x_1||_2^2\)</span></p></li>
<li><p>minimizing yields <span class="math notranslate nohighlight">\(p^* \geq f(x) - 1/(2m) ||\nabla f(x)||_2^2\)</span></p></li>
<li><p>if the gradient of f at x is small enough, then the difference between f(x) and pâ‹† is small</p></li>
</ul>
</li>
<li><p><em>smooth</em>: <span class="math notranslate nohighlight">\(\exists \: M, \: \nabla^2f(x) \preceq MI\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\implies f(y) \leq f(x) + \nabla f(x)^T(y-x) + M/2 ||y-x||_2^2\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>cond(<em>C</em>) = <span class="math notranslate nohighlight">\(W_{\max}^2 / W_{\min}^2â€‹\)</span></p>
<ul>
<li><p><em>width</em> of convex set <span class="math notranslate nohighlight">\(C \subset \mathbb{R}^n\)</span> in direction q with <span class="math notranslate nohighlight">\(||q||_2=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W(C, q) = \underset{z \in C}{\sup} \: q^Tz - \underset{z \in C}{\inf} \: q^Tz\)</span></p></li>
</ul>
</li>
<li><p><em>alpha-level subset</em>: <span class="math notranslate nohighlight">\(C_\alpha = \{x|f(x) \leq \alpha\}\)</span></p></li>
</ul>
</div>
<div class="section" id="descent-methods">
<h3>3.6.4.2. descent methods<a class="headerlink" href="#descent-methods" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>update rule <span class="math notranslate nohighlight">\(x = x + t \Delta x\)</span></p></li>
<li><p><em>exact line search</em>: <span class="math notranslate nohighlight">\(t = \underset{s \geq 0}{\text{argmin}} \:f(x+s \Delta x)\)</span></p></li>
<li><p><em>backtracking line search</em></p>
<ul>
<li><p>given a descent direction <span class="math notranslate nohighlight">\(\Delta x \text{ for } f, x \in dom \: f, \alpha \in (0, 0.5), \beta \in (0, 1)\)</span></p></li>
<li><p>t:=1, <span class="math notranslate nohighlight">\(\alpha \in (0, 0.5), \beta \in (0, 1)\)</span></p></li>
<li><p>while <span class="math notranslate nohighlight">\(f(x + t \Delta x) &gt; f(x) + \alpha t \nabla f(x)^T \Delta x\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(t *= \beta\)</span></p></li>
</ul>
</li>
<li><p>![Screen Shot 2018-07-30 at 10.23.19 PM-3014637](assets/Screen Shot 2018-07-30 at 10.23.19 PM-3014637.png)</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="gd-method">
<h3>3.6.4.3. gd method<a class="headerlink" href="#gd-method" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>convergence</p>
<ul>
<li><p>can bound number of iterations required to be less than <span class="math notranslate nohighlight">\(\epsilon\)</span></p></li>
</ul>
</li>
<li><p>examples</p>
<ul>
<li><p>a quadratic problem in <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p>non-quadratic problem in <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p>a problem in <span class="math notranslate nohighlight">\(R^{100}\)</span></p></li>
<li><p>gradient method and condition number</p></li>
</ul>
</li>
<li><p>conclusions</p>
<ul>
<li><p>gd often exhibits approximately linear convergence</p></li>
<li><p>convergence rate depends greatly on <span class="math notranslate nohighlight">\(cond (\nabla^2 f(x))\)</span> or sublevel sets</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="steepest-descent-method">
<h3>3.6.4.4. steepest descent method<a class="headerlink" href="#steepest-descent-method" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>examples</p>
<ul>
<li><p>euclidean norm: <span class="math notranslate nohighlight">\(\Delta x_{sd} = - \nabla f(x)\)</span></p></li>
<li><p>quadratic norm <span class="math notranslate nohighlight">\(||z||_P = (z^TPz)^{1/2} = ||P^{1/2}z||_2\)</span> where <span class="math notranslate nohighlight">\(P \in S_{++}^n\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\Delta x_{sd} = -P^{-1} \nabla f(x)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\ell_1\)</span> norm: <span class="math notranslate nohighlight">\(\Delta_{sd} = -\frac{\partial f(x)}{\partial x_i} e_i\)</span></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="newton-s-method">
<h3>3.6.4.5. newtonâ€™s method<a class="headerlink" href="#newton-s-method" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p><em>Newton step</em> <span class="math notranslate nohighlight">\(\Delta x_{nt} = - \nabla^2 f(x)^{-1} \nabla f(x)\)</span></p>
<ul>
<li><p>PSD <span class="math notranslate nohighlight">\(\implies \nabla f(x)^T \Delta x_{nt} = - \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) &lt; 0\)</span></p></li>
</ul>
</li>
<li><p><em>Newtonâ€™s method</em></p>
<ol class="simple">
<li><p>compute the newton step <span class="math notranslate nohighlight">\(\Delta x_{nt}\)</span> and decrement <span class="math notranslate nohighlight">\(\lambda^2 = \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x)\)</span></p></li>
<li><p>stopping criterion: quit if <span class="math notranslate nohighlight">\(\lambda^2 / 2 \leq \epsilonâ€‹\)</span></p></li>
<li><p>line search: choose step size t w/ backtracking line search</p></li>
<li><p>update: <span class="math notranslate nohighlight">\(x += t \Delta x_{nt}\)</span></p></li>
</ol>
</li>
</ul>
</div>
</div>
<div class="section" id="basic-algorithms">
<h2>3.6.5. basic algorithms<a class="headerlink" href="#basic-algorithms" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p>types: <em>batch</em> (have full data) vs <em>online</em></p></li>
</ul>
<ol class="simple">
<li><p>gradient descent = <em>batch gradient descent</em></p>
<ul class="simple">
<li><p><em>gradient</em> - vector that points to direction of maximum increase</p></li>
<li><p>at every step, subtract gradient multiplied by learning rate: <span class="math notranslate nohighlight">\(x_k = x_{k-1} - \alpha \nabla_x F(x_{k-1})\)</span></p></li>
<li><p>alpha = 0.05 seems to work</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\theta) = 1/2 (\theta ^T X^T X \theta - 2 \theta^T X^T y + y^T y)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_\theta J(\theta) = X^T X \theta - X^T Y\)</span></p>
<ul>
<li><p>= <span class="math notranslate nohighlight">\(\sum_i  x_i  (x_i^T - y_i)\)</span></p></li>
<li><p>this represents residuals * examples</p></li>
</ul>
</li>
</ul>
</li>
<li><p>stochastic gradient descent</p>
<ul class="simple">
<li><p>donâ€™t use all training examples - approximates gradient</p>
<ul>
<li><p>single-sample</p></li>
<li><p>mini-batch (usually better in offline case)</p></li>
</ul>
</li>
<li><p><em>coordinate-descent</em> algorithm</p></li>
<li><p><em>online</em> algorithm - update theta while training data is changing</p></li>
<li><p>when to stop?</p>
<ul>
<li><p>predetermined number of iterations</p></li>
<li><p>stop when improvement drops below a threshold</p></li>
</ul>
</li>
<li><p>each pass of the whole data = 1 epoch</p></li>
<li><p>benefits</p>
<ol class="simple">
<li><p>less prone to getting stuck to shallow local minima</p></li>
<li><p>donâ€™t need huge ram</p></li>
<li><p>faster</p></li>
</ol>
</li>
</ul>
</li>
<li><p>newtonâ€™s method for optimization</p>
<ul class="simple">
<li><p><em>second-order</em> optimization - requires 1st &amp; 2nd derivatives</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_{k+1} = \theta_k - H_K^{-1} g_k\)</span></p></li>
<li><p>update with inverse of Hessian as alpha - this is an approximation to a taylor series</p></li>
<li><p>finding inverse of Hessian can be hard / expensive</p></li>
</ul>
</li>
<li><p>ADMM - <em>alternating direction method of multipliers</em> (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle</p></li>
</ol>
</div>
<div class="section" id="expectation-maximization-j-11">
<h2>3.6.6. expectation maximization - j 11<a class="headerlink" href="#expectation-maximization-j-11" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p>method to maximize likelihood on model with observed X and hidden Z</p>
<ol class="simple">
<li><p><em>expectation step</em> - values of unobserved latent variables are filled in</p>
<ul>
<li><p>calculates prob of latent variables given observed variables and current param values</p></li>
</ul>
</li>
<li><p><em>maximization step</em> - parameters are adjusted based on filled-in variables</p></li>
</ol>
</li>
<li><p>goal: maximize <em>complete log-likelihood</em>, but donâ€™t know z</p>
<ul>
<li><p><em>expected complete log-likelihood</em> <span class="math notranslate nohighlight">\(E_{p'}[l(\theta; x,z)] = \sum_z p'(z|x,\theta) \cdot \log \: p(x,z|\theta)\)</span></p>
<ul>
<li><p>pâ€™ distribution is assignment to z vars</p></li>
</ul>
</li>
<li><p>deriving <em>auxilary function</em> <span class="math notranslate nohighlight">\(\mathcal L(q, \theta, x) = \sum_z p'(z|x) \log \frac{p(x,z|\theta)}{p'(z|x)}\)</span> - lower bound for the log likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{align} l(\theta; x) &amp;= \log \: p(x|\theta) &amp; \text{incomplete log-likelihood} \\&amp;= \log \sum_z p(x,z|\theta) &amp;\text{complete log-likelihood}\\&amp;= \log\sum_z p'(z|x) \frac{p(x,z|\theta)}{p'(z|x)} &amp;\text{multiplying by 1} \\ &amp;\geq \sum_z p'(z|x) \log \frac{p(x,z|\theta)}{p'(z|x)} &amp;\text{Jensen's inequality}\\&amp;\triangleq \mathcal L (p', \theta) \end{align}\)</span></p></li>
<li><p>this removes dependence on z</p></li>
</ul>
</li>
<li><p>steps</p>
<ul>
<li><p>E: <span class="math notranslate nohighlight">\(p'(z|x, \theta) = \underset{p'}{\text{argmax}}\: \mathcal L(p',\theta, x)\)</span></p></li>
<li><p>M: <span class="math notranslate nohighlight">\(\theta = \underset{\theta}{\text{argmax}} \: \mathcal L(p', \theta, x)\)</span></p></li>
<li><p>equivalent to maximizing expected complete log-likelihood</p></li>
<li><p><em>stochastically</em> converges to <em>local</em> minimum</p></li>
</ul>
</li>
<li><p>alternatively, can look at kl-divergences</p></li>
</ul>
</div>
<div class="section" id="nn-optimization">
<h2>3.6.7. nn optimization<a class="headerlink" href="#nn-optimization" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="why-is-it-hard">
<h3>3.6.7.1. why is it hard?<a class="headerlink" href="#why-is-it-hard" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>plateaus</p></li>
<li><p>winding canyons</p></li>
<li><p>cliffs</p></li>
<li><p>local maxima to dodge</p></li>
<li><p>saddle points (local max and local min)</p></li>
<li><p>most popular</p>
<ul>
<li><p>sgd</p></li>
<li><p>sgd + nesterov momentum</p></li>
<li><p>adam</p></li>
<li><p>adagrad - maintains a per-parameter learning rate that improves performance on problems with sparse gradients</p></li>
<li><p>rmsprop - (ignore) per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing)</p></li>
</ul>
</li>
<li><p>adam - â€œadaptive moment estimationâ€ (kingma_2015)</p>
<ul>
<li><p>keep track of per-parameter learning rate (based on first moment of gradients tracked) and per-parameter second moment (based on variance of gradients tracked)</p></li>
<li><p>alpha - learning rate</p></li>
<li><p>beta1 - exponential decay rate for first moment estimate</p>
<ul>
<li><p>default 0.9</p></li>
</ul>
</li>
<li><p>beta2 - exponential decay rate for 2nd moment estimates (should be higher when gradients sparser)</p>
<ul>
<li><p>default 0.999</p></li>
</ul>
</li>
<li><p>epsilon - small number to prevent division by zero</p>
<ul>
<li><p>default 1e-8 - usually requires tuning (ex. inception requires 1e-1) <img alt="Screen Shot 2018-10-11 at 8.07.56 AM" src="../../_images/adam.png" />visualization</p></li>
</ul>
</li>
</ul>
</li>
<li><p>requires low dims</p>
<ul>
<li><p>goodfellow 2015 â€œQualitatively characterizing neural network optimization problemsâ€ plots loss on line from starting point to ending point</p></li>
<li><p>could do PCA on params</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="complicated-is-simpler">
<h3>3.6.7.2. complicated is simpler<a class="headerlink" href="#complicated-is-simpler" title="Permalink to this headline">Â¶</a></h3>
<ul class="simple">
<li><p>ex. <span class="math notranslate nohighlight">\(x^3 \sin(x)\)</span> is simpler than just <span class="math notranslate nohighlight">\(x\)</span> on the domain [âˆ’0.01, 0.01]</p></li>
<li><p>dropout is like ridge</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes/math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="signals.html" title="previous page">3.5. signals</a>
    <a class='right-next' id="next-link" href="calculus.html" title="next page">3.7. calculus</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Chandan Singh<br/>
        
            &copy; Copyright None.<br/>
          <div class="extra_footer">
            <p>
Many of these images are taken from resources on the web.
</p>
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>