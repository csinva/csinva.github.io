
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.2. graphical models</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/stat/graphical_models';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4.3. causal inference" href="causal_inference.html" />
    <link rel="prev" title="4.1. time series" href="time_series.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview üëã
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../ai/ai.html">1. ai</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ai/knowledge_rep.html">1.1. representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/psychology.html">1.2. psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/fairness_sts.html">1.3. fairness, sts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/philosophy.html">1.4. philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/ai_futures.html">1.5. ai futures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/cogsci.html">1.6. cognitive science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/llms.html">1.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/logic.html">1.8. logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/search.html">1.9. search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai/decisions_rl.html">1.10. decisions, rl</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../math/math.html">2. math</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../math/linear_algebra.html">2.1. linear algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/optimization.html">2.2. optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/differential_equations.html">2.3. differential equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/chaos.html">2.4. chaos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/math_basics.html">2.5. math basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/signals.html">2.6. signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/calculus.html">2.7. calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/proofs.html">2.8. proofs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../math/analysis.html">2.9. real analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ml/ml.html">3. ml</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ml/unsupervised.html">3.1. unsupervised</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/structure_ml.html">3.2. structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/learning_theory.html">3.3. learning theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/deep_learning.html">3.4. deep learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/comp_vision.html">3.5. computer vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/kernels.html">3.6. kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/nlp.html">3.7. nlp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/feature_selection.html">3.8. feature selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/evaluation.html">3.9. evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ml/classification.html">3.10. classification</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="stat.html">4. stat</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="time_series.html">4.1. time series</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">4.2. graphical models</a></li>
<li class="toctree-l2"><a class="reference internal" href="causal_inference.html">4.3. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="game_theory.html">4.4. game theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="info_theory.html">4.5. info theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_models.html">4.6. linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_analysis.html">4.7. data analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="testing.html">4.8. testing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neuro/neuro.html">5. neuro</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../neuro/motor.html">5.1. motor system</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/memory.html">5.2. memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/development.html">5.3. development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/sensory_input.html">5.4. sensory input</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/comp_neuro.html">5.5. comp neuro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/disease.html">5.6. disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/vissci.html">5.7. vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../neuro/brain_basics.html">5.8. brain basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cs/cs.html">6. cs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cs/comp_theory.html">6.1. cs theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/graphs.html">6.2. graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/retrieval.html">6.3. info retrieval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/data_structures.html">6.4. data structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/os.html">6.5. os</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/quantum.html">6.6. quantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/software.html">6.7. software engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/algo.html">6.8. algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/arch.html">6.9. architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/languages.html">6.10. languages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cs/reproducibility.html">6.11. reproducibility</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../research_ovws/research_ovws.html">7. research_ovws</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_disentanglement.html">7.1. disentanglement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_uncertainty.html">7.2. uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_generalization.html">7.3. generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_causal_inference.html">7.4. causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_omics.html">7.5. omics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">7.6. ml in medicine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_llms.html">7.7. llms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">7.8. transfer learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interp.html">7.9. interpretability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_complexity.html">7.10. complexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_dl_theory.html">7.11. dl theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_scat.html">7.12. scattering transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research_ovws/ovw_interesting_science.html">7.13. interesting science</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/csinva/csinva.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notes/stat/graphical_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>graphical models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">4.2.1. overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks-r-n-14-1-5-j-2">4.2.2. bayesian networks - R &amp; N 14.1-5 + J 2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-bn-both-continuous-discrete-vars">4.2.2.1. hybrid BN (both continuous &amp; discrete vars)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-inference">4.2.2.2. exact inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-inferences-in-bns">4.2.2.3. approximate inferences in BNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-independence-properties">4.2.2.4. conditional independence properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#undirected">4.2.3. undirected</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elimination-j-3">4.2.4. elimination - J 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propagation-factor-graphs-j-4">4.2.5. propagation factor graphs - J 4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-inference-on-trees">4.2.5.1. probabilistic inference on trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-graphs">4.2.5.2. factor graphs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map">4.2.5.3. maximum a posteriori (MAP)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-nets">4.2.6. dynamic bayesian nets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-model">4.2.6.1. state space model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hmm">4.2.6.2. hmm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-filtering">4.2.6.3. kalman filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-dbns">4.2.6.4. general dbns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-learning">4.2.7. structure learning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="graphical-models">
<h1><span class="section-number">4.2. </span>graphical models<a class="headerlink" href="#graphical-models" title="Link to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">4.2.1. </span>overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>network types</p>
<ol class="arabic simple">
<li><p><em>bayesian networks</em> - directed</p></li>
<li><p>undirected models</p></li>
</ol>
</li>
<li><p>latent variable types</p>
<ol class="arabic simple">
<li><p><em>mixture models</em> - discrete latent variable</p></li>
<li><p><em>factor analysis models</em> - continuous latent variable</p></li>
</ol>
</li>
<li><p>graph representation: missing edges specify independence (converse is not true)</p>
<ul>
<li><p>encode conditional independence relationships</p>
<ul>
<li><p>helpful for inference</p></li>
</ul>
</li>
<li><p>compact representation of joint prob. distr. over the variables</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/models.png" /></p>
<ul class="simple">
<li><p>dark is observed for HMMs, for other things unclear what it means</p></li>
</ul>
</section>
<section id="bayesian-networks-r-n-14-1-5-j-2">
<h2><span class="section-number">4.2.2. </span>bayesian networks - R &amp; N 14.1-5 + J 2<a class="headerlink" href="#bayesian-networks-r-n-14-1-5-j-2" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>examples</p>
<ol class="arabic simple">
<li><p><em>causal model</em>: causes <span class="math notranslate nohighlight">\(\to\)</span> symptoms</p></li>
<li><p><em>diagnostic model</em>: symptoms <span class="math notranslate nohighlight">\(\to\)</span> causes</p>
<ul>
<li><p>generally requires more dependencies</p></li>
</ul>
</li>
</ol>
</li>
<li><p>learning</p>
<ol class="arabic simple">
<li><p>expert-designed</p></li>
<li><p>data-driven</p></li>
</ol>
</li>
<li><p>properties</p>
<ul>
<li><p>each node is random variable</p></li>
<li><p>weights as tables of conditional probabilities for all possibilities</p></li>
<li><p>represented by directed acyclic graph</p></li>
</ul>
</li>
<li><p>joint distr: <span class="math notranslate nohighlight">\(P(X_1 = x_1,...X_n=x_n)=\prod_{i=1}^n P[X_i = x_i \vert  Parents(X_i)]\)</span></p>
<ul>
<li><p><em>markov condition</em>: given parents, node is conditionally independent of its non-descendants</p>
<ul>
<li><p>marginally, they can still be dependent (e.g. explaining away)</p></li>
</ul>
</li>
<li><p>given its <em>markov blanket</em> (parents, children, and children‚Äôs parents), a node is independent of all other nodes</p></li>
</ul>
</li>
<li><p>BN has no redundancy <span class="math notranslate nohighlight">\(\implies\)</span> no chance for inconsistency</p>
<ul>
<li><p>forming a BN: keep adding nodes, and only previous nodes are allowed to be parents of new nodes</p></li>
</ul>
</li>
</ul>
<section id="hybrid-bn-both-continuous-discrete-vars">
<h3><span class="section-number">4.2.2.1. </span>hybrid BN (both continuous &amp; discrete vars)<a class="headerlink" href="#hybrid-bn-both-continuous-discrete-vars" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>for continuous variables can sometimes discretize</p></li>
</ul>
<ol class="arabic simple">
<li><p><em>linear Gaussian</em> - for continuous children</p></li>
</ol>
<ul class="simple">
<li><p>parents all discrete <span class="math notranslate nohighlight">\(\implies\)</span> <em>conditional Gaussian</em> - multivariate Gaussian given assignment to discrete variables</p></li>
<li><p>parents all continuous <span class="math notranslate nohighlight">\(\implies\)</span> <em>multivariate Gaussian</em> over all the variables, and a multivariate posterior distribution (given any evidence)</p></li>
<li><p>parents some discrete, some continuous</p>
<ul>
<li><p>h is continuous, s is discrete; a, b, <span class="math notranslate nohighlight">\(\sigma\)</span> all change when s changes</p></li>
<li><p><span class="math notranslate nohighlight">\(P(c|h,s) = N(a \cdot h + b, \sigma^2)\)</span>, so mean is linear function of h</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p>discrete children (continuous parents)</p></li>
<li><p><em>probit distr</em> - <span class="math notranslate nohighlight">\(P(buys|Cost=c) = \phi[(\mu - c)/\sigma]\)</span> - integral of standard normal distr
- like a soft threshold</p></li>
<li><p><em>logit distr.</em> - <span class="math notranslate nohighlight">\(P(buys|Cost=c) =s\left(\frac{-2 (\mu - c)}\sigma \right)\)</span>
- logistic function (sigmoid s) produces thresh</p></li>
</ol>
</section>
<section id="exact-inference">
<h3><span class="section-number">4.2.2.2. </span>exact inference<a class="headerlink" href="#exact-inference" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>given assignment to <em>evidence variables</em> E, find probs of <em>query variables</em> X</p>
<ul>
<li><p>other variables are <em>hidden variables</em> H</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple">
<li><p><em>enumeration</em> - just try summing over all hidden variables</p></li>
</ol>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X|e) = \alpha P(X, e) = \alpha \sum_h P(X, e, h)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> can be calculated as <span class="math notranslate nohighlight">\(1 / \sum_x P(x, e)\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(O(n \cdot 2^n)\)</span></p>
<ul>
<li><p>one summation for each of <em>n</em> variables</p></li>
</ul>
</li>
<li><p>ENUMERATION-ASK evaluates in depth-first order: <span class="math notranslate nohighlight">\(O(2^n)\)</span></p>
<ul>
<li><p>we removed the factor of <em>n</em></p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>variable elimination</em> - dynamic programming <strong>(see elimination)</strong></p></li>
</ol>
<ul class="simple">
<li><p><img alt="Screen Shot 2018-07-26 at 8.52.30 AM" src="../../_images/bayesian_net_example.png" /></p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|j, m) = \alpha \underbrace{P(B)}_{f_1(B)} \sum_e \underbrace{P(e)}_{f_2(E)} \sum_a \underbrace{P(a|B,e)}_{f_3(A, B, E)} \underbrace{P(j|a)}_{f_4(A)} \underbrace{P(m|a)}_{f_5(A)}\)</span></p>
<ul>
<li><p>calculate factors in reverse order (bottom-up)</p></li>
<li><p>each factor is a vector with num entries = <span class="math notranslate nohighlight">\(\prod\)</span> |num_elements| * |num_values|</p></li>
<li><p>when we multiply them, pointwise products</p></li>
</ul>
</li>
<li><p>ordering</p>
<ul>
<li><p>any ordering works, some are more efficient</p></li>
<li><p>every variable that is not an ancestor of a query variable or evidence variable is irrelevant to the query</p></li>
<li><p>complexity depends on largest factor formed</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="3">
<li><p><em>clustering algorithms</em> = <em>join tree</em> algorithms <strong>(see propagation factor graphs)</strong></p></li>
</ol>
<ul class="simple">
<li><p>join individual nodes in such a way that resulting network is a polytree</p>
<ul>
<li><p><img alt="Screen Shot 2018-07-26 at 8.52.30 AM-2621781" src="../../_images/bayesian_net_example2.png" /></p></li>
<li><p><em>polytree</em>=<em>singly-connected network</em> - only 1 undirected paths between any 2 nodes</p>
<ul>
<li><p>time and space complexity of exact inference is linear in the size of the network</p></li>
<li><p>holds even if the number of parents of each node is bounded by a constant</p></li>
</ul>
</li>
</ul>
</li>
<li><p>can compute posterior probabilities in <span class="math notranslate nohighlight">\(O(n)\)</span></p>
<ul>
<li><p>however, conditional probability tables may still be exponentially large</p></li>
</ul>
</li>
</ul>
</section>
<section id="approximate-inferences-in-bns">
<h3><span class="section-number">4.2.2.3. </span>approximate inferences in BNs<a class="headerlink" href="#approximate-inferences-in-bns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>randomized sampling algorithms = <em>monte carlo</em> algorithms</p></li>
</ul>
<ol class="arabic simple">
<li><p><em>direct sampling</em> methods:</p></li>
</ol>
<ul class="simple">
<li><p><em>simplest</em> - sample network in topological order</p></li>
<li><p><em>rejection sampling</em> - sample in order and stop once evidence is violated</p>
<ul>
<li><p>want P(D|A)</p></li>
<li><p>sample N times, throw out samples where A is false</p></li>
<li><p>return probability of D being true</p></li>
<li><p>this is slow</p></li>
</ul>
</li>
<li><p><em>likelihood weighting</em> - fix evidence to be more efficient</p>
<ul>
<li><p>generating a sample</p>
<ul>
<li><p>fix our evidence variables to their observed values, then simulate the network</p></li>
<li><p>can‚Äôt just fix variables - distr. might be inconsistent</p></li>
<li><p>calculate <em>W</em> = prob of sample being generated</p>
<ul>
<li><p>when we get to an evidence variable, multiply by prob it appears given its parents</p></li>
</ul>
</li>
</ul>
</li>
<li><p>for each observation</p>
<ul>
<li><p>if positive, Count = Count + <em>W</em></p></li>
<li><p>Total = Total + <em>W</em></p></li>
</ul>
</li>
<li><p>return Count/Total</p></li>
<li><p>this way we don‚Äôt have to throw out wrong samples</p></li>
<li><p>doesn‚Äôt solve all problems - evidence only influences the choice of downstream variables</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>Markov chain monte carlo</em> - ex. <em>Gibbs sampling</em>, <em>Metropolis-Hastings</em></p></li>
</ol>
<ul class="simple">
<li><p>fix evidence variables</p></li>
<li><p>sample a nonevidence variable <span class="math notranslate nohighlight">\(X_i\)</span> conditioned on the current values of its Markov blanket</p></li>
<li><p>repeatedly resample one-at-a-time in arbitrary order</p></li>
<li><p>why it works</p>
<ul>
<li><p>the sampling process settles into a dynamic equilibrium where time spent in each state is proportional to its posterior probability</p></li>
<li><p>provided transition matrix q is <em>ergodic</em> - every state is reachable and there are no periodic cycles - only 1 steady-state soln</p></li>
</ul>
</li>
<li><p>2 steps</p>
<ol class="arabic simple">
<li><p>create markov chain with write stationary distr.</p></li>
<li><p>draw samples by simulating the chain</p></li>
</ol>
</li>
<li><p>methods</p>
<ul>
<li><p>0th order methods - query density</p>
<ul>
<li><p>metropolized random walk</p></li>
<li><p>ball walk</p></li>
<li><p>hit-and-run algorithm</p></li>
</ul>
</li>
<li><p>1st order methods - uses gradient of the density</p>
<ul>
<li><p>Gibbs: we have conditionals</p></li>
<li><p>metropolis adjusted langevin algorithm (MALA) = langevin monte carlo</p>
<ul>
<li><p>use gradient to propose new states</p></li>
<li><p>accept / reject using metropolis-hastings algorithm</p></li>
</ul>
</li>
<li><p>unadjusted langevin algorithm (ULA)</p></li>
<li><p>hamiltonian monte carlo (neal, 2011)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>log-concave distr. density (analog of convexity)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\pi(x) = \frac{e^{-f(x)}}{\int e^{-f(y)}dy}\)</span></p></li>
<li><p>examples: normal distr., exponential distr., Laplace distr.</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="3">
<li><p><em>variational inference</em> - formulate inference as optimization</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1601.00670">good intro paper</a></p></li>
<li><p>minimize KL-divergence between observed samples and assumed distribution</p>
<ul>
<li><p>the actual KL is hard to minimize so instead we maximize the ELBO, which is equivalent</p></li>
</ul>
</li>
<li><p>do this over a class of possible distrs.</p></li>
<li><p>variational inference tends to be faster, but may not be as good as MCMC</p></li>
</ul>
</li>
</ol>
</section>
<section id="conditional-independence-properties">
<h3><span class="section-number">4.2.2.4. </span>conditional independence properties<a class="headerlink" href="#conditional-independence-properties" title="Link to this heading">#</a></h3>
<ul>
<li><p>multiple, competing explanations (‚Äúexplaining-away‚Äù)</p>
<p><img alt="" src="../../_images/j2_1.png" /></p>
<ul class="simple">
<li><p>in fact any descendant of the base of the v suffices for explaining away</p></li>
</ul>
</li>
<li><p><em>d-separation</em> = directed separation</p></li>
<li><p><em>Bayes ball algorithm</em> - is <span class="math notranslate nohighlight">\(( X_A \perp X_B )| X_C\)</span>?</p>
<ul class="simple">
<li><p>initialize</p>
<ul>
<li><p>shade <span class="math notranslate nohighlight">\(X_C\)</span></p></li>
<li><p>place ball at each of <span class="math notranslate nohighlight">\(X_A\)</span></p></li>
<li><p>if any ball reaches <span class="math notranslate nohighlight">\(X_B\)</span>, then not conditionally independent</p></li>
</ul>
</li>
<li><p>rules</p>
<ul>
<li><p>balls can‚Äôt pass through shaded unless shaded is at base of v</p></li>
<li><p>balls pass through unshaded unless unshaded is at base of v</p></li>
</ul>
</li>
</ul>
</li>
<li><p><img alt="Screen Shot 2018-09-16 at 7.12.22 PM" src="../../_images/triples.png" /></p></li>
</ul>
</section>
</section>
<section id="undirected">
<h2><span class="section-number">4.2.3. </span>undirected<a class="headerlink" href="#undirected" title="Link to this heading">#</a></h2>
<ul>
<li><p><span class="math notranslate nohighlight">\(X_A \perp X_C | X_B\)</span> if the set of nodes <span class="math notranslate nohighlight">\(X_B\)</span> separates the nodes <span class="math notranslate nohighlight">\(X_A\)</span> from <span class="math notranslate nohighlight">\(X_C\)</span></p>
<p><img alt="Screen Shot 2018-07-24 at 11.16.29 PM" src="../../_images/graph_separation.png" /></p>
</li>
<li><p>can‚Äôt convert directed / undirected</p></li>
</ul>
<p><img alt="Screen Shot 2018-07-24 at 11.17.57 PM" src="../../_images/full_graph_vs_missing.png" /></p>
<ul class="simple">
<li><p>factor over <em>maximal cliques</em> (largest sets of fully connected nodes)</p></li>
<li><p>potential function <span class="math notranslate nohighlight">\(\psi_{X_C} (x_C)\)</span> function on possible realizations <span class="math notranslate nohighlight">\(x_C\)</span> of the maximal clique <span class="math notranslate nohighlight">\(X_C\)</span></p>
<ul>
<li><p>non-negative, but not a probability (specifying conditional probs. doesn‚Äôt work)</p></li>
<li><p>commonly let these be exponential: <span class="math notranslate nohighlight">\(\psi_{X_C} (x_C) = \exp(-f_C(x_C))\)</span></p>
<ul>
<li><p>yields energy <span class="math notranslate nohighlight">\(f(x) = \sum_C f_C(x_C)\)</span></p></li>
<li><p>yields <em>Boltzmann distribution</em>: <span class="math notranslate nohighlight">\(p(x) = \frac{1}{Z} \exp (-f(x))\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(p(x) = \frac{1}{Z} \prod_{C \in Cliques} \psi_{X_C}(x_c)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Z = \sum_x \prod_{C \in Cliques} \psi_{X_C} (x_C)\)</span></p></li>
</ul>
</li>
<li><p><em>reduced parameterizations</em> - impose constraints on probability distributions (e.g. Gaussian)</p></li>
<li><p>if x is dependent on all its neighbors</p>
<ul>
<li><p><em>Ising model</em> - if x is binary</p></li>
<li><p><em>Potts model</em> - x is multiclass</p></li>
</ul>
</li>
</ul>
</section>
<section id="elimination-j-3">
<h2><span class="section-number">4.2.4. </span>elimination - J 3<a class="headerlink" href="#elimination-j-3" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>the elimination algorithm is for <em>probabilistic inference</em></p>
<ul>
<li><p>want <span class="math notranslate nohighlight">\(p(x_F|x_E)\)</span> where E and F are disjoint</p></li>
<li><p>any var that is not ancestor of evidence or ancestor of query is irrelevant</p></li>
</ul>
</li>
<li><p>here, let <span class="math notranslate nohighlight">\(X_F\)</span> be a single node</p></li>
<li><p>notation</p>
<ul>
<li><p>define <span class="math notranslate nohighlight">\(m_i (x_{S_i})\)</span> = <span class="math notranslate nohighlight">\(\sum_{x_i}\)</span> where <span class="math notranslate nohighlight">\(x_{S_i}\)</span> are the variables, other than <span class="math notranslate nohighlight">\(x_i\)</span>, that appear in the summand</p></li>
<li><p>define <em>evidence potential</em> <span class="math notranslate nohighlight">\(\delta(x_i, \bar{x_i})\)</span> is defined as <span class="math notranslate nohighlight">\(x_i == \bar{x_i}\)</span></p>
<ul>
<li><p>then $<span class="math notranslate nohighlight">\(g(\bar{x_i}) = \sum_{x_i} \delta (x_i, \bar{x_i})\)</span>$</p></li>
<li><p>for a set <span class="math notranslate nohighlight">\(\delta (x_E, \bar{x_E}) = \prod_{i \in E} \delta (x_i, \bar{x_i})\)</span></p></li>
<li><p>lets us define <span class="math notranslate nohighlight">\(p(x, \bar{x}_E) = p^E(x) = p(x) \delta (x_E, \bar{x_E})\)</span></p></li>
</ul>
</li>
<li><p>undirected graphs</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\psi_i^E(x_i) \triangleq \psi_i(x_i) \delta(x_i, \bar{x}_i)\)</span></p></li>
<li><p>this lets us write <span class="math notranslate nohighlight">\(p^E (x) = \frac{1}{Z} \prod_{c\in C} \psi^E_{X_c} (x_c)\)</span></p>
<ul>
<li><p>can ignore z since this is unnormalized anyway</p></li>
<li><p>to find conditional probability, divide by all sum of <span class="math notranslate nohighlight">\(p^E(x)\)</span> for all values of E</p></li>
</ul>
</li>
<li><p>in actuality don‚Äôt compute the product, just take the correct slice</p></li>
</ul>
</li>
</ul>
</li>
<li><p>eliminate algorithm</p>
<ol class="arabic simple">
<li><p>initialize: choose an ordering with query last</p></li>
<li><p>evidence: set evidence vars to their values</p></li>
<li><p>update: loop over element <span class="math notranslate nohighlight">\(x_i\)</span> in ordering</p>
<ol class="arabic simple">
<li><p>let <span class="math notranslate nohighlight">\(\phi_i(x_{T_i})\)</span> be product of all potentials involving <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p>sum over the product of these potentials <span class="math notranslate nohighlight">\(m_i(x_{S_i}) = \sum_x \phi_i(x_{T_i})\)</span></p></li>
</ol>
</li>
<li><p>normalize: <span class="math notranslate nohighlight">\(p(x_F|\bar{x}_E) = \phi_F(x_F) / \sum_{x_F} \phi_F (x_F)\)</span></p></li>
</ol>
</li>
<li><p>undirected graph elimination algorithm</p>
<ul>
<li><p>for directed graph, first <em>moralize</em></p>
<ul>
<li><p>for each node connect its parents</p></li>
<li><p>drop edges orientation</p></li>
</ul>
</li>
<li><p>for each node X</p>
<ul>
<li><p>connect all remaining neighbors of X</p></li>
<li><p>remove X from graph</p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>reconstituted graph</em> - same nodes, includes all edges that were added</p>
<ul>
<li><p><em>elimination cliques</em> - includes X and its neighbors when X is removed</p></li>
<li><p>computational complexity is the exponential in the number of variables in the elimination clique</p></li>
<li><p>involves <em>treewidth</em> - one less than smallest achievable value of cardinality of largest elimination clique</p>
<ul>
<li><p>range over all possible elimination orderings</p></li>
<li><p>NP-hard to find elimination ordering that achieves the treewidth</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="propagation-factor-graphs-j-4">
<h2><span class="section-number">4.2.5. </span>propagation factor graphs - J 4<a class="headerlink" href="#propagation-factor-graphs-j-4" title="Link to this heading">#</a></h2>
<ul>
<li><p><em>tree</em> - undirected graph in which there is exactly one path between any pair of nodes</p>
<ul>
<li><p>if directed, then moralized graph should be a tree</p></li>
<li><p><em>polytree</em> - directed graph that reduces to an undirected tree if we convert each directed edge to an undirected edge</p></li>
<li><p><img alt="Screen Shot 2018-07-31 at 11.44.52 AM" src="../../_images/polytree.png" /></p></li>
<li><div class="math notranslate nohighlight">
\[p(x) = \frac{1}{Z} \left[ \prod_{i \in V} \psi (x_i) \prod_{(i,j)\in E} \psi (x_i,x_j) \right]\]</div>
<ul class="simple">
<li><p>for directed, root has individual prob and others are conditionals</p></li>
</ul>
</li>
<li><p>can once again use evidence potentials for conditioning</p></li>
</ul>
</li>
</ul>
<section id="probabilistic-inference-on-trees">
<h3><span class="section-number">4.2.5.1. </span>probabilistic inference on trees<a class="headerlink" href="#probabilistic-inference-on-trees" title="Link to this heading">#</a></h3>
<ul>
<li><p>eliminate algorithm through message-passing</p>
<ul class="simple">
<li><p>ordering I should be <strong>depth-first traversal</strong> of tree with f as root and all edges pointing away</p>
<ul>
<li><p><em>message</em> <span class="math notranslate nohighlight">\(m_{ji}(x_i)\)</span> from <span class="math notranslate nohighlight">\(j\)</span> to <span class="math notranslate nohighlight">\(i\)</span> =<em>intermediate factor</em></p></li>
</ul>
</li>
</ul>
</li>
<li><p>2 key equations</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m_{ji}(x_i) = \sum_{x_j} \left( \psi^E (x_j) \psi (x_i, x_j) \prod_{k \in N(j) \backslash i} m_{kj} (x_j) \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p(x_f | \bar{x}_E) \propto \psi^E (x_f) \prod_{e \in N(f)} m_{ef} (x_f) \)</span></p>
<ul>
<li><p><img alt="Screen Shot 2018-07-31 at 9.55.08 PM" src="../../_images/undirected_message_passing.png" /></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>sum-product</strong> = <strong>belief propagation</strong> - inference algorithm</p>
<ul>
<li><p>computes all single-node marginals (for certain classes of graphs) rather than only a single marginal</p></li>
<li><p>only works in trees or tree-like graphs</p></li>
<li><p><img alt="Screen Shot 2018-07-31 at 10.07.15 PM" src="../../_images/message_passing_misc.png" /></p>
<p><img alt="Screen Shot 2018-07-31 at 10.07.40 PM" src="../../_images/message_passing_parallel.png" /></p>
</li>
<li><p><em>message-passing protocol</em> - a node can send a message to a neighboring node when, and only when, it has received messages from all of its other neighbors (parallel algorithm)</p>
<ol class="arabic simple">
<li><p>evidence(E)</p></li>
<li><p>choose root</p></li>
<li><p>collect: send messages evidence to root</p></li>
<li><p>distribute: send messages root back out</p></li>
</ol>
<p><img alt="Screen Shot 2018-07-31 at 10.22.55 PM" src="../../_images/message_passing_individual.png" /> <img alt="Screen Shot 2018-07-31 at 10.23.01 PM" src="../../_images/message_passing_distribute.png" /></p>
</li>
</ul>
</li>
</ul>
</section>
<section id="factor-graphs">
<h3><span class="section-number">4.2.5.2. </span>factor graphs<a class="headerlink" href="#factor-graphs" title="Link to this heading">#</a></h3>
<ul>
<li><p><em>factor graphs</em> capture factorizations, not conditional independence statements</p>
<ul>
<li><p>ex <span class="math notranslate nohighlight">\(\psi (x_1, x_2, x_3) = f_a(x_1,x_2) f_b(x_2,x_3) f_c (x_1,x_3)\)</span> factors but has no conditional independence</p>
<ul class="simple">
<li><p><img alt="Screen Shot 2018-07-31 at 11.30.19 PM" src="../../_images/factor_graph.png" /></p></li>
</ul>
</li>
<li><div class="math notranslate nohighlight">
\[f(x_1,...,x_n) = \prod_s f_s (x_{C_s})\]</div>
</li>
<li><p>neighborhood N(s) for a factor index s is all the variables the factor references</p></li>
<li><p>neighborhood N(i) for a node i is set of factors that reference <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p>provide more fine-grained representation of prob. distr.</p>
<ul class="simple">
<li><p>could add more nodes to normal graphical model to do this</p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>factor tree</em> - if factors are made nodes, resulting undirected graph is tree</p>
<ul>
<li><p>two kinds of messages (variable-&gt; factor &amp; factor-&gt; variable)</p></li>
<li><p>run all the factor <span class="math notranslate nohighlight">\(\to\)</span> variables first</p></li>
<li><p><img alt="" src="../../_images/j4_2.png" /></p></li>
<li><div class="math notranslate nohighlight">
\[p(x_i) \propto \prod_{s \in N(i)} \mu_{si} (x_i)\]</div>
</li>
<li><p>if a graph is originally a tree, there is little to be gained from factor graph framework</p>
<ul class="simple">
<li><p>sometimes factor graph is factor tree, but original graph is not</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="maximum-a-posteriori-map">
<h3><span class="section-number">4.2.5.3. </span>maximum a posteriori (MAP)<a class="headerlink" href="#maximum-a-posteriori-map" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>want <span class="math notranslate nohighlight">\(\max_{x_F} p(x_F | \bar{x}_E)\)</span></p></li>
<li><p>MAP-eliminate algorithm is very similar to before</p>
<ul>
<li><p>initialize - choose ordering</p></li>
<li><p>evidence - set evidence</p></li>
<li><p>update - for each take max over variable and make new factor</p></li>
<li><p>maximum - marginalize</p></li>
</ul>
</li>
<li><p>products of probs tend to underflow, so take <span class="math notranslate nohighlight">\(\max_x \log p^E (x)\)</span></p></li>
<li><p>can also derive a <em>max-product algorithm</em> for trees</p></li>
</ul>
<ol class="arabic simple">
<li><p>find <span class="math notranslate nohighlight">\(\text{argmax}_x p^E (x)\)</span></p>
<ul class="simple">
<li><p>can solve by keeping track of maximizing values of variables in max-product algorithm</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="dynamic-bayesian-nets">
<h2><span class="section-number">4.2.6. </span>dynamic bayesian nets<a class="headerlink" href="#dynamic-bayesian-nets" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><em>dynamic bayesian nets</em> - represents a temporal prob. model</p></li>
</ul>
<section id="state-space-model">
<h3><span class="section-number">4.2.6.1. </span>state space model<a class="headerlink" href="#state-space-model" title="Link to this heading">#</a></h3>
<ul>
<li><p>state space model <img alt="" src="../../_images/j15_1.png" /></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X_{0:t}, E_{1:t}) = P(X_0) \prod_{i} \underbrace{P(X_i | X_{i-1}) }_{\text{transition model}}  \: \underbrace{P(E_i|X_i)}_{\text{sensor model}}\)</span></p>
<ul>
<li><p>agent maintains <em>belief state</em> of state variables <span class="math notranslate nohighlight">\(X_t\)</span> given evidence variables <span class="math notranslate nohighlight">\(E_t\)</span></p></li>
<li><p>improve accuracy</p>
<ol class="arabic simple">
<li><p>increase order of Markov transition model</p></li>
<li><p>increase set of state variables (can be equivalent to 1)</p></li>
</ol>
<ul class="simple">
<li><p>hard to maintain state variables over time, want more sensors</p></li>
</ul>
</li>
</ul>
</li>
<li><p>4 inference problems (here <span class="math notranslate nohighlight">\(\cdot\)</span> is elementwise multiplication)</p>
<ol class="arabic simple">
<li><p><em>filtering</em> = <em>state estimation</em> - compute <span class="math notranslate nohighlight">\(P(X_t | e_{1:t})\)</span></p></li>
</ol>
<ul class="simple">
<li><p><em>recursive estimation</em>:  $<span class="math notranslate nohighlight">\(\underbrace{P(X_{t+1}|e_{1:t+1})}_{\text{new state}} = \alpha \: \underbrace{P(e_{t+1}|X_{t+1})}_{\text{sensor}} \cdot \underset{x_t}{\sum} \: \underbrace{P(X_{t+1}|x_t)}_{\text{transition}} \cdot \underbrace{P(x_t|e_{1:t})}_{\text{old state}}\)</span><span class="math notranslate nohighlight">\( where \)</span>\alpha$ normalizes probs</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><em>prediction</em> - compute <span class="math notranslate nohighlight">\(P(X_{t+k}|e_{1:t})\)</span> for <span class="math notranslate nohighlight">\(k&gt;0\)</span></p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(\underbrace{P(X_{t+k+1} |e_{1:t})}_{\text{new state}} = \sum_{x_{t+k}} \underbrace{P(X_{t+k+1} |x_{t+k})}_{\text{transition}}  \cdot \underbrace{P(x_{t+k} |e_{1:t})}_{\text{old state}}\)</span></p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><em>smoothing</em> - compute <span class="math notranslate nohighlight">\(P(X_{k}|e_{1:t})\)</span> for <span class="math notranslate nohighlight">\(0 &lt; k &lt; t\)</span></p>
<ol class="arabic simple">
<li><p>2 components <span class="math notranslate nohighlight">\(P(X_k|e_{1:t}) = \alpha \underbrace{P(X_k|e_{1:k})}_{\text{forward}} \cdot \underbrace{P(e_{k+1:t}|X_k)}_{\text{backward}}\)</span></p></li>
<li><p>forward pass: filtering from <span class="math notranslate nohighlight">\(1:t\)</span>
2. backward pass from <span class="math notranslate nohighlight">\(t:1\)</span> <span class="math notranslate nohighlight">\(\underbrace{P(e_{k+1:t}|X_k)}_{\text{sensor past k}} = \sum_{x_{k+1}} \underbrace{P(e_{k+1}|x_{k+1})}_{\text{sensor}} \cdot \underbrace{P(e_{k+2:t}|x_{k+1})}_{\text{recursive call}} \cdot \underbrace{P(x_{k+1}|X_k)}_{\text{transition}}\)</span>
3. this is called the forward-backward algo(also there is a separate algorithm that doesn‚Äôt use the observations on the backward pass)</p></li>
</ol>
</li>
<li><p><em>most likely explanation</em> - <span class="math notranslate nohighlight">\(\underset{x_{1:t}}{\text{argmax}}\:P(x_{1:t}|e_{1:t})\)</span></p>
<ol class="arabic simple">
<li><p><em>Viterbi algorithm</em>: <span class="math notranslate nohighlight">\(\underbrace{\underset{x_{1:t}}{\text{max}} \: P(x_{1:t}, X_{t+1}|e_{1:t+1})}_{\text{mle x}} = \alpha \: \underbrace{P(e_{t+1}|X_{t+1})}_{\text{sensor}} \cdot \underset{x_t}{\text{max}} \left[ \: \underbrace{P(X_{t+1}|x_t)}_{\text{transition}} \cdot \underbrace{\underset{x_{1:t-1}}{\text{max}} \:P(x_{1:t-1}, x_{t+1}|e_{1:t})}_{\text{max prev state}} \right]\)</span></p></li>
<li><p>complexity</p>
<ul class="simple">
<li><p>K = number of states</p></li>
<li><p>M = number of observations</p></li>
<li><p>n = length of sequence</p></li>
<li><p>memory - <span class="math notranslate nohighlight">\(nK\)</span></p></li>
<li><p>runtime - <span class="math notranslate nohighlight">\(O(nK^2)\)</span></p></li>
</ul>
</li>
</ol>
</li>
</ol>
<ul class="simple">
<li><p><em>learning</em> - form of EM</p>
<ul>
<li><p>basically just count (maximizing joint likelihood of input and output)</p></li>
<li><p>initial state probs <span class="math notranslate nohighlight">\(\frac{count(start \to s)}{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(x'|x) = \frac{count(s \to s')}{count(s)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(y|x) = \frac{count (x \to y)}{count(x)}\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="hmm">
<h3><span class="section-number">4.2.6.2. </span>hmm<a class="headerlink" href="#hmm" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>state is a single discrete process</strong></p></li>
<li><p>transitions are all matrices (and no zeros in sensor model)<span class="math notranslate nohighlight">\(\implies\)</span> forward pass is invertible so can use constant space</p></li>
<li><p><em><strong>online smoothing (with lag)</strong></em></p></li>
<li><p>ex. robot localization</p></li>
</ul>
</section>
<section id="kalman-filtering">
<h3><span class="section-number">4.2.6.3. </span>kalman filtering<a class="headerlink" href="#kalman-filtering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>state is continuous</strong></p></li>
<li><p>ex. <img alt="" src="../../_images/r15_9.png" /></p></li>
<li><p>type of nodes (real-valued vectors) and prob model (linear-Gaussian) changes from HMM</p></li>
<li><p>1d example: <em>random walk</em></p></li>
<li><p>state nodes: <span class="math notranslate nohighlight">\(x_{t+1} = Ax_t + Gw_t\)</span></p></li>
<li><p>output nodes: <span class="math notranslate nohighlight">\(y_t = Cx_t+v_t\)</span></p>
<ul>
<li><p>x is linear Gaussian</p></li>
<li><p>w is noise Gaussian</p></li>
<li><p>y is linear Gaussian</p></li>
</ul>
</li>
<li><p>doing the integral for prediction involves completing the square</p></li>
<li><p>properties</p>
<ol class="arabic simple">
<li><p>new mean is weighted mean of new observation and old mean</p></li>
<li><p>update rule for variance is independent of the observation</p></li>
<li><p>variance converges quickly to fixed value that depends only on <span class="math notranslate nohighlight">\(\sigma^2_x, \sigma^2_z\)</span></p></li>
</ol>
</li>
<li><p><strong>Lyapunov eqn</strong>: evolution of variance of states</p></li>
<li><p><strong>information filter</strong> - mathematically the same but different parameterization</p></li>
<li><p><em>extended Kalman filter</em></p>
<ul>
<li><p>works on nonlinear systems</p></li>
<li><p>locally linear</p></li>
</ul>
</li>
<li><p><em>switching Kalman filter</em> - multiple Kalman filters run in parallel and weighted sum of predictions is used</p>
<ul>
<li><p>ex. one for straight flight, one for sharp left turns, one for sharp right turns</p></li>
<li><p>equivalent to adding discrete ‚Äúmaneuver‚Äù state variable</p></li>
</ul>
</li>
</ul>
</section>
<section id="general-dbns">
<h3><span class="section-number">4.2.6.4. </span>general dbns<a class="headerlink" href="#general-dbns" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>can be better to decompose state variable into multiple vars</p>
<ul>
<li><p>reduces size of transition matrix</p></li>
</ul>
</li>
<li><p><em>transient failure model</em> - allows probability of sensor giving wrong value</p></li>
<li><p><em>persistent failure model</em> - additional variable describing status of battery meter</p></li>
<li><p>exact inference - <em>v√•ariable elimination</em> mimics recursive filtering</p>
<ul>
<li><p>still difficult</p></li>
</ul>
</li>
<li><p>approximate inference - modification of likelihood weighting</p>
<ul>
<li><p>use samples as approximate representation of current state distr.</p></li>
<li><p><em><strong>particle filtering</strong></em> - focus set of samples on high-prob regions of the state space</p>
<ul>
<li><p>consistent</p></li>
<li><p>sample a state</p></li>
<li><p>sample the next state given the previous state</p></li>
<li><p>weight each sample by <span class="math notranslate nohighlight">\(P(e_t | x_t)\)</span></p></li>
<li><p>resample based on weight</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="structure-learning">
<h2><span class="section-number">4.2.7. </span>structure learning<a class="headerlink" href="#structure-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>conditional correlation - inverse covariance matrix = precision matrix</p>
<ul>
<li><p>estimates only good when <span class="math notranslate nohighlight">\(n &gt;&gt; p\)</span></p></li>
<li><p>eigenvalues are not well-approximated</p></li>
<li><p>often enforce sparsity</p></li>
<li><p>ex. threshold each value in the cov matrix (set to 0 unless greater than thresh) - this threshold can depend on different things</p></li>
<li><p>can also use regularization to enforce sparsity</p></li>
<li><p>POET doesn‚Äôt assume sparsity</p></li>
</ul>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notes/stat"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="time_series.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4.1. </span>time series</p>
      </div>
    </a>
    <a class="right-next"
       href="causal_inference.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.3. </span>causal inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">4.2.1. overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks-r-n-14-1-5-j-2">4.2.2. bayesian networks - R &amp; N 14.1-5 + J 2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hybrid-bn-both-continuous-discrete-vars">4.2.2.1. hybrid BN (both continuous &amp; discrete vars)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-inference">4.2.2.2. exact inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-inferences-in-bns">4.2.2.3. approximate inferences in BNs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-independence-properties">4.2.2.4. conditional independence properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#undirected">4.2.3. undirected</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elimination-j-3">4.2.4. elimination - J 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propagation-factor-graphs-j-4">4.2.5. propagation factor graphs - J 4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-inference-on-trees">4.2.5.1. probabilistic inference on trees</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-graphs">4.2.5.2. factor graphs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-map">4.2.5.3. maximum a posteriori (MAP)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-nets">4.2.6. dynamic bayesian nets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-model">4.2.6.1. state space model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hmm">4.2.6.2. hmm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kalman-filtering">4.2.6.3. kalman filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-dbns">4.2.6.4. general dbns</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-learning">4.2.7. structure learning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chandan Singh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright None.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Many of these images are taken from resources on the web.
</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>